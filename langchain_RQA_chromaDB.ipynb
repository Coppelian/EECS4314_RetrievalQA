{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IlYbPcRYQhhY"
      },
      "source": [
        "#### [LangChain Handbook](https://pinecone.io/learn/langchain)\n",
        "\n",
        "# Retrieval Augmentation\n",
        "\n",
        "**L**arge **L**anguage **M**odels (LLMs) have a data freshness problem. The most powerful LLMs in the world, like GPT-4, have no idea about recent world events.\n",
        "\n",
        "The world of LLMs is frozen in time. Their world exists as a static snapshot of the world as it was within their training data.\n",
        "\n",
        "A solution to this problem is *retrieval augmentation*. The idea behind this is that we retrieve relevant information from an external knowledge base and give that information to our LLM. In this notebook we will learn how to do that.\n",
        "\n",
        "To begin, we must install the prerequisite libraries that we will be using in this notebook. If we install all libraries we will find a conflict in the Hugging Face `datasets` library so we must install everything in a specific order like so:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "77aZv4KnQhhY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "13fdbdbf-bcf3-42f0-c491-79c9167856cc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m474.6/474.6 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.7/14.7 MB\u001b[0m \u001b[31m52.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m191.0/191.0 kB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.5/110.5 kB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m17.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m15.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.9/294.9 kB\u001b[0m \u001b[31m28.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.7/89.7 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.7/138.7 kB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m152.0/152.0 kB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.7/2.7 MB\u001b[0m \u001b[31m67.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.4/43.4 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m671.3/671.3 kB\u001b[0m \u001b[31m38.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.7/2.7 MB\u001b[0m \u001b[31m46.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m300.4/300.4 kB\u001b[0m \u001b[31m31.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.3/134.3 kB\u001b[0m \u001b[31m16.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.1/133.1 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m128.7/128.7 kB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m91.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m128.7/128.7 kB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m126.6/126.6 kB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.6/98.6 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m81.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m52.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for crcmod (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for dill (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for hdfs (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for multiprocess (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for docopt (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install -qU \\\n",
        "    datasets==2.12.0 \\\n",
        "    apache_beam \\\n",
        "    mwparserfromhell"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QljkCksDQhhZ"
      },
      "source": [
        "## Building the Knowledge Base"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vvWFDxw-tidd",
        "outputId": "9a920e12-1eb0-4c4d-8814-584fe0f7352e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain\n",
            "  Downloading langchain-0.0.293-py3-none-any.whl (1.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.20)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.8.5)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
            "Collecting dataclasses-json<0.6.0,>=0.5.7 (from langchain)\n",
            "  Downloading dataclasses_json-0.5.14-py3-none-any.whl (26 kB)\n",
            "Collecting langsmith<0.1.0,>=0.0.38 (from langchain)\n",
            "  Downloading langsmith-0.0.38-py3-none-any.whl (38 kB)\n",
            "Requirement already satisfied: numexpr<3.0.0,>=2.8.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.8.5)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.23.5)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.10.12)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.31.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (8.2.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (3.2.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.6.0,>=0.5.7->langchain)\n",
            "  Downloading marshmallow-3.20.1-py3-none-any.whl (49 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.6.0,>=0.5.7->langchain)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (4.5.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2023.7.22)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (2.0.2)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.10/dist-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json<0.6.0,>=0.5.7->langchain) (23.1)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.6.0,>=0.5.7->langchain)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Installing collected packages: mypy-extensions, marshmallow, typing-inspect, langsmith, dataclasses-json, langchain\n",
            "Successfully installed dataclasses-json-0.5.14 langchain-0.0.293 langsmith-0.0.38 marshmallow-3.20.1 mypy-extensions-1.0.0 typing-inspect-0.9.0\n",
            "Collecting pdfminer.six\n",
            "  Downloading pdfminer.six-20221105-py3-none-any.whl (5.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m23.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pypdf\n",
            "  Downloading pypdf-3.16.1-py3-none-any.whl (276 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m276.3/276.3 kB\u001b[0m \u001b[31m31.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from pdfminer.six) (3.2.0)\n",
            "Requirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.10/dist-packages (from pdfminer.six) (41.0.3)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography>=36.0.0->pdfminer.six) (1.15.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six) (2.21)\n",
            "Installing collected packages: pypdf, pdfminer.six\n",
            "Successfully installed pdfminer.six-20221105 pypdf-3.16.1\n"
          ]
        }
      ],
      "source": [
        "# !pip install unstructured pdfminer pdf2image install PIL pdfminer.high_level\n",
        "# !pip install pdfminer\n",
        "!pip install langchain --upgrade\n",
        "!pip install pdfminer.six pypdf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8vnOct3EsW9N"
      },
      "outputs": [],
      "source": [
        "from langchain.document_loaders import UnstructuredPDFLoader, OnlinePDFLoader, PyPDFLoader, PyPDFDirectoryLoader, WebBaseLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i4reP-D8tM61"
      },
      "outputs": [],
      "source": [
        "# loader = UnstructuredPDFLoader(\"./Learning_Apache_Flink.pdf\")\n",
        "# loader = PyPDFLoader(\"Introduction_to_Apache_Flink.pdf\")\n",
        "import os\n",
        "\n",
        "pdf_folder_path = './pdf'\n",
        "# loader = [PyPDFLoader(os.path.join(pdf_folder_path, fn)) for fn in os.listdir(pdf_folder_path)]\n",
        "loader = PyPDFDirectoryLoader(pdf_folder_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B_PCis2ztnWj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a0b52542-f17d-472e-b314-61e46178f579"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pypdf/_cmap.py:177: PdfReadWarning: Advanced encoding /UniGB-UTF16-H not implemented yet\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "data = loader.load()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "69gJqMYLtssR",
        "outputId": "807e9ec1-0b83-4e47-a28a-a28c97d7b549"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "You have 1000 document(s) in your data.\n",
            "There are 1461 characters in your document.\n"
          ]
        }
      ],
      "source": [
        "print(f\"You have {len(data)} document(s) in your data.\")\n",
        "print(f\"There are {len(data[30].page_content)} characters in your document.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-bLU7BgFsDj3",
        "outputId": "ae90f750-f02d-448d-c1e9-9a5f5a0452d8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'list'>\n",
            "<class 'langchain.schema.document.Document'>\n"
          ]
        }
      ],
      "source": [
        "print(type(data))\n",
        "print(type(data[0]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LarkabZgtbhQ",
        "outputId": "697b16a7-15ff-4386-c25a-110bcab32382"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Document(page_content='Figure 1: The execution graph for incremental word\\ncount\\nto maintain the current count for each word as their\\ninternal state.\\n1val env : StreamExecutionEnvironment = ...\\n2env.setParallelism(2)\\n3\\n4val wordStream = env.readTextFile(path)\\n5val countStream = wordStream.groupBy(_).count\\n6countStream.print\\nExample 1: Incremental Word Count\\n3.2 Distributed Dataﬂow Execution\\nWhen a user executes an application all DataStream\\noperators compile into an execution graph that is in\\nprinciple a directed graph G= (T,E), similarly to Na-\\niad [11] where vertices Trepresent tasks and edges E\\nrepresent data channels between tasks. An execution\\ngraph is depicted in Fig. 1 for the incremental word\\ncount example. As shown, every instance of an opera-\\ntor is encapsulated on a respective task. Tasks can be\\nfurther classiﬁed as sources when they have no input\\nchannels and sinks when no output channels are set.\\nFurthermore, Mdenotes the set of all records trans-\\nferred by tasks during their parallel execution. Each\\ntaskt∈Tencapsulates the independent execution of an\\noperator instance and is composed of the following: (1)\\na set of input and output channels: It,Ot⊆E; (2) an op-\\nerator state stand (3) a user deﬁned function (UDF) ft.\\nData ingestion is pull-based : during its execution each\\ntask consumes input records, updates its operator state\\nand generates new records according to its user deﬁned\\nfunction. More speciﬁcally, for each record r∈Mre-\\nceived by a task t∈Ta new state s′\\ntis produced along\\nwith a set of output records D⊆Maccording to its\\nUDF ft:st,r↦→⟨s′\\nt,D⟩.\\n4. Asynchronous Barrier Snapshotting\\nIn order to provide consistent results, distributed pro-\\ncessing systems need to be resilient to task failures.A way of providing this resilience is to periodically\\ncapture snapshots of the execution graph which can\\nbe used later to recover from failures. A snapshot is\\na global state of the execution graph, capturing all nec-\\nessary information to restart the computation from that\\nspeciﬁc execution state.\\n4.1 Problem Deﬁnition\\nWe deﬁne a global snapshot G∗= (T∗,E∗)of an exe-\\ncution graph G= (T,E)as a set of all task and edge\\nstates, T∗andE∗respectively. In more detail, T∗con-\\nsists of all operator states s∗\\nt∈T∗,∀t∈T, while E∗is\\na set of all channel states e∗∈E∗where e∗consists of\\nrecords that are in transit on e.\\nWe require that certain properties hold for each snap-\\nshotG∗in order to guarantee correct results after recov-\\nery such as termination andfeasibility as described by\\nTel [14].\\nTermination guarantees that a snapshot algorithm even-\\ntually ﬁnishes in ﬁnite time after its initiation if all pro-\\ncesses are alive. Feasibility expresses the meaningful-\\nness of a snapshot, i.e. that during the snapshotting pro-\\ncess no information has been lost regarding the com-\\nputation. Formally, this implies that causal order [9] is\\nmaintained in the snapshot such that records delivered\\nin tasks are also sent from the viewpoint of a snapshot.\\n4.2 ABS for Acyclic Dataﬂows\\nIt is feasible to do snapshots without persisting channel\\nstates when the execution is divided into stages. Stages\\ndivide the injected data streams and all associated com-\\nputations into a series of possible executions where all\\nprior inputs and generated outputs have been fully pro-\\ncessed. The set of operator states at the end of a stage\\nreﬂectsg the whole execution history, therefore, it can\\nbe solely used for a snapshot. The core idea behind our\\nalgorithm is to create identical snapshots with staged\\nsnapshotting while keeping a continuous data inges-\\ntion.\\nIn our approach, stages are emulated in a continu-\\nous dataﬂow execution by special barrier markers in-\\njected in the input data streams periodically that are\\npushed throughout the whole execution graph down\\nto the sinks. Global snapshots are incrementally con-\\nstructed as each task receives the barriers indicating ex-\\necution stages. We further make the following assump-\\ntions for our algorithm:', metadata={'source': 'pdf/1506.08603.pdf', 'page': 2})"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "data[2]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MjCaXKpdNGuh"
      },
      "source": [
        "[**RecursiveUrlLoader**](https://https://python.langchain.com/docs/integrations/document_loaders/recursive_url_loader): We may want to process load all URLs under a root directory.\n",
        "\n",
        "**Parameters**\n",
        "*   url: str, the target url to crawl.\n",
        "*   exclude_dirs: Optional[str], webpage directories to exclude.\n",
        "*   use_async: Optional[bool], wether to use async requests, using async requests is usually faster in large tasks. However, async will disable the lazy loading feature(the function still works, but it is not lazy). By default, it is set to False.\n",
        "*   extractor: Optional[Callable[[str], str]], a function to extract the text of the document from the webpage, by default it returns the page as it is. It is recommended to use tools like goose3 and beautifulsoup to extract the text. By default, it just returns the page as it is.\n",
        "*   max_depth: Optional[int] = None, the maximum depth to crawl. By default, it is set to 2. If you need to crawl the whole website, set it to a number that is large enough would simply do the job.\n",
        "*   timeout: Optional[int] = None, the timeout for each request, in the unit of seconds. By default, it is set to 10.\n",
        "*   prevent_outside: Optional[bool] = None, whether to prevent crawling outside the root url. By default, it is set to True."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OwIXQ4q6IO-S",
        "outputId": "1ccaedee-8158-49e3-f2c5-0f2564215364"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-9-b4f3757f9578>:5: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
            "  loader = RecursiveUrlLoader(url=url, max_depth=2, extractor=lambda x: Soup(x, \"html.parser\").text)\n",
            "/usr/local/lib/python3.10/dist-packages/langchain/document_loaders/recursive_url_loader.py:118: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
            "  soup = BeautifulSoup(raw_html, \"html.parser\")\n",
            "/usr/local/lib/python3.10/dist-packages/bs4/builder/__init__.py:545: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "646\n",
            "page_content='\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nLogging | Apache Flink\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nv1.19-SNAPSHOT\\n\\n\\n\\n\\n\\n\\n\\n\\n\\xa0\\xa0Try Flinkâ\\x96¾\\n\\n\\n\\nFirst steps\\n\\n\\nFraud Detection with the DataStream API\\n\\n\\nReal Time Reporting with the Table API\\n\\n\\nFlink Operations Playground\\n\\n\\n\\n\\n\\n\\xa0\\xa0Learn Flinkâ\\x96¾\\n\\n\\n\\nOverview\\n\\n\\nIntro to the DataStream API\\n\\n\\nData Pipelines & ETL\\n\\n\\nStreaming Analytics\\n\\n\\nEvent-driven Applications\\n\\n\\nFault Tolerance\\n\\n\\n\\n\\n\\n\\xa0\\xa0Conceptsâ\\x96¾\\n\\n\\n\\nOverview\\n\\n\\nStateful Stream Processing\\n\\n\\nTimely Stream Processing\\n\\n\\nFlink Architecture\\n\\n\\nGlossary\\n\\n\\n\\n\\n\\n\\n\\xa0\\xa0Application Developmentâ\\x96¾\\n\\n\\n\\n\\nProject Configurationâ\\x96¾\\n\\n\\n\\nOverview\\n\\n\\nUsing Maven\\n\\n\\nUsing Gradle\\n\\n\\nConnectors and Formats\\n\\n\\nTest Dependencies\\n\\n\\nAdvanced Configuration\\n\\n\\n\\n\\n\\nDataStream APIâ\\x96¾\\n\\n\\n\\nOverview\\n\\n\\nExecution Mode (Batch/Streaming)\\n\\n\\n\\nEvent Timeâ\\x96¾\\n\\n\\n\\nGenerating Watermarks\\n\\n\\nBuiltin Watermark Generators\\n\\n\\n\\n\\n\\nState & Fault Toleranceâ\\x96¾\\n\\n\\n\\nWorking with State\\n\\n\\nThe Broadcast State Pattern\\n\\n\\nCheckpointing\\n\\n\\nState Backends\\n\\n\\n\\nData Types & Serializationâ\\x96¾\\n\\n\\n\\nOverview\\n\\n\\nState Schema Evolution\\n\\n\\nCustom State Serialization\\n\\n\\n3rd Party Serializers\\n\\n\\n\\n\\n\\n\\nUser-Defined Functions\\n\\n\\n\\nOperatorsâ\\x96¾\\n\\n\\n\\nOverview\\n\\n\\nWindows\\n\\n\\nJoining\\n\\n\\nProcess Function\\n\\n\\nAsync I/O\\n\\n\\n\\n\\nData Sources\\n\\n\\nSide Outputs\\n\\n\\nHandling Application Parameters\\n\\n\\nTesting\\n\\n\\nExperimental Features\\n\\n\\nScala API Extensions\\n\\n\\nJava Lambda Expressions\\n\\n\\n\\nManaging Executionâ\\x96¾\\n\\n\\n\\nExecution Configuration\\n\\n\\nProgram Packaging\\n\\n\\nParallel Execution\\n\\n\\n\\n\\n\\n\\n\\nTable API & SQLâ\\x96¾\\n\\n\\n\\nOverview\\n\\n\\nConcepts & Common API\\n\\n\\nDataStream API Integration\\n\\n\\n\\nStreaming Conceptsâ\\x96¾\\n\\n\\n\\nOverview\\n\\n\\nDeterminism in Continuous Queries\\n\\n\\nDynamic Tables\\n\\n\\nTime Attributes\\n\\n\\nVersioned Tables\\n\\n\\nTemporal Table Function\\n\\n\\n\\n\\nData Types\\n\\n\\nTime Zone\\n\\n\\nTable API\\n\\n\\n\\nSQLâ\\x96¾\\n\\n\\n\\nSQL\\n\\n\\nGetting Started\\n\\n\\n\\nQueriesâ\\x96¾\\n\\n\\n\\nOverview\\n\\n\\nHints\\n\\n\\nWITH clause\\n\\n\\nSELECT & WHERE\\n\\n\\nSELECT DISTINCT\\n\\n\\nWindowing TVF\\n\\n\\nWindow Aggregation\\n\\n\\nGroup Aggregation\\n\\n\\nOver Aggregation\\n\\n\\nJoins\\n\\n\\nWindow JOIN\\n\\n\\nSet Operations\\n\\n\\nORDER BY clause\\n\\n\\nLIMIT clause\\n\\n\\nTop-N\\n\\n\\nWindow Top-N\\n\\n\\nDeduplication\\n\\n\\nWindow Deduplication\\n\\n\\nPattern Recognition\\n\\n\\nTime Travel\\n\\n\\n\\n\\nCREATE Statements\\n\\n\\nDROP Statements\\n\\n\\nALTER Statements\\n\\n\\nINSERT Statement\\n\\n\\nANALYZE Statements\\n\\n\\nDESCRIBE Statements\\n\\n\\nTRUNCATE Statements\\n\\n\\nEXPLAIN Statements\\n\\n\\nUSE Statements\\n\\n\\nSHOW Statements\\n\\n\\nLOAD Statements\\n\\n\\nUNLOAD Statements\\n\\n\\nSET Statements\\n\\n\\nRESET Statements\\n\\n\\nJAR Statements\\n\\n\\nJOB Statements\\n\\n\\nUPDATE Statements\\n\\n\\nDELETE Statements\\n\\n\\nCALL Statements\\n\\n\\n\\n\\n\\nFunctionsâ\\x96¾\\n\\n\\n\\nOverview\\n\\n\\nSystem (Built-in) Functions\\n\\n\\nUser-defined Functions\\n\\n\\n\\n\\nProcedures\\n\\n\\nModules\\n\\n\\nCatalogs\\n\\n\\nQuickstart for Flink OLAP\\n\\n\\nSQL Client\\n\\n\\nSQL JDBC Driver\\n\\n\\n\\nSQL Gatewayâ\\x96¾\\n\\n\\n\\nOverview\\n\\n\\nREST Endpoint\\n\\n\\nHiveServer2 Endpoint\\n\\n\\n\\n\\n\\nHive Compatibilityâ\\x96¾\\n\\n\\n\\n\\nHive Dialectâ\\x96¾\\n\\n\\n\\nOverview\\n\\n\\n\\nQueriesâ\\x96¾\\n\\n\\n\\nOverview\\n\\n\\nSort/Cluster/Distributed By\\n\\n\\nGroup By\\n\\n\\nJoin\\n\\n\\nSet Operations\\n\\n\\nLateral View Clause\\n\\n\\nWindow Functions\\n\\n\\nSub-Queries\\n\\n\\nCTE\\n\\n\\nTransform Clause\\n\\n\\nTable Sample\\n\\n\\n\\n\\nCREATE Statements\\n\\n\\nDROP Statements\\n\\n\\nALTER Statements\\n\\n\\nINSERT Statements\\n\\n\\nLoad Data Statements\\n\\n\\nSHOW Statements\\n\\n\\nADD Statements\\n\\n\\nSET Statements\\n\\n\\n\\n\\nHiveServer2 Endpoint\\n\\n\\n\\n\\nConfiguration\\n\\n\\nPerformance Tuning\\n\\n\\nUser-defined Sources & Sinks\\n\\n\\n\\n\\n\\nPython APIâ\\x96¾\\n\\n\\n\\nOverview\\n\\n\\nInstallation\\n\\n\\nTable API Tutorial\\n\\n\\nDataStream API Tutorial\\n\\n\\n\\nTable APIâ\\x96¾\\n\\n\\n\\nIntro to the Python Table API\\n\\n\\nTableEnvironment\\n\\n\\n\\nOperationsâ\\x96¾\\n\\n\\n\\nOverview\\n\\n\\nRow-based Operations\\n\\n\\n\\n\\nData Types\\n\\n\\nSystem (Built-in) Functions\\n\\n\\n\\nUser Defined Functionsâ\\x96¾\\n\\n\\n\\nOverview\\n\\n\\nGeneral User-defined Functions\\n\\n\\nVectorized User-defined Functions\\n\\n\\n\\n\\nConversions between PyFlink Table and Pandas DataFrame\\n\\n\\nConversions between Table and DataStream\\n\\n\\nSQL\\n\\n\\nCatalogs\\n\\n\\nMetrics\\n\\n\\nConnectors\\n\\n\\n\\n\\n\\nDataStream APIâ\\x96¾\\n\\n\\n\\nIntro to the Python DataStream API\\n\\n\\n\\nOperatorsâ\\x96¾\\n\\n\\n\\nOverview\\n\\n\\nWindows\\n\\n\\nProcess Function\\n\\n\\n\\n\\nData Types\\n\\n\\nState\\n\\n\\n\\n\\nDependency Management\\n\\n\\nExecution Mode\\n\\n\\nConfiguration\\n\\n\\nDebugging\\n\\n\\nEnvironment Variables\\n\\n\\nFAQ\\n\\n\\n\\n\\n\\n\\n\\n\\xa0\\xa0Librariesâ\\x96¾\\n\\n\\n\\nEvent Processing (CEP)\\n\\n\\nState Processor API\\n\\n\\n\\n\\n\\n\\xa0\\xa0Connectorsâ\\x96¾\\n\\n\\n\\n\\nDataStream Connectorsâ\\x96¾\\n\\n\\n\\nOverview\\n\\n\\nFault Tolerance Guarantees\\n\\n\\n\\nFormatsâ\\x96¾\\n\\n\\n\\nOverview\\n\\n\\nAvro\\n\\n\\nAzure Table storage\\n\\n\\nCSV\\n\\n\\nHadoop\\n\\n\\nJSON\\n\\n\\nParquet\\n\\n\\nText files\\n\\n\\n\\n\\nDataGen\\n\\n\\nKafka\\n\\n\\nCassandra\\n\\n\\nDynamoDB\\n\\n\\nElasticsearch\\n\\n\\nFirehose\\n\\n\\nKinesis\\n\\n\\nMongoDB\\n\\n\\nOpensearch\\n\\n\\nFileSystem\\n\\n\\nRabbitMQ\\n\\n\\nGoogle Cloud PubSub\\n\\n\\nHybrid Source\\n\\n\\nPulsar\\n\\n\\nJDBC\\n\\n\\n\\n\\n\\nTable API Connectorsâ\\x96¾\\n\\n\\n\\nOverview\\n\\n\\n\\nFormatsâ\\x96¾\\n\\n\\n\\nFormats\\n\\n\\nCSV\\n\\n\\nJSON\\n\\n\\nAvro\\n\\n\\nConfluent Avro\\n\\n\\nProtobuf\\n\\n\\nDebezium\\n\\n\\nCanal\\n\\n\\nMaxwell\\n\\n\\nOgg\\n\\n\\nParquet\\n\\n\\nOrc\\n\\n\\nRaw\\n\\n\\n\\n\\nKafka\\n\\n\\nUpsert Kafka\\n\\n\\nDynamoDB\\n\\n\\nFirehose\\n\\n\\nKinesis\\n\\n\\nMongoDB\\n\\n\\nJDBC\\n\\n\\nElasticsearch\\n\\n\\nOpensearch\\n\\n\\nFileSystem\\n\\n\\nHBase\\n\\n\\nDataGen\\n\\n\\nPrint\\n\\n\\nBlackHole\\n\\n\\n\\nHiveâ\\x96¾\\n\\n\\n\\nOverview\\n\\n\\nHive Catalog\\n\\n\\nHive Read & Write\\n\\n\\nHive Functions\\n\\n\\n\\n\\nDownload\\n\\n\\n\\n\\n\\n\\n\\n\\xa0\\xa0Deploymentâ\\x96¾\\n\\n\\n\\nOverview\\n\\n\\n\\nResource Providersâ\\x96¾\\n\\n\\n\\n\\nStandaloneâ\\x96¾\\n\\n\\n\\nOverview\\n\\n\\nWorking Directory\\n\\n\\nDocker\\n\\n\\nKubernetes\\n\\n\\n\\n\\nNative Kubernetes\\n\\n\\nYARN\\n\\n\\n\\n\\nConfiguration\\n\\n\\n\\nMemory Configurationâ\\x96¾\\n\\n\\n\\nSet up Flink\\'s Process Memory\\n\\n\\nSet up TaskManager Memory\\n\\n\\nSet up JobManager Memory\\n\\n\\nMemory Tuning Guide\\n\\n\\nTroubleshooting\\n\\n\\nMigration Guide\\n\\n\\nNetwork Buffer Tuning\\n\\n\\n\\n\\nCommand-Line Interface\\n\\n\\nElastic Scaling\\n\\n\\nFine-Grained Resource Management\\n\\n\\nSpeculative Execution\\n\\n\\n\\nFile Systemsâ\\x96¾\\n\\n\\n\\nOverview\\n\\n\\nCommon Configurations\\n\\n\\nAmazon S3\\n\\n\\nGoogle Cloud Storage\\n\\n\\nAliyun OSS\\n\\n\\nAzure Blob Storage\\n\\n\\nPlugins\\n\\n\\n\\n\\n\\nHigh Availabilityâ\\x96¾\\n\\n\\n\\nOverview\\n\\n\\nZooKeeper HA Services\\n\\n\\nKubernetes HA Services\\n\\n\\n\\n\\nMetric Reporters\\n\\n\\n\\nSecurityâ\\x96¾\\n\\n\\n\\nSSL Setup\\n\\n\\nKerberos\\n\\n\\nDelegation tokens\\n\\n\\n\\n\\n\\nREPLsâ\\x96¾\\n\\n\\n\\nPython REPL\\n\\n\\n\\n\\n\\nAdvancedâ\\x96¾\\n\\n\\n\\nExternal Resources\\n\\n\\nHistory Server\\n\\n\\nLogging\\n\\n\\nFailure Enrichers\\n\\n\\n\\n\\n\\n\\n\\n\\xa0\\xa0Operationsâ\\x96¾\\n\\n\\n\\n\\nState & Fault Toleranceâ\\x96¾\\n\\n\\n\\nCheckpoints\\n\\n\\nCheckpointing under backpressure\\n\\n\\nSavepoints\\n\\n\\nCheckpoints vs. Savepoints\\n\\n\\nState Backends\\n\\n\\nTuning Checkpoints and Large State\\n\\n\\nTask Failure Recovery\\n\\n\\n\\n\\nMetrics\\n\\n\\nREST API\\n\\n\\n\\nBatchâ\\x96¾\\n\\n\\n\\nBatch Shuffle\\n\\n\\n\\n\\n\\nDebuggingâ\\x96¾\\n\\n\\n\\nDebugging Windows & Event Time\\n\\n\\nDebugging Classloading\\n\\n\\nFlame Graphs\\n\\n\\nApplication Profiling & Debugging\\n\\n\\n\\n\\n\\nMonitoringâ\\x96¾\\n\\n\\n\\nMonitoring Checkpointing\\n\\n\\nMonitoring Back Pressure\\n\\n\\n\\n\\nUpgrading Applications and Flink Versions\\n\\n\\nProduction Readiness Checklist\\n\\n\\n\\n\\n\\n\\n\\xa0\\xa0Flink Developmentâ\\x96¾\\n\\n\\n\\nImporting Flink into an IDE\\n\\n\\nBuilding Flink from Source\\n\\n\\n\\n\\n\\n\\xa0\\xa0Internalsâ\\x96¾\\n\\n\\n\\nJobs and Scheduling\\n\\n\\nTask Lifecycle\\n\\n\\nFile Systems\\n\\n\\n\\n\\n\\n\\n Project Homepage\\n\\n JavaDocs\\n\\n ScalaDocs\\n\\n PyDocs\\n\\n\\n\\n\\n\\n\\nPick Docs Version\\nâ\\x96¾\\n\\n\\n\\n      1.19-SNAPSHOT (â\\x9c\\x93)\\n    \\n\\n\\n\\n          v1.17\\n        \\n\\n\\n\\n          v1.16\\n        \\n\\n\\n\\n\\n          All Versions\\n      \\n\\n\\n\\n\\n\\xa0\\xa0\\n  ä¸\\xadæ\\x96\\x87ç\\x89\\x88\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nLogging\\n\\n\\n\\n\\n\\nOn This Page \\n\\nConfiguring Log4j 2\\n\\nCompatibility with Log4j 1\\n\\n\\nConfiguring Log4j1\\nConfiguring logback\\nBest practices for developers\\n\\n\\n\\n\\n\\n\\n        This documentation is for an unreleased version of Apache Flink. We recommend you use the latest stable version.\\n    \\n\\n\\n\\n  How to use logging\\n  #\\n\\nAll Flink processes create a log text file that contains messages for various events happening in that process.\\nThese logs provide deep insights into the inner workings of Flink, and can be used to detect problems (in the form of WARN/ERROR messages) and can help in debugging them.\\nThe log files can be accessed via the Job-/TaskManager pages of the WebUI. The used Resource Provider (e.g., YARN) may provide additional means of accessing them.\\nThe logging in Flink uses the SLF4J logging interface.\\nThis allows you to use any logging framework that supports SLF4J, without having to modify the Flink source code.\\nBy default, Log4j 2 is used as the underlying logging framework.\\n\\n  Configuring Log4j 2\\n  #\\n\\nLog4j 2 is controlled using property files.\\nThe Flink distribution ships with the following log4j properties files in the conf directory, which are used automatically if Log4j 2 is enabled:\\n\\nlog4j-cli.properties: used by the command line interface (e.g., flink run)\\nlog4j-session.properties: used by the command line interface when starting a Kubernetes/Yarn session cluster (i.e., kubernetes-session.sh/yarn-session.sh)\\nlog4j-console.properties: used for Job-/TaskManagers if they are run in the foreground (e.g., Kubernetes)\\nlog4j.properties: used for Job-/TaskManagers by default\\n\\nLog4j periodically scans this file for changes and adjusts the logging behavior if necessary.\\nBy default this check happens every 30 seconds and is controlled by the monitorInterval setting in the Log4j properties files.\\n\\n  Compatibility with Log4j 1\\n  #\\n\\nFlink ships with the Log4j API bridge, allowing existing applications that work against Log4j1 classes to continue working.\\nIf you have custom Log4j 1 properties files or code that relies on Log4j 1, please check out the official Log4j compatibility and migration guides.\\n\\n  Configuring Log4j1\\n  #\\n\\nTo use Flink with Log4j 1 you must ensure that:\\n\\norg.apache.logging.log4j:log4j-core, org.apache.logging.log4j:log4j-slf4j-impl and org.apache.logging.log4j:log4j-1.2-api are not on the classpath,\\nlog4j:log4j, org.slf4j:slf4j-log4j12, org.apache.logging.log4j:log4j-to-slf4j and org.apache.logging.log4j:log4j-api are on the classpath.\\n\\nIn the IDE this means you have to replace such dependencies defined in your pom, and possibly add exclusions on dependencies that transitively depend on them.\\nFor Flink distributions this means you have to\\n\\nremove the log4j-core, log4j-slf4j-impl and log4j-1.2-api jars from the lib directory,\\nadd the log4j, slf4j-log4j12 and log4j-to-slf4j jars to the lib directory,\\nreplace all log4j properties files in the conf directory with Log4j1-compliant versions.\\n\\n\\n  Configuring logback\\n  #\\n\\nTo use Flink with logback you must ensure that:\\n\\norg.apache.logging.log4j:log4j-slf4j-impl is not on the classpath,\\nch.qos.logback:logback-core and ch.qos.logback:logback-classic are on the classpath.\\n\\nIn the IDE this means you have to replace such dependencies defined in your pom, and possibly add exclusions on dependencies that transitively depend on them.\\nFor Flink distributions this means you have to\\n\\nremove the log4j-slf4j-impl jar from the lib directory,\\nadd the logback-core, and logback-classic jars to the lib directory.\\n\\nThe Flink distribution ships with the following logback configuration files in the conf directory, which are used automatically if logback is enabled:\\n\\nlogback-session.properties: used by the command line interface when starting a Kubernetes/Yarn session cluster (i.e., kubernetes-session.sh/yarn-session.sh)\\nlogback-console.properties: used for Job-/TaskManagers if they are run in the foreground (e.g., Kubernetes)\\nlogback.xml: used for command line interface and Job-/TaskManagers by default\\n\\n\\n  Logback 1.3+ requires SLF4J 2, which is currently not supported.\\n\\n\\n  Best practices for developers\\n  #\\n\\nYou can create an SLF4J logger by calling org.slf4j.LoggerFactory#LoggerFactory.getLogger with the Class of your class as an argument.\\nWe highly recommend storing this logger in a private static final field.\\nimport org.slf4j.Logger;\\nimport org.slf4j.LoggerFactory;\\n\\npublic class Foobar {\\n\\tprivate static final Logger LOG = LoggerFactory.getLogger(Foobar.class);\\n\\n\\tpublic static void main(String[] args) {\\n\\t\\tLOG.info(\"Hello world!\");\\n\\t}\\n}\\nIn order to benefit most from SLF4J, it is recommended to use its placeholder mechanism.\\nUsing placeholders allows avoiding unnecessary string constructions in case that the logging level is set so high that the message would not be logged.\\nThe syntax of placeholders is the following:\\nLOG.info(\"This message contains {} placeholders. {}\", 2, \"Yippie\");\\nPlaceholders can also be used in conjunction with exceptions which shall be logged.\\ncatch(Exception exception){\\n\\tLOG.error(\"An {} occurred.\", \"error\", exception);\\n}\\n\\n Back to top\\n\\n\\nWant to contribute translation?\\n\\nEdit This Page\\n\\n\\n\\n\\n\\n\\nOn This Page \\n\\nConfiguring Log4j 2\\n\\nCompatibility with Log4j 1\\n\\n\\nConfiguring Log4j1\\nConfiguring logback\\nBest practices for developers\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n' metadata={'source': 'https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/advanced/logging/', 'title': 'Logging | Apache Flink', 'description': 'How to use logging # All Flink processes create a log text file that contains messages for various events happening in that process. These logs provide deep insights into the inner workings of Flink, and can be used to detect problems (in the form of WARN/ERROR messages) and can help in debugging them.\\nThe log files can be accessed via the Job-/TaskManager pages of the WebUI. The used Resource Provider (e.', 'language': 'en'}\n"
          ]
        }
      ],
      "source": [
        "# from langchain.document_loaders.recursive_url_loader import RecursiveUrlLoader\n",
        "\n",
        "# from bs4 import BeautifulSoup as Soup\n",
        "# url = \"https://nightlies.apache.org/flink/flink-docs-master/\"\n",
        "# loader = RecursiveUrlLoader(url=url, max_depth=2, extractor=lambda x: Soup(x, \"html.parser\").text)\n",
        "# webs2 = loader.load()\n",
        "# print(len(webs2))\n",
        "# print(webs2[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nYnyHfLOPOjL",
        "outputId": "d4cc6ce3-e359-4d49-b5cc-0218d4686f61"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'list'>\n",
            "<class 'langchain.schema.document.Document'>\n"
          ]
        }
      ],
      "source": [
        "print(type(webs2))\n",
        "print(type(webs2[0]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JvI2UkV-Nz6y"
      },
      "source": [
        "[**URLLoader**](https://python.langchain.com/docs/integrations/document_loaders/url):\n",
        "This covers how to load HTML documents from a list of URLs into a document format that we can use downstream.\n",
        "Note that the data format could be different from WebBaseLoader."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nyl_kkw-NzP6"
      },
      "outputs": [],
      "source": [
        "from langchain.document_loaders import UnstructuredURLLoader\n",
        "\n",
        "# This is for testing source in GPT.\n",
        "urls = [\n",
        "    \"https://www.understandingwar.org/backgrounder/russian-offensive-campaign-assessment-february-8-2023\",\n",
        "    \"https://www.understandingwar.org/backgrounder/russian-offensive-campaign-assessment-february-9-2023\",\n",
        "]\n",
        "loader = UnstructuredURLLoader(urls=urls)\n",
        "webs3 = loader.load()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9iXFzVPrOhIy"
      },
      "source": [
        "[**WebBaseLoader**](https://python.langchain.com/docs/integrations/document_loaders/web_base): This covers how to use WebBaseLoader to load all text from HTML webpages into a document format that we can use downstream. For more custom logic for loading webpages look at some child class examples such as IMSDbLoader, AZLyricsLoader, and CollegeConfidentialLoader.\n",
        "\n",
        "*   Load HTML pages using urllib and parse them with `BeautifulSoup’.\n",
        "*   Initialize with webpage path."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lLfKEefMfvhn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5c2ef263-074b-49da-af3e-28a4cc3857bc"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(page_content='\\n\\n\\n\\nApache Flink Home - Apache Flink - Apache Software Foundation\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nLog in\\nSkip to sidebar\\nSkip to main content\\nLinked ApplicationsLoading...Apache Software Foundation\\n\\n\\nSpaces\\n\\n\\n\\n\\n\\n\\n\\nHit enter to search\\n\\n\\n\\nHelp\\n\\n\\n\\n\\n\\n\\n        Online Help\\n\\n\\n\\n\\n        Keyboard Shortcuts\\n\\n\\n\\n\\n        Feed Builder\\n\\n\\n\\n\\n        What’s new\\n\\n\\n\\n\\n        What’s new\\n\\n\\n\\n\\n        Available Gadgets\\n\\n\\n\\n\\n        About Confluence\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n        Log in\\n\\n\\n\\n\\n        Sign up\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nApache FlinkPage tree\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nBrowse pagesConfigureSpace tools\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n \\n\\n\\n\\n\\n\\n                                Attachments (0)\\n             \\n\\n\\n\\n\\n                                Page History\\n             \\n\\n\\n\\n\\n\\n\\n\\n\\n                                Resolved comments\\n             \\n\\n\\n\\n\\n                                Page Information\\n             \\n\\n\\n\\n\\n                                View in Hierarchy\\n             \\n\\n\\n\\n\\n                                View Source\\n             \\n\\n\\n\\n\\n                                Delete comments\\n             \\n\\n\\n\\n\\n                                Export to PDF\\n             \\n\\n\\n\\n\\n                                Export to Word\\n             \\n\\n\\n\\n\\n\\n\\n\\n\\n                                Copy Page Tree\\n             \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nPages\\n\\n\\n\\nJira links\\n\\nApache Flink Home\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n            \\n        \\n    \\n    \\n        \\n    \\n        \\n            \\n            Created by  Gavin McDonald, last modified by  Chesnay Schepler on Nov 25, 2022\\n\\n\\n\\n\\n\\n\\n\\n\\nWelcome to Apache Flink (http://flink.apache.org).To get edit permission to edit this Wiki, please send a mail to\\xa0dev@flink.apache.org\\xa0(make sure you are subscribed), or to one of the committers.\\n\\n\\n\\n\\n\\nBylaws Flink Bylaws\\xa0defines the bylaws under which Apache Flink operates. If you have any suggestions to the bylaws, please add them to the\\xa0Ideas for Bylaw changes.\\n\\n\\n\\n\\n\\nRoadmaps and Feature Designs FLIP - Flink Improvement Proposals  \\xa0are both proposals and design docs for all bigger changes in Flink. The community asks for a FLIP whenever public APIs or behavior are changed, or a new feature is adopted that is more than a trivial extension of an existing feature.The  Apache Flink Roadmap  is on the main website and periodically updated with the active FLIPs and JIRA issues.\\n\\n\\n\\n\\n\\nUsersGeneral information for and about Apache Flink users Powered By Flink   \\xa0\\xa0- A (non-complete) list of Apache Flink usersPresentationsFlink Forward San Francisco, 2018: [slides & videos]Flink Forward Berlin, 2017:\\xa0[slides],\\xa0[videos]Flink Forward San Francisco, 2017:\\xa0[slides],\\xa0[videos]Flink Forward Berlin, 2016:\\xa0[slides & videos]Flink Forward Berlin, 2015:\\xa0[slides & videos] Various PresentationsHow-ToRemote Debugging of Flink Clusters\\n\\n\\n\\nContributorsInformation for contributors and those that want to start contributingDevelopment ProcessApache Flink development guidelinesScala coding guidelinesPull Request management (2019)Flink Jira ProcessChinese Translation SpecificationsHow-TosWriting a Blog Post\\n\\n\\n\\nCommittersGeneral Information for CommittersInfrastructureWebsiteSnapshotsDocumentationContinuous IntegrationDependenciesLicensingContinuous Benchmarks (CodeSpeed)Development processExternalized Connector developmentMerging Pull RequestsRelease ProcessCreating a Flink ReleaseVerifying a Flink ReleaseCreating a flink-connector releaseCreating a Flink Stateful Functions ReleaseCreating a flink-shaded releaseVerifying a flink-shaded releaseReleasesFlink Release ManagementRelease Management and Feature PlanTime-based releases definition\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n            No labels\\n        \\n\\n\\n\\n\\n\\n\\n\\n\\nOverview\\nContent Tools\\nApps\\n\\n\\n\\n\\n\\n                    Powered by a free Atlassian Confluence Open Source Project License granted to Apache Software Foundation. Evaluate Confluence today.\\n\\n\\nPowered by Atlassian Confluence 7.19.4\\nPrinted by Atlassian Confluence 7.19.4\\nReport a bug\\nAtlassian News\\n\\nAtlassian\\n\\n\\n\\n\\n\\n{\"serverDuration\": 239, \"requestCorrelationId\": \"ab7ab80e6c37110f\"}\\n\\n\\n', metadata={'source': 'https://cwiki.apache.org/confluence/display/FLINK/Apache+Flink+Home', 'title': 'Apache Flink Home - Apache Flink - Apache Software Foundation', 'language': 'en-US'}),\n",
              " Document(page_content='\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nJavaScript is not available.\\nWe’ve detected that JavaScript is disabled in this browser. Please enable JavaScript or switch to a supported browser to continue using twitter.com. You can see a list of supported browsers in our Help Center.\\nHelp Center\\n\\nTerms of Service\\nPrivacy Policy\\nCookie Policy\\nImprint\\nAds info\\n      © 2023 X Corp.\\n    \\nSomething went wrong, but don’t fret — let’s give it another shot.Try again', metadata={'source': 'https://twitter.com/ApacheFlink', 'language': 'en'}),\n",
              " Document(page_content='Skip to content', metadata={'source': 'https://www.meetup.com/topics/apache-flink/', 'language': 'en-US'}),\n",
              " Document(page_content=\"\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nFlink Blog | Apache Flink\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nWhat is Apache Flink?▾\\n\\n\\n\\n\\nArchitecture\\n\\n\\n\\n\\nApplications\\n\\n\\n\\n\\nOperations\\n\\n\\n\\n\\nWhat is Stateful Functions? \\n\\n\\nWhat is Flink ML? \\n\\n\\nWhat is the Flink Kubernetes Operator? \\n\\n\\nWhat is Flink Table Store? \\n\\n\\nUse Cases\\n\\n\\nPowered By\\n\\n\\n\\nDownloads\\n\\n\\nGetting Started▾\\n\\n\\n\\n\\nWith Flink \\n\\n\\n\\n\\nWith Flink Stateful Functions \\n\\n\\n\\n\\nWith Flink ML \\n\\n\\n\\n\\nWith Flink Kubernetes Operator \\n\\n\\n\\n\\nWith Flink Table Store \\n\\n\\n\\n\\nTraining Course \\n\\n\\n\\n\\nDocumentation▾\\n\\n\\n\\n\\nFlink 1.17 (stable) \\n\\n\\n\\n\\nFlink Master (snapshot) \\n\\n\\n\\n\\nStateful Functions 3.2 (stable) \\n\\n\\n\\n\\nStateful Functions Master (snapshot) \\n\\n\\n\\n\\nML 2.3 (stable) \\n\\n\\n\\n\\nML Master (snapshot) \\n\\n\\n\\n\\nKubernetes Operator 1.6 (latest) \\n\\n\\n\\n\\nKubernetes Operator Main (snapshot) \\n\\n\\n\\n\\nTable Store 0.3 (stable) \\n\\n\\n\\n\\nTable Store Master (snapshot) \\n\\n\\n\\n\\nGetting Help\\n\\n\\nflink-packages.org \\n\\n\\n\\nCommunity & Project Info\\n\\n\\nRoadmap\\n\\n\\nHow to Contribute▾\\n\\n\\n\\n\\nOverview\\n\\n\\n\\n\\nContribute Code\\n\\n\\n\\n\\nReview Pull Requests\\n\\n\\n\\n\\nCode Style and Quality Guide\\n\\n\\n\\n\\nContribute Documentation\\n\\n\\n\\n\\nDocumentation Style Guide\\n\\n\\n\\n\\nContribute to the Website\\n\\n\\n\\n\\nSecurity\\n\\n\\n\\nFlink Blog\\n\\n\\n\\n\\nFlink on GitHub \\n\\n\\n@ApacheFlink \\n\\n\\n\\n\\n\\n\\nApache Software Foundation \\n\\n\\n\\n\\n\\nLicense \\n\\n\\n\\nSecurity \\n\\n\\n\\n\\n\\nDonate \\n\\n\\n\\nThanks \\n\\n\\n\\n\\n\\n\\xa0\\xa0\\n  中文版\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nFlink Blog\\n\\n\\n\\n\\n\\n\\n\\n\\nApache Flink Kubernetes Operator 1.6.0 Release Announcement\\n\\n\\nAnnouncing three new Apache Flink connectors, the new connector versioning strategy and externalization\\n\\n\\nSIGMOD Systems Award for Apache Flink\\n\\n\\nApache Flink 1.16.2 Release Announcement\\n\\n\\nApache Flink 1.17.1 Release Announcement\\n\\n\\nApache Flink Kubernetes Operator 1.5.0 Release Announcement\\n\\n\\nHowto test a batch source with the new Source framework\\n\\n\\nHowto migrate a real-life batch pipeline from the DataSet API to the DataStream API\\n\\n\\nHowto create a batch source with the new Source framework\\n\\n\\nApache Flink ML 2.2.0 Release Announcement\\n\\n\\nAnnouncing the Release of Apache Flink 1.17\\n\\n\\nApache Flink 1.15.4 Release Announcement\\n\\n\\nApache Flink Kubernetes Operator 1.4.0 Release Announcement\\n\\n\\nApache Flink 1.16.1 Release Announcement\\n\\n\\nDelegation Token Framework: Obtain, Distribute and Use Temporary Credentials Automatically\\n\\n\\nApache Flink Table Store 0.3.0 Release Announcement\\n\\n\\nApache Flink Kubernetes Operator 1.3.1 Release Announcement\\n\\n\\nApache Flink Kubernetes Operator 1.3.0 Release Announcement\\n\\n\\nOptimising the throughput of async sinks using a custom RateLimitingStrategy\\n\\n\\nApache Flink 1.15.3 Release Announcement\\n\\n\\nAnnouncing the Release of Apache Flink 1.16\\n\\n\\nApache Flink Table Store 0.2.1 Release Announcement\\n\\n\\nApache Flink Kubernetes Operator 1.2.0 Release Announcement\\n\\n\\nApache Flink 1.14.6 Release Announcement\\n\\n\\nRegarding Akka's licensing change\\n\\n\\nApache Flink Table Store 0.2.0 Release Announcement\\n\\n\\nApache Flink 1.15.2 Release Announcement\\n\\n\\nApache Flink Kubernetes Operator 1.1.0 Release Announcement\\n\\n\\nApache Flink ML 2.1.0 Release Announcement\\n\\n\\nFLIP-147: Support Checkpoints After Tasks Finished - Part One\\n\\n\\nFLIP-147: Support Checkpoints After Tasks Finished - Part Two\\n\\n\\nApache Flink 1.15.1 Release Announcement\\n\\n\\nApache Flink 1.14.5 Release Announcement\\n\\n\\nAdaptive Batch Scheduler: Automatically Decide Parallelism of Flink Batch Jobs\\n\\n\\nApache Flink Kubernetes Operator 1.0.0 Release Announcement\\n\\n\\nImproving speed and stability of checkpointing with generic log-based incremental checkpoints\\n\\n\\nGetting into Low-Latency Gears with Apache Flink - Part Two\\n\\n\\nGetting into Low-Latency Gears with Apache Flink - Part One\\n\\n\\nApache Flink Table Store 0.1.0 Release Announcement\\n\\n\\nExploring the thread mode in PyFlink\\n\\n\\nImprovements to Flink operations: Snapshots Ownership and Savepoint Formats\\n\\n\\nAnnouncing the Release of Apache Flink 1.15\\n\\n\\nApache Flink Kubernetes Operator 0.1.0 Release Announcement\\n\\n\\nThe Generic Asynchronous Base Sink\\n\\n\\nApache Flink 1.14.4 Release Announcement\\n\\n\\nScala Free in One Fifteen\\n\\n\\nApache Flink 1.13.6 Release Announcement\\n\\n\\nStateful Functions 3.2.0 Release Announcement\\n\\n\\nPravega Flink Connector 101\\n\\n\\nApache Flink 1.14.3 Release Announcement\\n\\n\\nApache Flink ML 2.0.0 Release Announcement\\n\\n\\nHow We Improved Scheduler Performance for Large-scale Jobs - Part One\\n\\n\\nHow We Improved Scheduler Performance for Large-scale Jobs - Part Two\\n\\n\\nApache Flink StateFun Log4j emergency release\\n\\n\\nApache Flink Log4j emergency releases\\n\\n\\nAdvise on Apache Log4j Zero Day (CVE-2021-44228)\\n\\n\\nFlink Backward - The Apache Flink Retrospective\\n\\n\\nSort-Based Blocking Shuffle Implementation in Flink - Part One\\n\\n\\nSort-Based Blocking Shuffle Implementation in Flink - Part Two\\n\\n\\nApache Flink 1.13.3 Released\\n\\n\\nApache Flink 1.14.0 Release Announcement\\n\\n\\nImplementing a Custom Source Connector for Table API and SQL - Part One \\n\\n\\nImplementing a custom source connector for Table API and SQL - Part Two \\n\\n\\nStateful Functions 3.1.0 Release Announcement\\n\\n\\nHelp us stabilize Apache Flink 1.14.0 RC0\\n\\n\\nApache Flink 1.11.4 Released\\n\\n\\nApache Flink 1.12.5 Released\\n\\n\\nApache Flink 1.13.2 Released\\n\\n\\nHow to identify the source of backpressure?\\n\\n\\nApache Flink 1.13.1 Released\\n\\n\\nApache Flink 1.12.4 Released\\n\\n\\nScaling Flink automatically with Reactive Mode\\n\\n\\nApache Flink 1.13.0 Release Announcement\\n\\n\\nApache Flink 1.12.3 Released\\n\\n\\nStateful Functions 3.0.0: Remote Functions Front and Center\\n\\n\\nA Rundown of Batch Execution Mode in the DataStream API\\n\\n\\nApache Flink 1.12.2 Released\\n\\n\\nHow to natively deploy Flink on Kubernetes with High-Availability (HA)\\n\\n\\nApache Flink 1.10.3 Released\\n\\n\\nApache Flink 1.12.1 Released\\n\\n\\nUsing RocksDB State Backend in Apache Flink: When and How\\n\\n\\nExploring fine-grained recovery of bounded data sets on Flink\\n\\n\\nWhat's New in the Pulsar Flink Connector 2.7.0\\n\\n\\nStateful Functions 2.2.2 Release Announcement\\n\\n\\nApache Flink 1.11.3 Released\\n\\n\\nApache Flink 1.12.0 Release Announcement\\n\\n\\nImprovements in task scheduling for batch workloads in Apache Flink 1.12\\n\\n\\nStateful Functions 2.2.1 Release Announcement\\n\\n\\nFrom Aligned to Unaligned Checkpoints - Part 1: Checkpoints, Alignment, and Backpressure\\n\\n\\nStateful Functions Internals: Behind the scenes of Stateful Serverless\\n\\n\\nStateful Functions 2.2.0 Release Announcement\\n\\n\\nApache Flink 1.11.2 Released\\n\\n\\nFlink Community Update - August'20\\n\\n\\nMemory Management improvements for Flink’s JobManager in Apache Flink 1.11\\n\\n\\nApache Flink 1.10.2 Released\\n\\n\\nThe State of Flink on Docker\\n\\n\\nMonitoring and Controlling Networks of IoT Devices with Flink Stateful Functions\\n\\n\\nAccelerating your workload with GPU and other external resources\\n\\n\\nPyFlink: The integration of Pandas into PyFlink\\n\\n\\nAdvanced Flink Application Patterns Vol.3: Custom Window Processing\\n\\n\\nFlink Community Update - July'20\\n\\n\\nFlink SQL Demo: Building an End-to-End Streaming Application\\n\\n\\nSharing is caring - Catalogs in Flink SQL\\n\\n\\nApache Flink 1.11.1 Released\\n\\n\\nApplication Deployment in Flink: Current State and the new Application Mode\\n\\n\\nApache Flink 1.11.0 Release Announcement\\n\\n\\nFlink on Zeppelin Notebooks for Interactive Data Analysis - Part 2\\n\\n\\nFlink on Zeppelin Notebooks for Interactive Data Analysis - Part 1\\n\\n\\nFlink Community Update - June'20\\n\\n\\nStateful Functions 2.1.0 Release Announcement\\n\\n\\nApache Flink 1.10.1 Released\\n\\n\\nFlink Community Update - May'20\\n\\n\\nApplying to Google Season of Docs 2020\\n\\n\\nApache Flink 1.9.3 Released\\n\\n\\nMemory Management Improvements with Apache Flink 1.10\\n\\n\\nFlink Serialization Tuning Vol. 1: Choosing your Serializer — if you can\\n\\n\\nPyFlink: Introducing Python Support for UDFs in Flink's Table API\\n\\n\\nStateful Functions 2.0 - An Event-driven Database on Apache Flink\\n\\n\\nFlink Community Update - April'20\\n\\n\\nFlink as Unified Engine for Modern Data Warehousing: Production-Ready Hive Integration\\n\\n\\nAdvanced Flink Application Patterns Vol.2: Dynamic Updates of Application Logic\\n\\n\\nApache Beam: How Beam Runs on Top of Flink\\n\\n\\nNo Java Required: Configuring Sources and Sinks in SQL\\n\\n\\nApache Flink 1.10.0 Release Announcement\\n\\n\\nA Guide for Unit Testing in Apache Flink\\n\\n\\nApache Flink 1.9.2 Released\\n\\n\\nState Unlocked: Interacting with State in Apache Flink\\n\\n\\nAdvanced Flink Application Patterns Vol.1: Case Study of a Fraud Detection System\\n\\n\\nApache Flink 1.8.3 Released\\n\\n\\nHow to query Pulsar Streams using Apache Flink\\n\\n\\nRunning Apache Flink on Kubernetes with KUDO\\n\\n\\nApache Flink 1.9.1 Released\\n\\n\\nThe State Processor API: How to Read, write and modify the state of Flink applications\\n\\n\\nApache Flink 1.8.2 Released\\n\\n\\nFlink Community Update - September'19\\n\\n\\nApache Flink 1.9.0 Release Announcement\\n\\n\\nFlink Network Stack Vol. 2: Monitoring, Metrics, and that Backpressure Thing\\n\\n\\nApache Flink 1.8.1 Released\\n\\n\\nA Practical Guide to Broadcast State in Apache Flink\\n\\n\\nA Deep-Dive into Flink's Network Stack\\n\\n\\nState TTL in Flink 1.8.0: How to Automatically Cleanup Application State in Apache Flink\\n\\n\\nFlux capacitor, huh? Temporal Tables and Joins in Streaming SQL\\n\\n\\nWhen Flink & Pulsar Come Together\\n\\n\\nApache Flink's Application to Season of Docs\\n\\n\\nApache Flink 1.8.0 Release Announcement\\n\\n\\nFlink and Prometheus: Cloud-native monitoring of streaming applications\\n\\n\\nWhat to expect from Flink Forward San Francisco 2019\\n\\n\\nApache Flink 1.6.4 Released\\n\\n\\nMonitoring Apache Flink Applications 101\\n\\n\\nApache Flink 1.7.2 Released\\n\\n\\nBatch as a Special Case of Streaming and Alibaba's contribution of Blink\\n\\n\\nApache Flink 1.5.6 Released\\n\\n\\nApache Flink 1.6.3 Released\\n\\n\\nApache Flink 1.7.1 Released\\n\\n\\nApache Flink 1.7.0 Release Announcement\\n\\n\\nApache Flink 1.5.5 Released\\n\\n\\nApache Flink 1.6.2 Released\\n\\n\\nApache Flink 1.5.4 Released\\n\\n\\nApache Flink 1.6.1 Released\\n\\n\\nApache Flink 1.5.3 Released\\n\\n\\nApache Flink 1.6.0 Release Announcement\\n\\n\\nApache Flink 1.5.2 Released\\n\\n\\nApache Flink 1.5.1 Released\\n\\n\\nApache Flink 1.5.0 Release Announcement\\n\\n\\nApache Flink 1.3.3 Released\\n\\n\\nApache Flink 1.4.2 Released\\n\\n\\nAn Overview of End-to-End Exactly-Once Processing in Apache Flink (with Apache Kafka, too!)\\n\\n\\nApache Flink 1.4.1 Released\\n\\n\\nManaging Large State in Apache Flink: An Intro to Incremental Checkpointing\\n\\n\\nApache Flink in 2017: Year in Review\\n\\n\\nApache Flink 1.4.0 Release Announcement\\n\\n\\nLooking Ahead to Apache Flink 1.4.0 and 1.5.0\\n\\n\\nApache Flink 1.3.2 Released\\n\\n\\nA Deep Dive into Rescalable State in Apache Flink\\n\\n\\nApache Flink 1.3.1 Released\\n\\n\\nApache Flink 1.3.0 Release Announcement\\n\\n\\nIntroducing Docker Images for Apache Flink\\n\\n\\nApache Flink 1.2.1 Released\\n\\n\\nContinuous Queries on Dynamic Tables\\n\\n\\nFrom Streams to Tables and Back Again: An Update on Flink's Table & SQL API\\n\\n\\nApache Flink 1.1.5 Released\\n\\n\\nAnnouncing Apache Flink 1.2.0\\n\\n\\nApache Flink 1.1.4 Released\\n\\n\\nApache Flink in 2016: Year in Review\\n\\n\\nApache Flink 1.1.3 Released\\n\\n\\nApache Flink 1.1.2 Released\\n\\n\\nFlink Forward 2016: Announcing Schedule, Keynotes, and Panel Discussion\\n\\n\\nAnnouncing Apache Flink 1.1.0\\n\\n\\nFlink 1.1.1 Released\\n\\n\\nStream Processing for Everyone with SQL and Apache Flink\\n\\n\\nFlink 1.0.3 Released\\n\\n\\nFlink 1.0.2 Released\\n\\n\\nFlink Forward 2016 Call for Submissions Is Now Open\\n\\n\\nIntroducing Complex Event Processing (CEP) with Apache Flink\\n\\n\\nFlink 1.0.1 Released\\n\\n\\nAnnouncing Apache Flink 1.0.0\\n\\n\\nFlink 0.10.2 Released\\n\\n\\nFlink 2015: A year in review, and a lookout to 2016\\n\\n\\nStorm Compatibility in Apache Flink: How to run existing Storm topologies on Flink\\n\\n\\nIntroducing Stream Windows in Apache Flink\\n\\n\\nFlink 0.10.1 released\\n\\n\\nAnnouncing Apache Flink 0.10.0\\n\\n\\nOff-heap Memory in Apache Flink and the curious JIT compiler\\n\\n\\nAnnouncing Flink Forward 2015\\n\\n\\nApache Flink 0.9.1 available\\n\\n\\nIntroducing Gelly: Graph Processing with Apache Flink\\n\\n\\nAnnouncing Apache Flink 0.9.0\\n\\n\\nApril 2015 in the Flink community\\n\\n\\nJuggling with Bits and Bytes\\n\\n\\nAnnouncing Flink 0.9.0-milestone1 preview release\\n\\n\\nMarch 2015 in the Flink community\\n\\n\\nPeeking into Apache Flink's Engine Room\\n\\n\\nFebruary 2015 in the Flink community\\n\\n\\nIntroducing Flink Streaming\\n\\n\\nJanuary 2015 in the Flink community\\n\\n\\nApache Flink 0.8.0 available\\n\\n\\nDecember 2014 in the Flink community\\n\\n\\nHadoop Compatibility in Flink\\n\\n\\nApache Flink 0.7.0 available\\n\\n\\nUpcoming Events\\n\\n\\nApache Flink 0.6.1 available\\n\\n\\nApache Flink 0.6 available\\n\\n\\n\\n\\n\\n\\n\\nApache Flink Kubernetes Operator 1.6.0 Release Announcement\\n\\n    \\n  August 15, 2023 -\\n\\n\\n\\n  Gyula Fora\\n\\n  (@GyulaFora)\\nThe Apache Flink community is excited to announce the release of Flink Kubernetes Operator 1.6.0! The release features a large number of improvements all across the operator.\\nWe encourage you to download the release and share your feedback with the community through the Flink mailing lists or JIRA! We hope you like the new release and we’d be eager to learn about your experience with it.\\nHighlights # Improved and simplified rollback mechanism # Previously the rollback mechanism had some serious limitations always requiring the presence of HA metadata.\\n        ...\\n\\nContinue reading »\\n\\n\\n\\nAnnouncing three new Apache Flink connectors, the new connector versioning strategy and externalization\\n\\n    \\n  August 4, 2023 -\\n\\n\\n\\n  Elphas Toringepi\\n\\n  (@elphastori)\\nNew connectors # We’re excited to announce that Apache Flink now supports three new connectors: Amazon DynamoDB, MongoDB and OpenSearch! The connectors are available for both the DataStream and Table/SQL APIs.\\nAmazon DynamoDB - This connector includes a sink that provides at-least-once delivery guarantees. MongoDB connector - This connector includes a source and sink that provide at-least-once guarantees. OpenSearch sink - This connector includes a sink that provides at-least-once guarantees.\\n        ...\\n\\nContinue reading »\\n\\n\\n\\nSIGMOD Systems Award for Apache Flink\\n\\n    \\n  July 3, 2023 -\\n\\n\\n\\n  Hang Ruan\\n\\n\\n\\n\\n    Apache Flink received the 2023 SIGMOD Systems Award, which is awarded to an individual or set of individuals to recognize the development of a software or hardware system whose technical contributions have had significant impact on the theory or practice of large-scale data management systems:\\nThe 2023 SIGMOD Systems Award goes to Apache Flink:\\n“Apache Flink greatly expanded the use of stream data-processing.”\\nWinning of SIGMOD Systems Award indicates the high recognition of Flink’s technological advancement and industry influence from academia.\\n        ...\\n\\nContinue reading »\\n\\n\\n\\nApache Flink 1.16.2 Release Announcement\\n\\n    \\n  May 25, 2023 -\\n\\n\\n\\n  Weijie Guo\\n\\n  (@WeijieGuo12)\\nThe Apache Flink Community is pleased to announce the second bug fix release of the Flink 1.16 series.\\nThis release includes 104 bug fixes, vulnerability fixes, and minor improvements for Flink 1.16. Below you will find a list of all bugfixes and improvements (excluding improvements to the build infrastructure and build stability). For a complete list of all changes see: JIRA.\\nWe highly recommend all users upgrade to Flink 1.16.2.\\n        ...\\n\\nContinue reading »\\n\\n\\n\\nApache Flink 1.17.1 Release Announcement\\n\\n    \\n  May 25, 2023 -\\n\\n\\n\\n  Weijie Guo\\n\\n  (@WeijieGuo12)\\nThe Apache Flink Community is pleased to announce the first bug fix release of the Flink 1.17 series.\\nThis release includes 75 bug fixes, vulnerability fixes, and minor improvements for Flink 1.17. Below you will find a list of all bugfixes and improvements (excluding improvements to the build infrastructure and build stability). For a complete list of all changes see: JIRA.\\nWe highly recommend all users upgrade to Flink 1.17.1.\\n        ...\\n\\nContinue reading »\\n\\n\\n\\nApache Flink Kubernetes Operator 1.5.0 Release Announcement\\n\\n    \\n  May 17, 2023 -\\n\\n\\n\\n  Gyula Fora\\n\\n  (@GyulaFora)\\nThe Apache Flink community is excited to announce the release of Flink Kubernetes Operator 1.5.0! The release focuses on improvements to the job autoscaler that was introduced in the previous release and general operational hardening of the operator.\\nWe encourage you to download the release and share your feedback with the community through the Flink mailing lists or JIRA! We hope you like the new release and we’d be eager to learn about your experience with it.\\n        ...\\n\\nContinue reading »\\n\\n\\n\\nHowto test a batch source with the new Source framework\\n\\n    \\n  May 12, 2023 -\\n\\n\\n\\n  Etienne Chauchot\\n\\n  (@echauchot)\\nIntroduction # The Flink community has designed a new Source framework based on FLIP-27 lately. This article is the continuation of the howto create a batch source with the new Source framework article . Now it is time to test the created source ! As the previous article, this one was built while implementing the Flink batch source for Cassandra.\\nUnit testing the source # Testing the serializers # Example Cassandra SplitSerializer and SplitEnumeratorStateSerializer\\n        ...\\n\\nContinue reading »\\n\\n\\n\\nHowto migrate a real-life batch pipeline from the DataSet API to the DataStream API\\n\\n    \\n  May 9, 2023 -\\n\\n\\n\\n  Etienne Chauchot\\n\\n  (@echauchot)\\nIntroduction # The Flink community has been deprecating the DataSet API since version 1.12 as part of the work on FLIP-131: Consolidate the user-facing Dataflow SDKs/APIs (and deprecate the DataSet API) . This blog article illustrates the migration of a real-life batch DataSet pipeline to a batch DataStream pipeline. All the code presented in this article is available in the tpcds-benchmark-flink repo. The use case shown here is extracted from a broader work comparing Flink performances of different APIs by implementing TPCDS queries using these APIs.\\n        ...\\n\\nContinue reading »\\n\\n\\n\\nHowto create a batch source with the new Source framework\\n\\n    \\n  May 3, 2023 -\\n\\n\\n\\n  Etienne Chauchot\\n\\n  (@echauchot)\\nIntroduction # The Flink community has designed a new Source framework based on FLIP-27 lately. Some connectors have migrated to this new framework. This article is a how-to for creating a batch source using this new framework. It was built while implementing the Flink batch source for Cassandra. If you are interested in contributing or migrating connectors, this blog post is for you!\\nImplementing the source components # The source architecture is depicted in the diagrams below:\\n        ...\\n\\nContinue reading »\\n\\n\\n\\nApache Flink ML 2.2.0 Release Announcement\\n\\n    \\n  April 19, 2023 -\\n\\n\\n\\n  Dong Lin\\n\\n\\n\\n\\n    The Apache Flink community is excited to announce the release of Flink ML 2.2.0! This release focuses on enriching Flink ML’s feature engineering algorithms. The library now includes 33 feature engineering algorithms, making it a more comprehensive library for feature engineering tasks.\\nWith the addition of these algorithms, we believe Flink ML library is ready for use in production jobs that require feature engineering capabilities, whose input can then be consumed by both offline and online machine learning tasks.\\n        ...\\n\\nContinue reading »\\n\\n\\n\\n««\\n\\n\\n«\\n\\n\\n1\\n\\n\\n2\\n\\n\\n3\\n\\n\\n4\\n\\n\\n5\\n\\n\\n»\\n\\n\\n»»\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nApache Flink Kubernetes Operator 1.6.0 Release Announcement\\n\\n\\nAnnouncing three new Apache Flink connectors, the new connector versioning strategy and externalization\\n\\n\\nSIGMOD Systems Award for Apache Flink\\n\\n\\nApache Flink 1.16.2 Release Announcement\\n\\n\\nApache Flink 1.17.1 Release Announcement\\n\\n\\nApache Flink Kubernetes Operator 1.5.0 Release Announcement\\n\\n\\nHowto test a batch source with the new Source framework\\n\\n\\nHowto migrate a real-life batch pipeline from the DataSet API to the DataStream API\\n\\n\\nHowto create a batch source with the new Source framework\\n\\n\\nApache Flink ML 2.2.0 Release Announcement\\n\\n\\nAnnouncing the Release of Apache Flink 1.17\\n\\n\\nApache Flink 1.15.4 Release Announcement\\n\\n\\nApache Flink Kubernetes Operator 1.4.0 Release Announcement\\n\\n\\nApache Flink 1.16.1 Release Announcement\\n\\n\\nDelegation Token Framework: Obtain, Distribute and Use Temporary Credentials Automatically\\n\\n\\nApache Flink Table Store 0.3.0 Release Announcement\\n\\n\\nApache Flink Kubernetes Operator 1.3.1 Release Announcement\\n\\n\\nApache Flink Kubernetes Operator 1.3.0 Release Announcement\\n\\n\\nOptimising the throughput of async sinks using a custom RateLimitingStrategy\\n\\n\\nApache Flink 1.15.3 Release Announcement\\n\\n\\nAnnouncing the Release of Apache Flink 1.16\\n\\n\\nApache Flink Table Store 0.2.1 Release Announcement\\n\\n\\nApache Flink Kubernetes Operator 1.2.0 Release Announcement\\n\\n\\nApache Flink 1.14.6 Release Announcement\\n\\n\\nRegarding Akka's licensing change\\n\\n\\nApache Flink Table Store 0.2.0 Release Announcement\\n\\n\\nApache Flink 1.15.2 Release Announcement\\n\\n\\nApache Flink Kubernetes Operator 1.1.0 Release Announcement\\n\\n\\nApache Flink ML 2.1.0 Release Announcement\\n\\n\\nFLIP-147: Support Checkpoints After Tasks Finished - Part One\\n\\n\\nFLIP-147: Support Checkpoints After Tasks Finished - Part Two\\n\\n\\nApache Flink 1.15.1 Release Announcement\\n\\n\\nApache Flink 1.14.5 Release Announcement\\n\\n\\nAdaptive Batch Scheduler: Automatically Decide Parallelism of Flink Batch Jobs\\n\\n\\nApache Flink Kubernetes Operator 1.0.0 Release Announcement\\n\\n\\nImproving speed and stability of checkpointing with generic log-based incremental checkpoints\\n\\n\\nGetting into Low-Latency Gears with Apache Flink - Part Two\\n\\n\\nGetting into Low-Latency Gears with Apache Flink - Part One\\n\\n\\nApache Flink Table Store 0.1.0 Release Announcement\\n\\n\\nExploring the thread mode in PyFlink\\n\\n\\nImprovements to Flink operations: Snapshots Ownership and Savepoint Formats\\n\\n\\nAnnouncing the Release of Apache Flink 1.15\\n\\n\\nApache Flink Kubernetes Operator 0.1.0 Release Announcement\\n\\n\\nThe Generic Asynchronous Base Sink\\n\\n\\nApache Flink 1.14.4 Release Announcement\\n\\n\\nScala Free in One Fifteen\\n\\n\\nApache Flink 1.13.6 Release Announcement\\n\\n\\nStateful Functions 3.2.0 Release Announcement\\n\\n\\nPravega Flink Connector 101\\n\\n\\nApache Flink 1.14.3 Release Announcement\\n\\n\\nApache Flink ML 2.0.0 Release Announcement\\n\\n\\nHow We Improved Scheduler Performance for Large-scale Jobs - Part One\\n\\n\\nHow We Improved Scheduler Performance for Large-scale Jobs - Part Two\\n\\n\\nApache Flink StateFun Log4j emergency release\\n\\n\\nApache Flink Log4j emergency releases\\n\\n\\nAdvise on Apache Log4j Zero Day (CVE-2021-44228)\\n\\n\\nFlink Backward - The Apache Flink Retrospective\\n\\n\\nSort-Based Blocking Shuffle Implementation in Flink - Part One\\n\\n\\nSort-Based Blocking Shuffle Implementation in Flink - Part Two\\n\\n\\nApache Flink 1.13.3 Released\\n\\n\\nApache Flink 1.14.0 Release Announcement\\n\\n\\nImplementing a Custom Source Connector for Table API and SQL - Part One \\n\\n\\nImplementing a custom source connector for Table API and SQL - Part Two \\n\\n\\nStateful Functions 3.1.0 Release Announcement\\n\\n\\nHelp us stabilize Apache Flink 1.14.0 RC0\\n\\n\\nApache Flink 1.11.4 Released\\n\\n\\nApache Flink 1.12.5 Released\\n\\n\\nApache Flink 1.13.2 Released\\n\\n\\nHow to identify the source of backpressure?\\n\\n\\nApache Flink 1.13.1 Released\\n\\n\\nApache Flink 1.12.4 Released\\n\\n\\nScaling Flink automatically with Reactive Mode\\n\\n\\nApache Flink 1.13.0 Release Announcement\\n\\n\\nApache Flink 1.12.3 Released\\n\\n\\nStateful Functions 3.0.0: Remote Functions Front and Center\\n\\n\\nA Rundown of Batch Execution Mode in the DataStream API\\n\\n\\nApache Flink 1.12.2 Released\\n\\n\\nHow to natively deploy Flink on Kubernetes with High-Availability (HA)\\n\\n\\nApache Flink 1.10.3 Released\\n\\n\\nApache Flink 1.12.1 Released\\n\\n\\nUsing RocksDB State Backend in Apache Flink: When and How\\n\\n\\nExploring fine-grained recovery of bounded data sets on Flink\\n\\n\\nWhat's New in the Pulsar Flink Connector 2.7.0\\n\\n\\nStateful Functions 2.2.2 Release Announcement\\n\\n\\nApache Flink 1.11.3 Released\\n\\n\\nApache Flink 1.12.0 Release Announcement\\n\\n\\nImprovements in task scheduling for batch workloads in Apache Flink 1.12\\n\\n\\nStateful Functions 2.2.1 Release Announcement\\n\\n\\nFrom Aligned to Unaligned Checkpoints - Part 1: Checkpoints, Alignment, and Backpressure\\n\\n\\nStateful Functions Internals: Behind the scenes of Stateful Serverless\\n\\n\\nStateful Functions 2.2.0 Release Announcement\\n\\n\\nApache Flink 1.11.2 Released\\n\\n\\nFlink Community Update - August'20\\n\\n\\nMemory Management improvements for Flink’s JobManager in Apache Flink 1.11\\n\\n\\nApache Flink 1.10.2 Released\\n\\n\\nThe State of Flink on Docker\\n\\n\\nMonitoring and Controlling Networks of IoT Devices with Flink Stateful Functions\\n\\n\\nAccelerating your workload with GPU and other external resources\\n\\n\\nPyFlink: The integration of Pandas into PyFlink\\n\\n\\nAdvanced Flink Application Patterns Vol.3: Custom Window Processing\\n\\n\\nFlink Community Update - July'20\\n\\n\\nFlink SQL Demo: Building an End-to-End Streaming Application\\n\\n\\nSharing is caring - Catalogs in Flink SQL\\n\\n\\nApache Flink 1.11.1 Released\\n\\n\\nApplication Deployment in Flink: Current State and the new Application Mode\\n\\n\\nApache Flink 1.11.0 Release Announcement\\n\\n\\nFlink on Zeppelin Notebooks for Interactive Data Analysis - Part 2\\n\\n\\nFlink on Zeppelin Notebooks for Interactive Data Analysis - Part 1\\n\\n\\nFlink Community Update - June'20\\n\\n\\nStateful Functions 2.1.0 Release Announcement\\n\\n\\nApache Flink 1.10.1 Released\\n\\n\\nFlink Community Update - May'20\\n\\n\\nApplying to Google Season of Docs 2020\\n\\n\\nApache Flink 1.9.3 Released\\n\\n\\nMemory Management Improvements with Apache Flink 1.10\\n\\n\\nFlink Serialization Tuning Vol. 1: Choosing your Serializer — if you can\\n\\n\\nPyFlink: Introducing Python Support for UDFs in Flink's Table API\\n\\n\\nStateful Functions 2.0 - An Event-driven Database on Apache Flink\\n\\n\\nFlink Community Update - April'20\\n\\n\\nFlink as Unified Engine for Modern Data Warehousing: Production-Ready Hive Integration\\n\\n\\nAdvanced Flink Application Patterns Vol.2: Dynamic Updates of Application Logic\\n\\n\\nApache Beam: How Beam Runs on Top of Flink\\n\\n\\nNo Java Required: Configuring Sources and Sinks in SQL\\n\\n\\nApache Flink 1.10.0 Release Announcement\\n\\n\\nA Guide for Unit Testing in Apache Flink\\n\\n\\nApache Flink 1.9.2 Released\\n\\n\\nState Unlocked: Interacting with State in Apache Flink\\n\\n\\nAdvanced Flink Application Patterns Vol.1: Case Study of a Fraud Detection System\\n\\n\\nApache Flink 1.8.3 Released\\n\\n\\nHow to query Pulsar Streams using Apache Flink\\n\\n\\nRunning Apache Flink on Kubernetes with KUDO\\n\\n\\nApache Flink 1.9.1 Released\\n\\n\\nThe State Processor API: How to Read, write and modify the state of Flink applications\\n\\n\\nApache Flink 1.8.2 Released\\n\\n\\nFlink Community Update - September'19\\n\\n\\nApache Flink 1.9.0 Release Announcement\\n\\n\\nFlink Network Stack Vol. 2: Monitoring, Metrics, and that Backpressure Thing\\n\\n\\nApache Flink 1.8.1 Released\\n\\n\\nA Practical Guide to Broadcast State in Apache Flink\\n\\n\\nA Deep-Dive into Flink's Network Stack\\n\\n\\nState TTL in Flink 1.8.0: How to Automatically Cleanup Application State in Apache Flink\\n\\n\\nFlux capacitor, huh? Temporal Tables and Joins in Streaming SQL\\n\\n\\nWhen Flink & Pulsar Come Together\\n\\n\\nApache Flink's Application to Season of Docs\\n\\n\\nApache Flink 1.8.0 Release Announcement\\n\\n\\nFlink and Prometheus: Cloud-native monitoring of streaming applications\\n\\n\\nWhat to expect from Flink Forward San Francisco 2019\\n\\n\\nApache Flink 1.6.4 Released\\n\\n\\nMonitoring Apache Flink Applications 101\\n\\n\\nApache Flink 1.7.2 Released\\n\\n\\nBatch as a Special Case of Streaming and Alibaba's contribution of Blink\\n\\n\\nApache Flink 1.5.6 Released\\n\\n\\nApache Flink 1.6.3 Released\\n\\n\\nApache Flink 1.7.1 Released\\n\\n\\nApache Flink 1.7.0 Release Announcement\\n\\n\\nApache Flink 1.5.5 Released\\n\\n\\nApache Flink 1.6.2 Released\\n\\n\\nApache Flink 1.5.4 Released\\n\\n\\nApache Flink 1.6.1 Released\\n\\n\\nApache Flink 1.5.3 Released\\n\\n\\nApache Flink 1.6.0 Release Announcement\\n\\n\\nApache Flink 1.5.2 Released\\n\\n\\nApache Flink 1.5.1 Released\\n\\n\\nApache Flink 1.5.0 Release Announcement\\n\\n\\nApache Flink 1.3.3 Released\\n\\n\\nApache Flink 1.4.2 Released\\n\\n\\nAn Overview of End-to-End Exactly-Once Processing in Apache Flink (with Apache Kafka, too!)\\n\\n\\nApache Flink 1.4.1 Released\\n\\n\\nManaging Large State in Apache Flink: An Intro to Incremental Checkpointing\\n\\n\\nApache Flink in 2017: Year in Review\\n\\n\\nApache Flink 1.4.0 Release Announcement\\n\\n\\nLooking Ahead to Apache Flink 1.4.0 and 1.5.0\\n\\n\\nApache Flink 1.3.2 Released\\n\\n\\nA Deep Dive into Rescalable State in Apache Flink\\n\\n\\nApache Flink 1.3.1 Released\\n\\n\\nApache Flink 1.3.0 Release Announcement\\n\\n\\nIntroducing Docker Images for Apache Flink\\n\\n\\nApache Flink 1.2.1 Released\\n\\n\\nContinuous Queries on Dynamic Tables\\n\\n\\nFrom Streams to Tables and Back Again: An Update on Flink's Table & SQL API\\n\\n\\nApache Flink 1.1.5 Released\\n\\n\\nAnnouncing Apache Flink 1.2.0\\n\\n\\nApache Flink 1.1.4 Released\\n\\n\\nApache Flink in 2016: Year in Review\\n\\n\\nApache Flink 1.1.3 Released\\n\\n\\nApache Flink 1.1.2 Released\\n\\n\\nFlink Forward 2016: Announcing Schedule, Keynotes, and Panel Discussion\\n\\n\\nAnnouncing Apache Flink 1.1.0\\n\\n\\nFlink 1.1.1 Released\\n\\n\\nStream Processing for Everyone with SQL and Apache Flink\\n\\n\\nFlink 1.0.3 Released\\n\\n\\nFlink 1.0.2 Released\\n\\n\\nFlink Forward 2016 Call for Submissions Is Now Open\\n\\n\\nIntroducing Complex Event Processing (CEP) with Apache Flink\\n\\n\\nFlink 1.0.1 Released\\n\\n\\nAnnouncing Apache Flink 1.0.0\\n\\n\\nFlink 0.10.2 Released\\n\\n\\nFlink 2015: A year in review, and a lookout to 2016\\n\\n\\nStorm Compatibility in Apache Flink: How to run existing Storm topologies on Flink\\n\\n\\nIntroducing Stream Windows in Apache Flink\\n\\n\\nFlink 0.10.1 released\\n\\n\\nAnnouncing Apache Flink 0.10.0\\n\\n\\nOff-heap Memory in Apache Flink and the curious JIT compiler\\n\\n\\nAnnouncing Flink Forward 2015\\n\\n\\nApache Flink 0.9.1 available\\n\\n\\nIntroducing Gelly: Graph Processing with Apache Flink\\n\\n\\nAnnouncing Apache Flink 0.9.0\\n\\n\\nApril 2015 in the Flink community\\n\\n\\nJuggling with Bits and Bytes\\n\\n\\nAnnouncing Flink 0.9.0-milestone1 preview release\\n\\n\\nMarch 2015 in the Flink community\\n\\n\\nPeeking into Apache Flink's Engine Room\\n\\n\\nFebruary 2015 in the Flink community\\n\\n\\nIntroducing Flink Streaming\\n\\n\\nJanuary 2015 in the Flink community\\n\\n\\nApache Flink 0.8.0 available\\n\\n\\nDecember 2014 in the Flink community\\n\\n\\nHadoop Compatibility in Flink\\n\\n\\nApache Flink 0.7.0 available\\n\\n\\nUpcoming Events\\n\\n\\nApache Flink 0.6.1 available\\n\\n\\nApache Flink 0.6 available\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\", metadata={'source': 'https://flink.apache.org/posts/', 'title': 'Flink Blog | Apache Flink', 'description': '', 'language': 'en'}),\n",
              " Document(page_content=\"\\n\\nStream processing & Apache Flink News and Best Practices \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSkip to content\\n\\n\\n\\n\\n\\n\\n\\n\\nVerverica Cloud, a fully-managed cloud service for stream processing!\\n\\n\\n        Learn more\\n      \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n        \\n       \\n        Products\\n        \\n        \\n\\n\\n\\n\\n\\n\\n\\n\\n                  Ververica Cloud\\n                  Cloud-native service for real-time data processing\\n\\n\\n\\n\\n\\n\\n                      Overview\\nWhat is Ververica Cloud\\n\\n\\n\\n\\n\\n\\n                      Getting started with Ververica Cloud \\nDocumentation & Help\\n\\n\\n\\n\\n\\n\\n                      Sign up for free\\nGet early access and give us your feedback\\n\\n\\n\\n\\n\\n\\n                      Access Ververica Cloud\\nStart working now\\n\\n\\n\\n\\n\\n\\nGet Started with Ververica\\nEnterprise stream processing based on Apache Flink\\nSign up for Ververica Cloud\\nGet Ververica Platform\\n\\n\\n\\n\\n\\n                  Ververica Platform\\n                  Apache Flink-powered stream processing platform\\n\\n\\n\\n\\n\\n\\n                      Overview\\nStream processing with Ververica & Flink\\n\\n\\n\\n\\n\\n\\n                      Apache Flink Operations\\nDeploy & scale Flink more easily and securely\\n\\n\\n\\n\\n\\n\\n                      Apache Flink SQL\\nAnalyze streaming data with SQL\\n\\n\\n\\n\\n\\n\\n                      Pricing & Editions\\nVerverica Platform pricing. Start for free\\n\\n\\n\\n\\n\\n\\n                      Special License Programs\\nSpecial pricing for Startups\\n\\n\\n\\n\\n\\n\\n                      Getting Started\\nGet started with Ververica Platform for free\\n\\n\\n\\n\\n\\n\\nGet Started with Ververica\\nEnterprise stream processing based on Apache Flink\\nSign up for Ververica Cloud\\nGet Ververica Platform\\n\\n\\n\\n\\n\\n\\n\\n        \\n       \\n        Learn\\n        \\n        \\n\\n\\n\\n\\n\\n\\n\\n\\n                  Ververica Academy\\n                  Learn all about  Apache Flink and stream processing\\n\\n\\n\\n\\n\\n\\n                      Overview\\nAbout Ververica Academy\\n\\n\\n\\n\\n\\n\\n                      Course Catalog\\nCheck current course catalogue.\\n\\n\\n\\n\\n\\n\\n                      Member Login\\nStart your training now and earn badges\\n\\n\\n\\n\\n\\n\\nFlink Forward Seattle 2023!\\nFlink Forward is the conference dedicated to Apache Flink and the stream processing community.\\nRegister here\\n\\n\\n\\n\\n\\n                  Ververica Documentation\\n                  Apache Flink-powered stream processing products\\n\\n\\n\\n\\n\\n\\n                      Platform Documentation\\nUser Guides & Release Notes for Ververica Platform\\n\\n\\n\\n\\n\\n\\n                      Cloud Documentation\\nGetting started with Ververica Cloud from documentation & help\\n\\n\\n\\n\\n\\n\\n                      Knowledge Base\\nTechnical articles about how to use and set up Ververica products\\n\\n\\n\\n\\n\\n\\nFlink Forward Seattle 2023!\\nFlink Forward is the conference dedicated to Apache Flink and the stream processing community.\\nRegister here\\n\\n\\n\\n\\n\\n\\n\\n        \\n       \\n        Partner\\n        \\n      \\n        \\n\\n\\n\\n        \\n       \\n        Customers\\n        \\n      \\n        \\n\\n\\n\\n        \\n       \\n        Blog\\n        \\n      \\n        \\n\\n\\n\\n        \\n       \\n        Careers\\n        \\n      \\n        \\n\\n\\n\\n        \\n       \\n        Contact\\n        \\n      \\n        \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nVerverica Blog\\n\\nLatest news and updates about stream processing with Apache Flink and Ververica Platform. Best Practices, Apache Flink Use Cases, Flink features\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nAll\\nApache Flink\\nFlink Features\\nVerverica Platform\\nFlink SQL\\nUse Cases\\nFlink Forward\\nGeneral | Company Updates\\nFlink CDC\\nMeet The Engineer\\nVerverica Cloud\\nPyFlink\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nFeatured\\n\\n\\n\\nVerverica Platform\\n\\nVerverica Platform 2.11.1 is Released!\\n\\nby Ververica\\n 07 September 23\\n\\nWe're thrilled to share the latest update to Ververica Platform! This is a patch release offering enhancements and improvements requested by the community and our customers â€“ big and small, old and...\\nRead More\\n\\n\\n\\n\\n\\n\\n\\nFeatured\\n\\n\\n\\nVerverica Platform\\n\\nVerverica Platform 2.11 is released!\\n\\nby Ververica\\n 01 August 23\\n\\nðŸš€ We're thrilled to share the latest update packed with new features and enhancements to take your data processing to new heights!\\nRead More\\n\\n\\n\\n\\n\\n\\n\\nFeatured\\n\\n\\n\\nGeneral | Company Updates,\\n          \\n          Ververica Cloud\\n\\nReleasing Ververica Cloud Beta - Fully Managed Cloud Native Service\\n\\nby Vladimir Jandreski\\n 03 April 23\\n\\nWe are thrilled to announce that Ververica Cloud has entered its public beta phase, starting today! This release enables you to effortlessly embark on real-time data processing and take advantage of...\\nRead More\\n\\n\\n\\n\\n\\n\\n\\nFeatured\\n\\n\\n\\nVerverica Platform\\n\\nAnnouncing Ververica Platform 2.8\\n\\nby Daisy Tsang\\n 17 October 22\\n\\nThe latest minor release includes an improved High Availability service, improvements to user experience, and vulnerability fixes! The patch release of Apache Flink 1.15.2 includes several bug fixes,...\\nRead More\\n\\n\\n\\n\\n\\n\\n\\nFeatured\\n\\n\\n\\nVerverica Platform\\n\\nAnnouncing Ververica Platform 2.7 for Apache FlinkÂ® 1.15\\n\\nby Daisy Tsang\\n 12 July 22\\n\\nThe latest minor release includes full support for Flink 1.15, several improvements to the user experience, and a brand new look! The recent release of Apache Flink 1.15 brings a number of exciting...\\nRead More\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nby Alexey Novakov\\n13 September 23\\n\\nBootstrap Data Pipeline via Flink HybridSource\\n\\n            A common requirement in the area of data engineering is to first process existing historical data...\\n          \\n\\nRead More\\n\\n\\n\\n\\n\\n\\nVerverica Platform\\n\\n\\nby Ververica\\n07 September 23\\n\\nVerverica Platform 2.11.1 is Released!\\n\\n            We're thrilled to share the latest update to Ververica Platform! This is a patch release offering...\\n          \\n\\nRead More\\n\\n\\n\\n\\n\\n\\n\\n\\nby Ververica\\n31 August 23\\n\\nVerverica Academy: Pioneering the Future of Apache FlinkÂ® Education\\n\\n            The world of data processing is in constant evolution, with real-time data streaming at the helm of...\\n          \\n\\nRead More\\n\\n\\n\\n\\n\\n\\n\\n\\nby Ververica\\n30 August 23\\n\\nBatch Processing vs Stream Processing\\n\\n            Batch processingand stream processing are two very different models for processing data. Both have...\\n          \\n\\nRead More\\n\\n\\n\\n\\n\\n\\nApache Flink\\nFlink Features\\n\\n\\nby Ververica\\n16 August 23\\n\\nStream Enrichment in Flink\\n\\n            Imagine a photo without its vibrant colors; intriguing but lacking depth. Stream enrichment works...\\n          \\n\\nRead More\\n\\n\\n\\n\\n\\n\\nApache Flink\\nFlink Features\\n\\n\\nby Dian Fu\\n02 August 23\\n\\nAll You Need to Know About PyFlink\\n\\n            PyFlink serves as a Python API for Apache Flink, providing users with a medium to develop Flink...\\n          \\n\\nRead More\\n\\n\\n\\n\\n\\n\\nVerverica Platform\\n\\n\\nby Ververica\\n01 August 23\\n\\nVerverica Platform 2.11 is released!\\n\\n            ðŸš€ We're thrilled to share the latest update packed with new features and enhancements to take your...\\n          \\n\\nRead More\\n\\n\\n\\n\\n\\n\\nFlink SQL\\nVerverica Cloud\\n\\n\\nby Ververica\\n12 July 23\\n\\nStream Processing Scalability: Challenges and Solutions\\n\\n            What is Stream Processing Stream processing is a programming paradigm which views data streams, or...\\n          \\n\\nRead More\\n\\n\\n\\n\\n\\n\\nVerverica Platform\\nFlink Features\\n\\n\\nby Ververica\\n30 June 23\\n\\nVerverica Platform 2.10.4 is released!\\n\\n            Ververica Platform release 2.10.4 is now released to General Availability (GA) for all customers...\\n          \\n\\nRead More\\n\\n\\n\\n\\n\\n1\\n2\\n3\\n4\\n5\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSign up for Monthly Blog Notifications\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nProduct\\n\\n\\nVerverica Platform\\nApache FlinkÂ® Operations\\nApache FlinkÂ® SQL\\nSpecial License Programs\\nCustomers\\nSupport\\n\\n\\n\\n\\nVerverica Platform\\n\\n\\nApache FlinkÂ®\\nApache FlinkÂ® Documentation\\nApache FlinkÂ® Training\\nApache FlinkÂ® Meetups\\nApache FlinkÂ® Blog\\n\\n\\n\\n\\nData Platform\\n\\n\\nData Platforms\\nEvent-driven Applications\\nFraud Detection\\nMachine Learning\\nReal-time Analytics\\nStreaming Pipelines\\nVerverica Platform\\n\\n\\n\\n\\nDevelopers\\n\\n\\nWhat is stream processing?\\nPlatform Documentation\\nCloud Documentation\\nKnowledge Base\\nFlink Forward Resources\\nMeetups\\nBlog\\n\\n\\n\\n\\nLegal\\n\\n\\nPrivacy Policy\\nTerms of Service\\nData Processing Agreement\\nImprint\\nLicense Agreement\\n\\n\\n\\n\\nAbout\\n\\n\\nCareers\\nPartners\\nBlog\\n\\n\\n\\n\\n\\n          Follow Us\\n        \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n          Follow Us\\n        \\n\\n\\n\\n\\n\\n\\n\\n\\nGet the Latest News and Tips\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nÂ© Copyright 2023 Ververica. Privacy Policy.\\xa0Imprint.\\xa0Apache FlinkÂ®, FlinkÂ®, ApacheÂ®, the squirrel logo, and the Apache feather logo are either registered trademarks or trademarks of The Apache Software Foundation.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\", metadata={'source': 'https://www.ververica.com/blog', 'title': 'Stream processing & Apache Flink News and Best Practices ', 'description': 'Latest news and updates about stream processing with Apache Flink and Ververica Platform. Best Practices, Apache Flink Use Cases, Flink features', 'language': 'en'}),\n",
              " Document(page_content='\\n\\n\\n\\n\\n\\n\\nflink – The musings of rawkintrevo\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSkip to content\\n\\n\\n\\nHot-rodder, opera enthusiast, mad data scientist; a man for all seasons.\\n\\n\\n\\n\\n\\nThe musings of rawkintrevo\\n\\n\\nMenu\\nApache Mahout\\nApache Flink\\nApache Streams-incubating\\nPress and Conference Talks\\nHire Me!\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nCategory: flink \\n\\n\\n\\n \\n\\n\\nBehind the Scenes Pt. 4: The K8s\\xa0Bugaloo. \\nSeptember 19, 2019 by rawkintrevo, posted in big data for noobs, flink, K8s  \\n\\nLet me start off by saying how glad I am to be done with this post series.\\nI knew when I finished the project, I should have just written all four posts, and then timed them out for delayed release.¬† But I said, “nah, writing blog posts is future-rawkintrevo’s problem and Fuuuuuuug that guy.”¬† So here I am again.¬† Trying to remember the important parts of a thing I did over a month ago, when what I really care about at the moment is Star Trek Bots. But unforuntaly I won’t get to write a blog post on that until I haven’t been working on it for a month too (jk, hopefully next week, though I trained that algorithm over a year ago I think).\\nOK. So let’s do this quick.\\nSetting up a K8s On IBM Cloud\\nSince we were using OpenWhisk earlier- I’m just going to assume you have an IBMCloud account.¬† The bummer is you will now have to give them some money for a K8s cluster. I know it sucks.¬† I had to give them money too (actually I might have done this on a work account, I forget).¬† Anyway, you need to give them money for a 3 cluster “real” thing, because the free ones will no allow Istio ingresses, and we are going to be using those like crazy.\\nService Installation Script\\nIf you do anything on computers in life, you should really make a script so next time you can do it in a single command line.¬† Following that them, here’s my (ugly) script.¬† The short outline is :\\n\\nInstall Flink\\nInstall / Expose Elasticsearch\\nInstall / Expose Kibana\\nChill out for a while.\\nInstall / Expose my cheap front end from a prior section.\\nSetup Ingresses.\\nUpload the big fat jar file.\\n\\nFlink / Elasticsearch / Kibana\\nThe Tao of Flink On K8s has long been talked about (like since at least last Flink Forward Berlin) and is outlined nicely here.¬† The observant reader will notice I even left a little note to myself in the script.¬† All in all, the Flink + K8s experience was quite pleasant.¬† There is one little kink I did have to hack around, and I will show you now.\\nCheck out this line.¬† The short of the long of it was, the jar we made is a verrrry fat boy, and blew out the limit. So we are tweaking this one setting to allow jars of any size to be uploaded.¬† The “right way” to do this in Flink is to leave the jars in the local lib/ folder, but for <reasons> on K8s, that’s a bad idea.\\nElasticsearh, I only deployed single node. I don’t think multi node is supposed to be that much harder, but for this demo I didn’t need it and was busy focusing on my trashy front end design.\\nKibana works fine IF ES is running smoothly. If Kibana is giving you a hard time, go check ES.\\nI’d like to have a moment of silence for all the hard work that went in to making this such an easy thing to do.\\nkubectl apply -f ...\\nkubectl expose deployment ...\\nThat’s life now.\\nMy cheap front end and establishing Ingresses.\\nA little kubectl apply/expose also was all it took to expose my bootleggy website.¬† There’s probably an entire blog post on just doing that, but again, we’re keeping this one high level. If you’re really interested check out.\\n\\nMake a simple static website, then Docker it up. (Example)\\nMake a yaml that runs the Dockerfile you just made (Example)\\nMake an ingress that points to your exposed service. (Example)\\n\\nWhich is actually a really nice segway into talking about Ingresses.¬† The idea is you K8s cluster is hidden away from the world, operating in it’s own little universe.¬† We want to poke a few holes and expose that universe to the outside.\\nBecause I ran out of time, I ended up just using the prepackaged Flink WebUI and Kibana as iFrames on my “website”.¬† As such, I poked¬†several holes and you can see how I did it here:\\n\\nFlink Ingress\\nKibana Ingress\\nElasticsearch Ingress (I think I needed this for Kibana to work right?)\\n\\nThose were hand rolled and have minimum nonsense, so I think they are pretty self explanatory. You give it a service, a port, and a domain host. Then it just sort of works, bc computers are magic.\\nConclusions\\nSo literally as I was finishing the last paragraph I got word that my little project has been awarded 3rd place, but there were a lot of people in the competition so it’s not like was 3rd of 3 ( I have a lot of friends who read this blog (only my friends read this blog?), and we tend to cut at each other a lot).\\nMore conclusively though, a lot of times when you’re tinkering like me, its easy to get off on one little thing and not build full end to end systems. Even if you suck at building parts, it helps illustrate the vision.¬† Imagine you’ve never seen a horse. Then imagine I draw the back of one, and tell you to just imagine what the front is like. You’re going to be like, “WTF?”.¬† So to tie this back in to Brian Holt’s “Full Stack Developer” tweet, this image is still better than “close your eyes and make believe”.\\n\\xa0\\nBrian Holt, Twitter\\n\\xa0\\nI take this even further in my next post.¬† I made the Star Trek Bot Algorithm over a year ago and had it (poorly) hooked up to twitter. I finally learned some React over the summer and now I have a great way to kill hours and hours of time, welcome to the new Facebook. startrekbots.com\\nAt any rate, thanks for playing. Don’t sue me.\\n\\n\\xa0\\n\\xa0\\n\\xa0\\n\\n\\nLeave a comment \\n\\n\\n\\n\\n \\n\\n\\nBehind the Scenes of ‚ÄúRT’s HoRT IoT-A, An AIoT-P MVP Demo‚Äù: Part 2- The Streaming\\xa0Engine \\nSeptember 5, 2019 by rawkintrevo, posted in Engineering, flink, How-Tos, IoT, openwhisk  \\n\\nWelcome back from Part 1. On our last episode we did an introduction, spoke briefly on the overall structure of the program, discussed our demo data source, and did some foreshadowing of this week. This week we’ll do a bit of what we foreshadowed.\\nThe Streaming Engine\\nOr How to Do Weird Stuff with Apache Flink\\nLet’s start off by looking at some pictures. Let me show you the DAG of the Flink Streaming Job to provide some motivation, and then we can go through piece by piece and look at the implementations.\\n\\nFrom the “architecture diagram” in Episode 1, this is what the Flink Job is doing.\\n\\nWe pick up the data from MQTT\\nWe apply a sliding window doing some simple analytics on the data (Count, Min, Max, Average, Sum Squared Deviations, Standard Deviation)\\nWe join each record to the last analytics emitted from the sliding window\\nWe update the list of REST endpoints which serve the models every 30 seconds\\nWe send each Record + Last Emitted Analytics pair as a REST request to each model, and then sink the Record, Analytics, and Model Result (for each model) to Elasticsearch.\\n\\nPretty easy peasy.¬† If you’re not interested in Apache Flink, you can basically stop here and know/understand that Record from Divvy Bike Station + Analytics + Model Results are sunk (written) into Elasticsearch and that by continuously polling an endpoint that returns other active endpoints we can dynamically add/delete/update the models being ran on this stream of data.\\nGetting Data from MQTT\\nInitially I was using a copy of luckyyuyong’s flink-mqtt-connector in my implementation and required some hacks and updates.\\nThe most important call out is that MQTT will disconnect brokers with the¬†same clientID because it thinks they are old and stale, so make we have to make the clientID random. This was a particularly nasty and inconsistent bug to track down, but turns out many other have had this problem.¬† The solution here was just to add a string of milliseconds since the epoch.¬† Probably would need something more for production but this is an MVP.\\n\\nString clientBase = String.format(\"a:%s:%s\", Org_ID, App_Id).substring(0, 10);\\nString clientID = clientBase + System.currentTimeMillis();\\nmqttProperties.setProperty(MQTTSource.CLIENT_ID, clientID );\\n\\nAside from that little quirk, this all works basically like an Apache Kafka or any other connector.\\nApply the Sliding Window\\nThis was a bit of a trick because I didn’t want to hard code the data structure into the engine. I would like to eventually auto determine the schema of the json, but because of time constraints I set it up to be passed as a command line argument (but ended up hard coding what the CLI argument would be – see here).\\nThis is important because we don’t want the window to be trying to compute analytics on text fields, and the data coming in form MQTT is all going to look like a string.\\nIf you look at the code in ProcessWindowAnalyticsFunction you can see that function expects a schema to come in with the records, and in that schema any field that is listed as ‘Numeric’ we will attempt to compute analytics on.¬† Admittedly here, we are trading off performance for a single engine that will compute any data source.\\nJoining the Records and Last Reported Analytics\\nAt this point, I had been doing A LOT of Java, which to be candid, I really don’t care for- so I switched over to Scala.¬† It’s really not a particularly interesting function. It simply joins all records with the last reported analytics from the sliding window.¬† You’re free to check it out here and make comments if you have questions. I realize there’s a lot of magic-hand-waiving and me telling the reader to “go look for yourself”,¬† for this blog / tutorial, but I am assuming you are fairly familiar with all of the tools I am using and I’m trying to give the reader a high level view of how this all fits together. If you have specific questions, please ask in the comments or email me (you can find me around the interwebs pretty easily).\\nThe “Other” Stream\\nNow let’s shift and consider our “other” stream. The stream that simply polls an endpoint which serves us other endpoints every thirty seconds.¬† This is accomplished effectively by doing a time windowed stream on all of the events coming from the records source- throwing them all away, and then once every 30 seconds (but you could configure that), sending an Asyncy REST request to a preordained URL that holds the REST endpoints for the models.\\nYou can see this in my github lines approx 140-159.¬† The endpoint that this system is hitting is being served in Apache OpenWhisk (which I absolutely love if you haven’t been able to gleam from my other blog posts, it’s like AWS / Google Cloud Functions except not proprietary vendor lock-in garbage).\\nYou can see the response this gives here.¬† Obviously, a “next step” would be for this to hit some sort of database where you could add/delete entries. (If you’re to lazy to click, it basically just returns a json of { endpoints: [ { modelOneName: “http://the-url&#8221;}, …]}\\nMerging it All Together and Making Async Calls\\nNow we¬† bring everything together.¬† From one stream we have Device Event Records and analytics on those records, from the other we have a list of URLs which will serve models.¬† Now- it’s worth pointing out here, that while not implemented in a “real” version an easy add would be to have another field in the model name that specifies what devices it applies to- since that model is expecting certain input fields and different devices will have different fields. Again- a MINIMUM viable product is presented.\\nThe rest is pretty simple conceptually- for each record/analytics item- it goes through the list of all applicable URLs (in this case all of the URLs), and pings each with the record and analytics as the payload. The code is here and may be more illuminating.¬† The magic happens in the main program right here.\\nThe Async ping models is nice because as different requests come back at different speeds, they don’t hold up the rest of the stream.¬† A bug/feature can be introduced though if you don’t want the entire program to go flying off the rails if there is a single REST request.¬† To do that you must set the “timeout” of the Async function, my choice was to “ignore” but you could in theory re-request, allow up to X fails in a Y time, etc.\\nConclusion\\nI want to state one more time- that this was a lot of waive-my-hands magic, and “go look for yourself”dom.¬† I probably could have made a 5 part blog just out of this post- but 1. I’m trying to write a book on something else already, and 2. the point of this blog series is an over view of how I built a “full stack” IoT Analytics solution from scratch part time in a couple of weeks.\\nNext Time\\nWe’ll follow our hero into his mis-adventures in React, especially with the Dread Design System: Carbon.\\nSee you, Space Cowboy.\\n\\n\\nLeave a comment \\n\\n\\n\\n\\n \\n\\n\\nBehind the Scenes of “Rawkintrevo’s House of Real Time IoT Analytics, An AIoT platform MVP\\xa0Demo” \\nAugust 27, 2019 by rawkintrevo, posted in big data for noobs, Engineering, flink, How-Tos, IoT, K8s, openwhisk, python, React, video  \\n\\nWoo, that’s a title- amirigh!?\\nIt’s got everything- buzzwords, a corresponding YouTube video, a Twitter handle conjugated as a proper noun.\\nIntroduction\\nJust go watch the video– I’m not trying to push traffic to YouTube, but it’s a sort of complicated thing and I don’t do a horrible job of explaining it in the video. You know what, I’m just gonna put it in line.\\nOk, so Now you’ve see that.¬† And you’re wondering? How in the heck?!¬† Well good news- because you’ve stumbled to the behind the scenes portion where I explain how the magic happened.\\nThere’s a lot of magic going on in there, and some you probably already know and some you’ve got no idea. But this is the story of my journey to becoming a full stack programmer.¬† As it is said in the Tao of Programming:\\nThere once was a Master Programmer who wrote unstructured programs. A novice programmer, seeking to imitate him, also began to write unstructured programs. When the novice asked the Master to evaluate his progress, the Master criticized him for writing unstructured programs, saying, “What is appropriate for the Master is not appropriate for the novice. You must understand Tao before transcending structure.”\\nI’m not sure if I’m the Master or the Novice- but this program is definitely unstructured AF. So here is a companion guide that maybe you can learn a thing or two / Fork my repo and tell your boss you did all of this yourself.\\nTable of Contents\\nHere’s my rough outline of how I’m going to proceed through the various silliness of this project and the code contained in my github repo¬†.\\n\\nYOU ARE HERE. A sarcastic introduction, including my dataset, WatsonIoT Platform (MQTT). Also we’ll talk about our data source- and how we shimmed it to push into MQTT, but obviously could (should?) do the same thing with Apache Kafka (instead). I’ll also introduce the chart- we might use that as a map as we move along.\\nIn the second post, I’ll talk about my Apache Flink streaming engine- how it picks up a list of REST endpoints and then hits each one of them.¬† In the comments of this section you will find people telling me why my way was wrong and what I should have done instead.\\nIn this post I’ll talk about my meandering adventures with React.js, and how little I like the Carbon Design System. In my hack-a-thon submission,¬† I just iFramed up the Flink WebUI and Kibana, but here’s where I would talk about all the cool things I would have made if I had more time / Carbon-React was a usable system.\\nIn the last post I’ll push this all on IBM’s K8s. I work for IBM, and this was a work thing. I don’t have enough experience on any one else’s K8s (aside from microK8s which doesn’t really count) to bad mouth IBM. They do pay me to tell people I work there, so anything to rude in the comments about them will most likely get moderated out. F.u.\\n\\nData Source\\nSee README.md and scroll down to Data Source. I’m happy with that description.\\nAs the program is currently, right about here the schema is passed as a string. My plan was to make that an argument so you could submit jobs from the UI.¬† Suffice to say, if you have some other interesting data source- either update that to be a command line parameter (PRs are accepted) or just change the string to match your data.¬† I was also going to do something with Schema inference, but my Scala is rusty and I never was great at Java, and tick-tock.\\nWatson IoT Platform\\nI work for IBM, specifically Watson IoT, so I can’t say anything bad about WatsonIoT.¬† It is basically based on MQTT, which is a pub-sub thing IBM wrote in 1999 (which was before Kafka by about 10 years, to be fair).\\nIf you want to see my hack to push data from the Divvy API into Watson IoT Platform, you can see it here. You will probably notice a couple of oddities.¬† Most notably, that only 3 stations are picked up to transmit data.¬† This is because the Free account gets shut down after 200MB of data and you have to upgrade to a $2500/mo plan bc IBM doesn’t really understand linear scaling. /shrug. Obviously this could be easily hacked to just use Kafka and update the Flink Source here.\\nThe Architecture Chart\\nThat’s also in the github, so I’m going to say just look at it on README.md.\\nComing Up Next:\\nWell, this was just about the easiest blog post I’ve ever written.¬† Up next, I may do some real work and get to talking about my Flink program which picks up a list of API endpoints every 30 seconds, does some sliding window analytics, and then sends each record and the most recent analytics to each of the end points that were picked up, and how in its way- this gives us dynamic model calling. Also- I’ll talk about the other cool things that could/should be done there that I just didn’t get to. /shrug.\\nSee you, Space Cowboy.\\n\\xa0\\n\\xa0\\n\\xa0\\n\\n\\nLeave a comment \\n\\n\\n\\n\\n \\n\\n\\nBorg System Architecture \\nNovember 7, 2017November 7, 2017 by rawkintrevo, posted in Engineering, flink, How-Tos, mahout  \\n\\nor “how I accidentally enslaved humanity to the Machine Overlords”.\\nThe Borg are a fictional group from the Sci-Fi classic, Star Trek, who among other things have a collective consciousness.¬† This creates a number of problems for the poor humans (and other species) that attempt to resist the Borg, as they are extremely adaptive. When a single Borg Drone learns something, its knowledge is very quickly propagated through the collective, presumably subject to network connectivity issues, and latency.\\nHere we create a system for an arbitrary number edge devices to report sensor data, a central processor to use the data to understand the environment the edge devices are participating in, and finally to make decisions / give instructions back to the edge device.¬† This is in essence what the Borg are doing.¬† Yes, there are some interesting biological / cybernetic integrations, however as far as the “hive mind” aspect is concerned, this is basic principles in play.\\nI originally built this toy to illustrate that “A.I.” has three principle components: Real time data going into a system, an understanding of the environment is reached, a decision is made. (1)¬†Real Time artifical intelligence, like the “actual” intelligence it supposedly mimics is not making E.O.D. batch decisions. (2) In real time the system is aware of what is happening around it- understanding its environment and then using that understanding to (3) make some sort of decision about how to manipulate that environment. Read up on definitions of intelligence, a murky subject itself.\\nAnother sweet bonus, I wanted to show that sophisticated A.I. can be produced with off-the-shelf components and a little creativity, despite what vendors want to tell you. Vendors have their place. It’s one thing to make something cool, another to productionalize it- and maybe you just don’t care enough. However, since you’re reading this- I hope you at least care a little.\\nArtificial Intelligence is by no means synonymous with Deep Learning, though Deep Learning can be a very useful tool for building A.I. systems.¬† This case does real time image recognition, and you’ll note does not invoke Deep Learning or even the less buzz-worthy “neural nets” at any point.¬† Those can be easily introduced to the solution, but you don’t need them.\\nLike the Great and Powerful Oz, once you pull back the curtain on A.I. you realize its just some old man who got lost and creatively used resources he had lying around to create a couple of interesting magic tricks.\\n\\n\\nSystem Architecture\\nOpenCV is the Occipital Lobe, this is where faces are identified in the video stream.\\nApache Kafka is the nervous system, how messages are passed around the collective. (If we later need to defeat the Borg, this is probably the best place to attack- presuming we of course we aren’t able to make the drones self aware).\\n\\nApache Flink is the collective consciousness of our Borg Collective, where thoughts of the Hive Mind are achieved.¬† This is probably intuitive if you are familiar with Apache Flink.\\nApache Solr is the store of the “memories” of the collective consciousness.\\nThe Apache Mahout library is the “higher order brain functions” for understanding. It is an ideal choice as it is well integrated with Apache Flink and Apache Spark\\nApache Spark with Apache Mahout gives our creation a sense of conext, e.g. how do I recognize faces? It quickly allows us to bootstrap millions of years of evolutionary biological processes.\\nA Walk Through\\n(1) Spark + Mahout used to calculate eigenfaces¬†(see previous blog post).\\n(2) Flink is started, it loads the calculated eigenfaces from (1)\\n(3) A video feed is processed with OpenCV .\\n(4) OpenCV uses Haar Cascade Filters¬†to detect faces.\\n(5) Detected faces are turned in to Java Buffered Images, greyscaled and size-scaled to the size used for Eigenface calculations and binarized (inefficient). The binary arrays are passed as messages to Kafka.\\n(6) Flink picks up the images, converts them back to buffered images. The buffered image is then decomposed into linear a linear combination of the Eigenfaces calculated in (1).\\n(7) Solr is queried for matching linear combinations. Names associated with best¬†N matches are assigned to each face. I.e. face is “identified”… poorly. See next comments.\\n(8) If the face is of a “new” person, the linear combinations are written to Solr as a new potential match for future queries.\\n(8) Instructions for edge device written back to Kafka messaging queue as appropriate.\\nProblems\\nA major problem we instantly encountered was that sometimes OpenCV will “see” faces that do not exist, as patterns in clothing, shadows, etc. To overcome this we use Flink’s sliding time window and Mahout’s Canopy clustering.¬† Intuitively, faces will not momentarily appear and disappear within a frame, cases where this happens are likely errors on the part of OpenCV. We create a short sliding time window and cluster all faces in the window based on their X, Y coordinates.¬† Canopy clustering is used because it is able to cluster all faces in one pass, reducing the amount of introduced latency.¬† This step happens between step (6) and (7)\\nIn the resulting clusters there are either lots of faces (a true face) or a very few faces (a ghost or shadow, which we do not want).¬† Images belonging to the former are further processed for matches in step (7).\\nAnother challenge is certain frames of a face may look like someone else, even though we have been correctly identifying the face in question in nearby frames.¬† We use our clusters generated in the previous hack, and decide that people do not spontaneously become other people for an instant and then return. We take our queries from step 7 and determine who the person is based on the cluster, not the individual frames.\\nFinally, as our Solr index of faces grows, our searches in Solr will become less and less effecient.¬† Hierarchical clustering is believed to speed up these results and be akin to how people actually recognize each other.¬† In the naive form, for each Solr Query will scan the entire index of faces looking for a match.¬† However we can clusters the eigenface combinations such that each query will first only scan the cluster centriods, and then only consider eigenfaces in that cluster. This can potentially speed up results greatly.\\nUsecases\\nBorg\\nThis is how the Borg were able to recognize Locutus of Borg.\\n\\nCylons\\nThis type of system also was imperative for Cylon Raiders and Centurions to recognize (and subsequently not inadvertently kill) the Final Five.\\n\\nShorter Term\\nThis toy was originally designed to work with the Petrone Battle Drones however as we see the rise of Sophia and Atlas, this technology could be employed to help multiple subjects with similar tasks learn and adapt more quickly.¬† Additionally there are numerous applications in security (think network of CCTV cameras, remote locks, alarms, fire control, etc.)\\nDo you want Cylons? Because that’s how you get Cylons.\\nAlas, there is no great and powerful Oz. Or- there is, and …\\n\\n\\xa0\\nReferences\\nFlink Forward, Berlin 2017\\nSlides¬†Video¬†(warning I was sick this day. Not my best work).\\nLucene Revolution, Las Vegas 2017\\nSlides Video\\nMy Git Repo\\nPR Donating this to Apache Mahout\\nIf you’re interested in contributing, please start here.\\n\\n\\nLeave a comment \\n\\n\\n\\n\\n \\n\\n\\nUsing JNIs (like OpenCV) in\\xa0Flink \\nAugust 14, 2017August 14, 2017 by rawkintrevo, posted in Engineering, flink, How-Tos  \\n\\nFor a bigger project which I hope to blog about soon, I needed to get the OpenCV Java Native Library (JNI) running in a Flink stream. It was a pain in the ass, so I’m putting this here to help the next person.\\nFirst thing I tried… Just doing it.\\nFor OpenCV, you need to statically initialize (I’m probably saying this wrong) the library, so I tried something like this\\nval stream = env\\n.addSource(rawVideoConsumer)\\n.map(record => {\\nSystem.loadLibrary(Core.NATIVE_LIBRARY_NAME)\\n…\\nWell, that kicks an error that looks like:\\n\\nException in thread \"Thread-143\" java.lang.UnsatisfiedLinkError:\\nNative Library /usr/lib/jni/libopencv_java330.so already loaded in\\nanother classloader\\n\\nOk, that’s fair. Multiple task managers, this is getting loaded all over the place. I get it. I tried moving this around a bunch. No luck.\\nSecond thing I tried… Monkey see- monkey do: The RocksDB way.\\nIn the Flink-RocksDB connector, and other people have given this advice, the idea is to include the JNI in the resources/fat-jar, then write out a tmp one and have that loaded.\\nThis, for me, resulted in seemingly one tmp copy being generated for each record processed.\\n\\nimport java.io._\\n\\n/**\\n* This is an example of an extremely stupid way (and consequently the way Flink does RocksDB) to handle the JNI problem.\\n*\\n* DO NOT USE!!\\n*\\n* Basically we include libopencv_java330.so in src/main/resources so then it creates a tmp version.\\n*\\n* I deleted from it resources, so this would fail. Only try it for academic purposes. E.g. to see what stupid looks like.\\n*\\n*/\\nobject NativeUtils {\\n   // heavily based on https://github.com/adamheinrich/native-utils/blob/master/src/main/java/cz/adamh/utils/NativeUtils.java\\n    def loadOpenCVLibFromJar() = {\\n\\n        val temp = File.createTempFile(\"libopencv_java330\", \".so\")\\n        temp.deleteOnExit()\\n\\n        val inputStream= getClass().getResourceAsStream(\"/libopencv_java330.so\")\\n\\n        import java.io.FileOutputStream\\n        import java.io.OutputStream\\n        val os = new FileOutputStream(temp)\\n        var readBytes: Int = 0\\n        var buffer = new Array[Byte](1024)\\n\\n        try {\\n           while ({(readBytes = inputStream.read(buffer))\\n              readBytes != -1}) {\\n              os.write(buffer, 0, readBytes)\\n           }\\n        }\\n        finally {\\n        // If read/write fails, close streams safely before throwing an exception\\n           os.close()\\n           inputStream.close\\n        }\\n\\n        System.load(temp.getAbsolutePath)\\n    }\\n\\n}\\n\\nThird way: Sanity.\\nThere were more accurately like 300 hundred ways I tried to make this S.O.B. work, I’m really just giving you the way points- major strategies I tried in my journey. This is the solution. This is the ‘Tomcat solution’ I’d seen referenced throughout my journey but didn’t understand what they meant. Hence why, I’m writing this blog post.\\nI created an entirely new module. I called it org.rawkintrevo.cylons.opencv. In that module there is one class.\\n\\npackage org.rawkintrevo.cylon.opencv;\\n\\nimport org.opencv.core.Core;\\n\\npublic class LoadNative {\\n\\n   static {\\n      System.loadLibrary(Core.NATIVE_LIBRARY_NAME);\\n   }\\n\\n   native void loadNative();\\n}\\n\\nI compiled that as a fat jar and dropped it in flink/lib\\nThen, where I would have run System.loadLibrary(Core.NATIVE_LIBRARY_NAME), I now put Class.forName(\"org.rawkintrevo.cylon.opencv.LoadNative\").\\n\\n   val stream = env\\n      .addSource(rawVideoConsumer)\\n      .map(record => {\\n         // System.loadLibrary(Core.NATIVE_LIBRARY_NAME)\\n         Class.forName(\"org.rawkintrevo.cylon.opencv.LoadNative\")\\n         ...\\n\\nFurther, I copy the OpenCV Java wrapper (opencv/build/bin/opencv-330.jar), and library (opencv/build/lib/opencv_java330.so) in flink/lib\\nThen I have great success and profit.\\nGood hunting,\\ntg\\n\\n\\nLeave a comment \\n\\n\\n\\n\\n \\n\\n\\nBig Data for n00bs: My first streaming Flink program\\xa0(twitter) \\nSeptember 30, 2016March 20, 2017 by rawkintrevo, posted in flink  \\n\\nIt’s learning Friday, and I’m getting this out late so I’ll try to make it short and sweet. In this post we’re going to create a simple streaming program that hits the twitter statuses endpoint.\\nThis is based on a demo from my recent talk at Flink Forward 2016. As always, I will be doing this in Apache Zeppelin notebooks, because I am lazy and don’t like to compile jars.\\nStep 1. Create API Keys\\nGo to twitter application management and create a new app. After you create the application, click on it. Under the Keys and Access Tokens Tab you will see the Consumer Key and Consumer Secret. Scroll down a little ways and you will also see Access Token and Access Token Secret. Leave this tab open, you’ll need it shortly.\\nStep 2. Open Zeppelin, create a new Interpreter for the Streaming Job.\\nIn the first paragraph we are going to define our new interpreter. We need to add the dependency org.apache.flink:flink-connector-twitter_2.10:1.1.2. Also, if you’re running in a cluster you also need to download this jar to $FLINK_HOME/lib and restart the cluster.\\n\\nNOTE If you haven’t updated your Zeppelin in a while, you should do that. You need to have a version of Zeppelin that is using Apache Flink v1.1.2. This is important because that update to Zeppelin also introduced the Streaming context to Flink notebooks in Zeppelin. A quick way to test your if your Zeppelin is OK, is to run the following code in a paragraph, and see no errors.\\n\\n%flink\\n\\nsenv\\n\\n\\nStep 3. Set your authorization keys.\\nRefer to Step 1 for your specific keys, but create a notebook with the following code:\\n\\n%flinkStreamingDemo\\n//////////////////////////////////////////////////////\\n// Enter our Creds\\nimport java.util.Properties\\nimport org.apache.flink.streaming.connectors.twitter.TwitterSource\\n\\nval p = new Properties();\\np.setProperty(TwitterSource.CONSUMER_KEY, &quot;&quot;);\\np.setProperty(TwitterSource.CONSUMER_SECRET, &quot;&quot;);\\np.setProperty(TwitterSource.TOKEN, &quot;&quot;);\\np.setProperty(TwitterSource.TOKEN_SECRET, &quot;&quot;);\\n\\nObviously, plug your keys/tokens in- I’ve deleted mine.\\nStep 4. Create an end point to track terms\\nI’ve created a simple endpoint to capture all tweats containing the words pizza OR puggle OR poland. See the docs for more information on how twitter queries work.\\n\\n%flinkStreamingDemo\\n\\nimport com.twitter.hbc.core.endpoint.{StatusesFilterEndpoint, StreamingEndpoint}\\nimport scala.collection.JavaConverters._\\n\\n// val chicago = new Location(new Location.Coordinate(-86.0, 41.0), new Location.Coordinate(-87.0, 42.0))\\n\\n//////////////////////////////////////////////////////\\n// Create an Endpoint to Track our terms\\nclass myFilterEndpoint extends TwitterSource.EndpointInitializer with Serializable {\\n    @Override\\n    def createEndpoint(): StreamingEndpoint = {\\n        val endpoint = new StatusesFilterEndpoint()\\n        //endpoint.locations(List(chicago).asJava)\\n        endpoint.trackTerms(List(&quot;pizza&quot;, &quot;puggle&quot;, &quot;poland&quot;).asJava)\\n        return endpoint\\n    }\\n}\\n\\nI’ve also commented out, but left as reference how you would do a location based filter.\\nStep 5. Set up the endpoints\\nNothing exciting here, just setting up and initializing the endpoints.\\n\\n%flinkStreamingDemo\\n\\nimport org.apache.flink.streaming.api.scala.DataStream\\nimport org.apache.flink.streaming.api.windowing.time.Time\\nimport org.apache.flink.core.fs.FileSystem.WriteMode\\n\\nval source = new TwitterSource(p)\\nval epInit = new myFilterEndpoint()\\n\\nsource.setCustomEndpointInitializer( epInit )\\n\\nval streamSource = senv.addSource( source );\\n\\nStep 6. Setup the Flink processing and windowing.\\nThis is an embarrassingly simple example. Flink can do so much, but all we’re going to do is show off a little bit of its windowing capability (light years ahead of Apache spark streaming).\\nEverything else is somewhat trivial- this is the code of interest that the user is encouraged to play with.\\n\\n%flinkStreamingDemo\\n\\nstreamSource.map(s =&gt; (0,1))\\n    .keyBy(0)\\n    // sliding time window of 1 minute length and 30 secs trigger interval\\n    .timeWindow(Time.minutes(2), Time.seconds(30))\\n    .sum(1)\\n    .map(t =&gt; t._2)\\n    .writeAsText(&quot;/tmp/2minCounts.txt&quot;, WriteMode.OVERWRITE)\\n\\nsenv.execute(&quot;Twitter Count&quot;)\\n\\nWe count the tweets in each window and the counts by window are saved to a text file. Not very exciting.\\nMore information on windowing in Flink.\\nExercises for the reader:\\n– Different windowing strategies\\n– Doing an action on the tweet like determining which keyword it contains\\n— Count by keyword\\n– Joining windows of different lengths (e.g. how many users tweeted in this window who have also tweeted in the current 24 hour window).\\nEtc.\\nStep 7. Visualize results.\\nBecause there is a two minute window, you need to give this some time collect a window and write the results to 2minCounts.txt. So go grab a coffee, tweet how great this post is, etc.\\nWhen you come back, run the following code:\\n\\n%flink\\nimport scala.io.Source\\n\\nval twoMinCnt = Source.fromFile(&quot;/tmp/2minCounts.txt&quot;).getLines.toList\\n\\nprintln(&quot;%table\\\\ntime\\\\tff_tweets\\\\n&quot; + twoMinCnt.zipWithIndex.map(o=&gt; o._2+&quot;\\\\t&quot;+o._1).mkString(&quot;\\\\n&quot;))\\n\\nThat code will read the text file and turn it in to a table that Zeppelin can understand and render in AngularJS. In this chart, the numbers on the x axis are the ‘epochs’ that is the 2 minute windows offset by 30 seconds each.\\n\\nUPDATE\\nRemember how we created a new interpreter called %flinkStreamingDemo?\\nSo here is the thing with a Flink streaming job, and Zeppelin.  When you ‘execute’ the streaming paragraph from Step 6, that job is going to run forever.\\nIf you are running Flink in a cluster, you have three options:\\n– Restart the interpreter.  This will free the interpreter, but the streaming job will still be running- you can verify this in the Flink WebUI (also, the Flink WebUI is where you can stop the job). Now that control of the interpreter has been returned to you, you can run the visualization code with the %flinkStreamingDemo interpreter.\\n– You can use another Flink interpreter such as %flink.  This will also work fine.\\n– You can use another Scala interpreter (such as the %ignite or %spark interpreter).  This is fine, because the visualization code doesn’t leverage Flink in anyway, only pure Scala to read the file and convert it into a Zeppelin %table\\nIf you’re in local mode (e.g. you don’t have a Flink Cluster running) you’ll likely see an error: java.net.BindException: Address already in use when you try to use another %flink interpreter.  This is an opportunity for improvement in Zeppelin as the FlinkLocalMiniCluster always binds to 6123.  In this case, the only option is #3 from above, simply run the code from any other Scala based interpreter (or use Python or R, but you’ll need to alter the code for those languages). All we need to do here is read the file, give it two headers and create a tab-separated string that starts with %table.\\nFor example, this would work:\\n\\n%spark\\nimport scala.io.Source\\n\\nval twoMinCnt = Source.fromFile(&quot;/tmp/2minCounts.txt&quot;).getLines.toList\\n\\nprintln(&quot;%table\\\\ntime\\\\tff_tweets\\\\n&quot; + twoMinCnt.zipWithIndex.map(o=&gt; o._2+&quot;\\\\t&quot;+o._1).mkString(&quot;\\\\n&quot;))\\n\\nAgain, because for the visualization, we don’t need anything that Flink does. We’re simply loading a file and creating a string.\\nTroubleshooting\\nIf you see NO DATA AVAILABLE you either:\\n– didn’t wait long enough (wait at least the length of the window plus a little)\\n– you entered your credentials wrong\\n– you have a rarely use term and there is no data\\nBad Auth\\ntail f*/log/*taskmanager*logprobably won’t work for you, but you need to check those logs for something that looks like this.\\n\\nThe whole enchilad\\n\\n%flinkStreamingDemo \\nimport java.util.Properties\\nimport org.apache.flink.streaming.connectors.twitter.TwitterSource\\nimport com.twitter.hbc.core.endpoint.{StatusesFilterEndpoint, StreamingEndpoint, Location}\\n\\nimport scala.collection.JavaConverters._\\n\\nimport org.apache.flink.streaming.api.scala.DataStream\\nimport org.apache.flink.streaming.api.windowing.time.Time\\nimport org.apache.flink.core.fs.FileSystem.WriteMode\\n\\n\\n//////////////////////////////////////////////////////\\n// Enter our Creds\\nval p = new Properties();\\np.setProperty(TwitterSource.CONSUMER_KEY, &quot;&quot;);\\np.setProperty(TwitterSource.CONSUMER_SECRET, &quot;&quot;);\\np.setProperty(TwitterSource.TOKEN, &quot;&quot;);\\np.setProperty(TwitterSource.TOKEN_SECRET, &quot;&quot;);\\n\\nval chicago = new Location(new Location.Coordinate(-86.0, 41.0), new Location.Coordinate(-87.0, 42.0))\\n\\n//////////////////////////////////////////////////////\\n// Create an Endpoint to Track our terms\\nclass myFilterEndpoint extends TwitterSource.EndpointInitializer with Serializable {\\n    @Override\\n    def createEndpoint(): StreamingEndpoint = {\\n        val endpoint = new StatusesFilterEndpoint()\\n        //endpoint.locations(List(chicago).asJava) \\n        endpoint.trackTerms(List(&quot;pizza&quot;, &quot;puggle&quot;, &quot;poland&quot;).asJava)\\n        return endpoint\\n    }\\n}\\n\\nval source = new TwitterSource(p)\\nval epInit = new myFilterEndpoint()\\n\\nsource.setCustomEndpointInitializer( epInit )\\n\\nval streamSource = senv.addSource( source );\\n\\nstreamSource.map(s =&gt; (0,1))\\n    .keyBy(0) \\n    // sliding time window of 2 minute length and 30 secs trigger interval\\n    .timeWindow(Time.minutes(2), Time.seconds(30))\\n    .sum(1)\\n    .map(t =&gt; t._2)\\n    .writeAsText(&quot;/tmp/2minCounts.txt&quot;, WriteMode.OVERWRITE)\\n\\n\\nsenv.execute(&quot;Twitter Count&quot;)\\n\\n\\n\\n3 Comments \\n\\n\\n\\n\\n \\n\\n\\nBig Data for n00bs: Gelly on Apache\\xa0Flink \\nSeptember 20, 2016March 20, 2017 by rawkintrevo, posted in big data for noobs, flink, How-Tos, zeppelin  \\n\\nBig Data for n00bs is a new series I’m working on targeted at absolute beginners like my self.¬† The goal is to make some confusing tasks more approachable.¬† The first few posts will be spin offs of a recent talk I gave at Flink Forward 2016 Apache Zeppelin- A Friendlier Way To Flink (will link video when posted).\\nGraph databases are becoming an increasingly popular way to store and analyze data, especially when relationships can be expressed in terms of object verb object.¬† For instance, social networks are usually represented in graphs such as¬†\\nJack likes Jills_picture\\nA full expose on the uses and value of graph databases is beyond the scope of this blog post however the reader is encouraged to follow these links for a more in depth discussion:\\n\\nWikipedia- Graph Database\\nIBM developerWorks- Processing large-scale graph data: A guide to current technology\\nneo4j\\n\\n¬†\\nGelly Library on Apache Flink\\nGelly is the Flink Graph API.\\n\\nUsing d3js and Apache Zeppelin to Visualize Graphs\\nFirst, download Apache Zeppelin (click the link and choose the binary package with all interpreters) then “install” Zeppelin by unzipping the downloaded file and running bin/zeppelin-daemon.sh start or bin/zeppelin.cmd (depending on if you are using windows or Linux / OSX).¬† See installation instructions here.\\nAfter you’ve started Zeppelin, open a browser and go to http://localhost:8080\\nYou should see a “Welcome to Zeppelin” page.¬† We’re going to create a new notebook by clicking the “Notebook” drop down, and the “+Create new note”.\\n\\nCall the notebook whatever you like.\\nAdd dependencies to the Flink interpreter\\nNext we need to add two dependencies to our Flink interpreter.¬† To do this we go to the “Interpreters” page, find the “Flink” interpreter and add the following dependencies:\\n\\ncom.typesafe.play:play-json_2.10:2.4.8\\n\\nused for reading JSONs\\n\\n\\norg.apache.flink:flink-gelly-scala_2.10:1.1.2\\n\\nused for the Flink Gelly library\\n\\n\\n\\nWe’re also going to exclude com.typesafe:config from the typesafe dependency.¬† This packaged tends to cause problems and is not necessary for what we are doing, so we exclude it.\\nThe dependencies section of our new interpreter will look something like this:\\n\\nDownloading some graph data\\nGo back to the notebook we’ve created.¬† In the first paragraph add the following code\\n%sh\\nmkdir tmp\\nwget https://raw.githubusercontent.com/d3/d3-plugins/master/graph/data/miserables.json -O tmp/miserables.json\\n\\nIt should look like this after you run the paragraph (clicking the little “play” button in top right corner of paragraph):\\n\\nWhat we’ve done there is use a Linux command wget to download our data. It is also an option to simply download the data your browser, you could for example right click on this link and click “Save As…” but if you do that, you’ll need to edit the next paragraph to load the data from where ever you saved it to.\\nVisualizing Data with d3js\\nd3js is a Javascript library for making some really cool visualizations. A fairly simple graph visualization was selected to keep this example fairly simple; a good next step would be to try a more advanced visualization.\\nFirst we need to parse our json:\\n\\nimport¬† scala.io.Source\\nimport play.api.libs.json._\\nimport org.apache.flink.graph.scala.Graph\\nimport org.apache.flink.graph.Edge\\nimport org.apache.flink.graph.Vertex\\n\\nimport collection.mutable._\\nimport org.apache.flink.api.scala._\\n\\nval dataJson = Source.fromFile(\"/home/guest/tmp/miserables.json\").getLines.toList.mkString\\nval json: JsValue = Json.parse(dataJson)\\n\\n\\n¬†\\nWe’re going to have some output that looks like this/\\nFor this hack, we’re going to render our d3js, by creating a string that contains our data.\\n(This is very hacky, but super effective).\\n\\n%flinkGelly\\nprintln( s\"\"\"%html\\n<style>\\n\\n.node {\\n  stroke: #000;\\n  stroke-width: 1.5px;\\n}\\n\\n.link {\\n  fill: none;\\n  stroke: #bbb;\\n}\\n\\n</style>\\n<div id=\"foo\">\\n\\n\\nvar width = 960,\\n    height = 300\\n\\nvar svg = d3.select(\"#foo\").append(\"svg\")\\n    .attr(\"width\", width)\\n    .attr(\"height\", height);\\n\\nvar force = d3.layout.force()\\n    .gravity(.05)\\n    .distance(100)\\n    .charge(-100)\\n    .size([width, height]);\\n\\nvar plot = function(json) {\\n\\n  force\\n      .nodes(json.nodes)\\n      .links(json.links)\\n      .start();\\n\\n  var link = svg.selectAll(\".link\")\\n      .data(json.links)\\n    .enter().append(\"line\")\\n      .attr(\"class\", \"link\")\\n    .style(\"stroke-width\", function(d) { return Math.sqrt(d.value); });\\n\\n  var node = svg.selectAll(\".node\")\\n      .data(json.nodes)\\n    .enter().append(\"g\")\\n      .attr(\"class\", \"node\")\\n      .call(force.drag);\\n\\n  node.append(\"circle\")\\n      .attr(\"r\",\"5\");\\n\\n  node.append(\"text\")\\n      .attr(\"dx\", 12)\\n      .attr(\"dy\", \".35em\")\\n      .text(function(d) { return d.name });\\n\\n  force.on(\"tick\", function() {\\n    link.attr(\"x1\", function(d) { return d.source.x; })\\n        .attr(\"y1\", function(d) { return d.source.y; })\\n        .attr(\"x2\", function(d) { return d.target.x; })\\n        .attr(\"y2\", function(d) { return d.target.y; });\\n\\n    node.attr(\"transform\", function(d) { return \"translate(\" + d.x + \",\" + d.y + \")\"; });\\n  });\\n}\\n\\nplot( $dataJson )\\n\\n\\n</div>\\n\"\"\")\\n\\nNow, check out what we just did there: println(s\"\"\"%html ... $dataJson. We just created a string that started with the %html tag, letting Zeppelin know, this is going to be a HTML paragraph, render it as such, and then passed the data directly in. If you were to inspect the page you would see the entire json is present in the html code.\\nThis is the (messy) graph we get.\\nFrom here, everything is a trivial exercise.\\nLet’s load this graph data into a Gelly Graph:\\n\\nval vertexDS = benv.fromCollection(\\n(json \\\\ \"nodes\" \\\\\\\\ \"name\")\\n.map(_.toString).toArray.zipWithIndex\\n.map(o => new Vertex(o._2.toLong, o._1)).toList)\\n\\nval edgeDS = benv.fromCollection(\\n((json \\\\ \"links\" \\\\\\\\ \"source\")\\n.map(_.toString.toLong) zip (json \\\\ \"links\" \\\\\\\\ \"target\")\\n.map(_.toString.toLong) zip (json \\\\ \"links\" \\\\\\\\ \"value\")\\n.map(_.toString.toDouble))\\n.map(o => new Edge(o._1._1, o._1._2, o._2)).toList)\\n\\nval graph = Graph.fromDataSet(vertexDS, edgeDS, benv)\\n\\nWoah, that looks spooky. But really is not bad. The original JSON contained a list called nodes which held all of our vertices, and a list called links which held all of our edges. We did a little hand waving to parse this into the format expected by Flink to create an edge and vertex DataSet respectively.\\nFrom here, we can do any number of graph operations on this data, and the user is encouraged to do more. For illustration, I will perform the most trivial of tasks: filtering on edges whose value is greater than 2.\\n\\nval filteredGraph = graph.filterOnEdges(edge => edge.getValue > 2.0)\\n\\nNow we convert our data back in to a json, and use the same method to re-display the graph. This is probably the most complex operation in the entire post.\\n\\nval jsonOutStr = \"\"\"{\"nodes\": [ \"\"\".concat(filteredGraph.getVertices.collect().map(v => \"\"\"{ \"name\": \"\"\" + v.getValue() + \"\"\" } \"\"\").mkString(\",\"))\\n.concat(\"\"\" ], \"links\": [ \"\"\")\\n.concat(filteredGraph.getEdges.collect().map(e => s\"\"\"{\"source\": \"\"\" + e.getSource() + \"\"\", \"target\": \"\"\" + e.getTarget + \"\"\", \"value\": \"\"\" + e.getValue + \"\"\"}\"\"\").mkString(\",\"))\\n.concat(\"] }\")\\n\\nAs we see we are creating a json string from the edges and vertices of the graph. We call filteredGraph.getVertices.collect() and then map those vertices into the format expected by the json. In this case, our rendering graph expects a list of dictionaries of the format { \"name\" : }. The edges follow a similar pattern. In summation though we are simply mapping a list of of collected vertices/edges to string representations in a json format.\\nFinally, we repeat our above procedure for rendering this new json. An imporant thing to note, our code for mapping the graph to the json will work for this no matter what operations we perform on the graph. That is to say, we spend a little time setting things up, from a perspective of translating our graphs to jsons and rendering our jsons with d3js, and then we can play as much as we want with our graphs.\\n\\nprintln( s\"\"\"\\n<style>\\n\\n.node {\\n  stroke: #000;\\n  stroke-width: 1.5px;\\n}\\n\\n.link {\\n  fill: none;\\n  stroke: #bbb;\\n}\\n\\n</style>\\n<div id=\"foo2\">\\n\\n\\nvar width = 960,\\n    height = 500\\n\\nvar svg = d3.select(\"#foo2\").append(\"svg\")\\n    .attr(\"width\", width)\\n    .attr(\"height\", height);\\n\\nvar force = d3.layout.force()\\n    .gravity(.05)\\n    .distance(100)\\n    .charge(-100)\\n    .size([width, height]);\\n\\nvar plot = function(json) {\\n\\n  force\\n      .nodes(json.nodes)\\n      .links(json.links)\\n      .start();\\n\\n  var link = svg.selectAll(\".link\")\\n      .data(json.links)\\n    .enter().append(\"line\")\\n      .attr(\"class\", \"link\")\\n    .style(\"stroke-width\", function(d) { return Math.sqrt(d.value); });\\n\\n  var node = svg.selectAll(\".node\")\\n      .data(json.nodes)\\n    .enter().append(\"g\")\\n      .attr(\"class\", \"node\")\\n      .call(force.drag);\\n\\n  node.append(\"circle\")\\n      .attr(\"r\",\"5\");\\n\\n  node.append(\"text\")\\n      .attr(\"dx\", 12)\\n      .attr(\"dy\", \".35em\")\\n      .text(function(d) { return d.name });\\n\\n  force.on(\"tick\", function() {\\n    link.attr(\"x1\", function(d) { return d.source.x; })\\n        .attr(\"y1\", function(d) { return d.source.y; })\\n        .attr(\"x2\", function(d) { return d.target.x; })\\n        .attr(\"y2\", function(d) { return d.target.y; });\\n\\n    node.attr(\"transform\", function(d) { return \"translate(\" + d.x + \",\" + d.y + \")\"; });\\n  });\\n}\\n\\nplot( $jsonOutStr )\\n\\n\\n</div>\\n\"\"\")\\n\\nAlso note we have changed $dataJson to $jsonOutStr as our new graph is contained in this new string.\\nA final important call out is the d3.select(\"#foo2\") and <div id=\"foo2\"> in the html string.  This is creating a container for the element, and then telling d3js where to render the element. This was the hardest part for me; before I figured this out, the graphs kept rendering on the grey background behind the notebooks- which is cool, if that’s what you’re going for (custom Zeppelin skins anyone?), but very upsetting if it is not what you want.\\nNew filtered graph.\\nConclusions\\nApache Zeppelin is a rare combination of easy and powerful.¬† Simple things like getting started with Apache Flink and the Gelly graph library are fairly simple, however we are still able to add in powerful features such as d3js visualizations, with relatively little work.\\n¬†\\n\\n\\nTagged big data, d3js, distributed computing, flink, graph processing, zeppelinLeave a comment \\n\\n\\n\\n\\n \\n\\n\\nDeep Magic Volume 1: Visualizing Apache Mahout in R via Apache Zeppelin\\xa0(incubating) \\nMay 19, 2016March 20, 2017 by rawkintrevo, posted in flink, How-Tos, mahout, zeppelin  \\n\\nI was at Apache Big Data last week and got to talking to some of the good folks at the Apache Mahout project. ¬†For those who aren’t familiar, Apache Mahout is a rich Machine Learning and Linear Algebra Library that originally ran on top of Apache Hadoop, and as of recently runs on top of Apache Flink and Apache Spark. It runs in the interactive Scala shell but exposes a domain specific language that makes it feel much more like R than Scala.\\nWell, the Apache Mahout folks had been wanting to build out some visualization capabilities comparable to matplotlib¬†and ggplot2 (Python and R respectively). ¬†They had considered integrating with Apache Zeppelin and utilizing the AngularJS framework native to Zeppelin. ¬†We talked it out, and decided it made much more sense to simply¬†use¬†the¬†matplotlib¬†and ggplot2¬†features of Python and R, and Apache Zeppelin could ¬†to facilitate that somewhat cumbersome pipeline.\\nSo I dinked around with it Monday and Tuesday, learning my way around Apache Mahout, and overcoming an issue with an upgrade I made when I rebuilt Zeppelin (in short I needed to refresh my browser cache…).\\nWithout further ado, here is a guide on how to get started playing with Apache Mahout yourself!\\nStep 1. Clone / Build Apache Mahout\\nAt the bash shell (e.g. command prompt, see my other blog post on setting up Zeppelin + Flink + Spark), enter the following:\\n\\ngit clone https://github.com/apache/mahout.git\\ncd mahout\\nmvn clean install -DskipTests\\nThat will install Apache Mahout.\\nStep 2. Create/Configure/Bind New Zeppelin Interpreter\\nStep 2a. Create\\nNext we are going to create a new Zeppelin Interpreter.\\nIn the interpreters page, at the top right, you’ll see a button that says: “+Create”. Click on that.\\nWe’re going to name this ‘spark-mahout’ (thought the name is not important).\\nOn the interpreter drop-down we’re going to select Spark.\\n\\nStep 2b. Configure\\nWe’re going to add the following properties and values by clicking the “+” sign at the bottom of the properties list:\\n\\n\\n\\nProperty\\nValue\\n\\n\\nspark.kryo.registrator\\norg.apache.mahout.sparkbindings.io.MahoutKryoRegistrator\\n\\n\\nspark.serializer\\norg.apache.spark.serializer.KryoSerializer\\n\\n\\nspark.kryo.referenceTracking\\nfalse\\n\\n\\nspark.kryoserializer.buffer\\n300m\\n\\n\\n\\nAnd below that we will add the following artifacts to the dependencies (no value necessary for the ‘exclude’ field)\\n\\n\\n\\nArtifact\\nExclude\\n\\n\\n/home/username/.m2/repository/org/apache/mahout/mahout-math/0.12.1-SNAPSHOT/mahout-math-0.12.1-SNAPSHOT.jar\\n\\n\\n\\n/home/username/.m2/repository/org/apache/mahout/mahout-math-scala_2.10/0.12.1-SNAPSHOT/mahout-math-scala_2.10-0.12.1-SNAPSHOT.jar\\n\\n\\n\\n/home/username/.m2/repository/org/apache/mahout/mahout-spark_2.10/0.12.1-SNAPSHOT/mahout-spark_2.10-0.12.1-SNAPSHOT.jar\\n\\n\\n\\n/home/username/.m2/repository/org/apache/mahout/mahout-spark-shell_2.10/0.12.1-SNAPSHOT/mahout-spark-shell_2.10-0.12.1-SNAPSHOT.jar\\n\\n\\n\\n/home/username/.m2/repository/org/apache/mahout/mahout-spark_2.10/0.12.1-SNAPSHOT/mahout-spark_2.10-0.12.1-SNAPSHOT-dependency-reduced.jar\\n\\n\\n\\n\\nMake sure to click ‘Save’ when you are done. Also, maybe this goes without saying, maybe it doesn’t… but\\nmake sure to change username to your actual username, don’t just copy and paste!\\n\\nStep 2c. Bind\\nIn any notebook in which you want to use the spark-mahout interpreter, not the regular old Spark one, you need to bind correct interpreter.\\nCreate a new notebook, lets call it “[MAHOUT] Binding Example”.\\nIn the top right, you’ll see a little black gear, click on it. A number of interpreters will pop up. You want to click on the Spark one at the top (such that is becomes un-highlighted) then click on the “spark-mahout” one toward the bottom. Finally drag the “spark-mahout” one up to the top. Finally, as always, click on ‘Save’.\\nNow, this notebook knows to use the spark-mahout interpreter instead of the regular spark interpreter (and so, all of the properties and dependencies you’ve added will also be used). ¬†You’ll need to do this for every notebook in which you wish to use the Mahout¬†Interpreter!\\n\\n¬†\\nStep 2d. Setting the Environment\\nBack at the command prompt, we need to tweek the environment a bit. At the command prompt (assuming you are in the mahout directory still):\\n\\n./bin/mahout-load-spark-env.sh \\n\\nAnd then we’re going to export some environment variables:\\n\\nexport MAHOUT_HOME=[directory into which you checked out Mahout]\\nexport SPARK_HOME=[directory where you unpacked Spark]\\nexport MASTER=[url of the Spark master]\\n\\nIf you are going to be using Mahout often, it would be wise to add those exports to $ZEPPELIN_HOME/conf/zeppelin-env.sh so they are loaded every time.\\nStep 3. Mahout it up!\\nI don’t like to repeat other people’s work, so I’m going to direct you to another great article explaining how to do simple matrix based linear regression. https://mahout.apache.org/users/sparkbindings/play-with-shell.html\\nI’m going to do you another favor. Go to the Zeppelin home page and click on ‘Import Note’. When given the option between URL and json, click on URL and enter the following link:\\nhttps://raw.githubusercontent.com/rawkintrevo/mahout-zeppelin/master/%5BMAHOUT%5D%5BPROVING-GROUNDS%5DLinear%20Regression%20in%20Spark.json\\nThat should run, and is in fact the Zeppelin version of the above blog post.\\nThe key thing I will point out however is the top of the first paragraph:\\n\\nimport org.apache.mahout.math._\\nimport org.apache.mahout.math.scalabindings._\\nimport org.apache.mahout.math.drm._\\nimport org.apache.mahout.math.scalabindings.RLikeOps._\\nimport org.apache.mahout.math.drm.RLikeDrmOps._\\nimport org.apache.mahout.sparkbindings._\\n\\nimplicit val sdc: org.apache.mahout.sparkbindings.SparkDistributedContext = sc2sdc(sc)\\n\\nThat is where the magic happens and introduces Mahout’s SparkDistributedContext and the R-like Domain Specific Language.\\nYou know how in Scala you can pretty much just write whatever you want (syntactic sugar run-amok) well a domain specific language (or DSL) lets you take that even further and change the syntax even further. This is not a precisely accurate statement, feel free to google if you want to know more.\\nThe moral of the story is: what was Scala, now smells much more like R.\\nFurther, for the rest of this notebook, you can now use the Mahout DSL, which is nice because it is the same for Flink and Spark. What that means is you can start playing with this right away using Spark-Mahout, but when the Flink-Mahout comes online soon (and I promise to update this post showing how to hook it up) you can copy/paste your code to your Flink-Mahout paragraphs and probably run it a bunch faster.\\nThe Main Event\\nSo the whole point of all of this madness was to monkey-patch Mahout into R/Python to take advantage of those graphics libraries.\\nI’ve done you another solid favor. Import this notebook:\\nhttps://raw.githubusercontent.com/rawkintrevo/mahout-zeppelin/master/%5BMAHOUT%5D%5BPROVING-GROUNDS%5DSpark-Mahout%2Bggplot2.json\\nUPDATE 5-29-16:¬†Originally, I had accidentally re-linked the first notebook (sloppy copy-paste on my part)- this one shows ggplot2 integration, e.g. the entire point of this Blog post…\\nIgnore the first couple of paragraphs (by the time you read this I might have (unlikely, lol) cleaned this notebook up and deleted).\\nThere is a paragraph that Creates Random Matrices\\n\\n…yawn. You can grok it later. But again, notice those imports and creating the SparkDistributedContext. ¬†We’re using our SparkContext (sc¬†) that Zeppelin automatically creates in the paragraph to initialize¬†this.\\nIn the next paragraph we sample 1000 rows from the matrix. Why a sample? Well in theory the whole point of using Mahout is you’re going to be working with matrices much to big to fit in the memory of a single machine, much less graph them in any sort of meaningful way (think millions to trillions to bajillions of rows). How many do you really need. If you want to get a feel for the matrix as a whole, random sample. Depending on what you’re trying to do will determine how exactly you sample the matrix, just be advised- it is a nasty habit to think you are just going to visualize the whole thing (even though it is possible on these trivial examples). If that were possible in the first place on your real data, you’d have actually been better served to just used R to begin with…\\nThe next paragraph basically converts the matrix into a tab-separated-file, except it is held as a string and never actually written to disk. This loop is effective, but not ideal. In the near future we hope to wrap some syntactic sugar around this, simply exposing a method on the matrix that spits out a sampled *.tsv.¬†Once there exists a tab-separated string, we can add %table¬†to the front of the string and print it- Zeppelin will automatically figure out this is supposed to be charted and you can see here how we could use Zeppelin’s predefined charts to explore this table.\\n\\nKeeping in mind this matrix was a sine function, this sampling looks more or less accurate. ¬†The Zeppelin graph is trying to take some liberties though and do aggregations on one of the columns. ¬†To be fair, we’re trying to do something weird here; something for which this chart wasn’t intended for.\\nNext, the tsv string is then¬†stored in something known to Zeppelin as the ResourcePool. ¬†Almost any interpreter can access the resource pool and it is a great way to share data between interpreters.\\nOnce we have a *.tsv in memory, and it’s in the resource pool, all that is left is to “fish it out” of the resource pool and load it as a dataframe. That is an uncommon but not altogether unheard of thing to do in R via the read.table function.\\nThanks to all of the work done on the SparkR-Zeppelin integration, we can now load our dataframe and simply use ggplot2 or a host of other R plotting packages (see the R tutorial).\\n\\nA post thought\\nAnother way to skin this cat would be to simply convert the Mahout Matrix to an RDD and then register it as a DataFrame in Spark. That is correct, however the point of Mahout is to be engine agnostic, and as Flink is mainly focused on streaming data and not building out Python and R extensions, it is unlikely a similar functionality would be exposed there.\\nHowever, you’re through the looking-glass now, and if doing the distributed row matrix -> resilient distributed data set -> Spark data frame -> read in R makes more sense to you/your use case, go nuts. Write a blog of your own and link back to me \\uf8ffüòâ\\n¬†\\n\\n\\nTagged apache, big data, distributed computing, ggplot2, linearalgebra, machinelearning, mahout, matplotlib, r, spark, zeppelin4 Comments \\n\\n\\n\\n\\n \\n\\n\\nHow to get paid a gagillion dollars building crazy big data stuff on nights and\\xa0weekends. \\nNovember 3, 2015March 20, 2017 by rawkintrevo, posted in Engineering, flink, How-Tos, zeppelin  \\n\\nUPDATE 5-23:¬†The world and ecosystem of Big Data evolves quickly. ¬†Most of these tools have gone through multiple releases since I first penned this article. ¬†I’ve tried to update accordingly. Good hunting.\\nThat title is a lie, probably.\\nSpark is the hot new thing in big data. Flink will be the hot new thing in big data as internet-of-things, real-time analytics, and other buzz-words go from being stary-eyed promises made in sales pitches to things that actually happen. At the end of the day, most people don’t want to get their hands dirty monkeying around with the mechanics of this stuff, they just want a pretty web interface to use.\\nSo to get paid a gagillion dollars, you basically just start tinkering with this and maybe contribute a bit here and there, then in 2 years when Flink and Spark are the new Microsoft Excel, you’re one of a couple thousand people in the world who have been working with this stuff for over a year. #!/bin/sh (pronounced ‘sha-bang’, more computer jokes, I digress) you’re getting paid a gagillion a year.\\nLet’s be real. Your proposed analytics stack could do some sci-fi black-magic analytics that perfectly predicts all of your KPIs, lottery numbers, and what color of ChuckTs you should rock with that awesome new dinosaur sweater you just got, but if it doesn’t have a sparkly, friendly, not-scary front-end you’re going to have a hard time getting any traction with it. (If you’re not doing this sort of thing day-to-day, then I’m sorry to say, this is the reality of things: people are inherently uncomfortable with submitting jobs via command line.)\\nUse Case #2: Having a pretty front end for your Spark / Flink like DataArtisans or DataBricks is nice, but for whatever reason you can’t put your data out on some cloud.\\nBecause that one time that this happened…\\nSo with out further ado, I present a nice recipe for setting up Apache Flink, Apache Spark, and Apache Zeppelin(incubating) in big-boy mode. (big-boy mode: Zeppelin comes pre-packaged with Flink and Spark, but you want to be pointing at full blown clusters of both of these because, y’know, science).\\nIngredients:\\n\\nA machine, real or virtual\\nUbuntu Server 14.04.3 LTS\\ngit\\nopenssh-server\\nmaven version 3.1+\\njdk 1.7+\\nApache Zeppelin\\nApache Flink\\nApache Spark\\n\\nPrep time: 2 hours.\\nSkill Level: Knows just enough to be dangerous\\nBut really, this is a heavily documented (over documented?) recipe. It assumes no familiarity with linux and provides little blurbs and links about what each command is doing. ¬†I am an autodidact when it comes to computers and while blindly following recipes is nice for getting something going it doesn’t teach you much and if the slightest thing goes wrong you are totally lost. So proceed with no fear.\\nStep 0:\\nBecause jebus-only-knows what kind of wierd-o setup any given box could have, I present this recipe on a fishbone (that is minimal install) Ubuntu Server 14.04.3 virtual machine. ¬†For demo purposes, this minimizes instances where\\n\\nsomething weird is causing unique problems and\\nI miss a dependency because I have it on my computer, but you may not.\\n\\nThere are lots of tutorials on how to install Ubuntu on a virtual machine, but to be honest, if this step scares you, you should plan on this being a full¬†day project or more. ¬†It’s really nothing more than a recipe, but you’re going to be doing a lot of learning along the way.\\nMake sure to setup the virtual box to use a bridged network adapter. A good write up on the differences can be found here. In short, in Oracle VirtualBox go in to the machine Settings -> Networking and select Bridged Adapter from the drop down.\\nSelect Bridged Adapter from the drop down menu.\\nA bridged adapter basically has the fewest restrictions on the virtual machine. We’re going to be installing a lot of things that will be accessing ports on the host machine and this is just going to be a lot simple. If you’re curious go read more.\\nGotchya\\nThroughout this tutorial I’ll point out things that seem trivial but if you variate from the script even slightly can totally derail you. I do this because I am not one to carefully follow directions and provide this information for others like me. In this case, the ‘gotchya’ is if the virtual machine is already running when you change this setting you need to restart the machine for the changes to take affect.\\nInstall Ubuntu 14.04.3\\nA good tutorial is here.\\nGotchya\\nI only attest to this recipe working on a bare bones Ubuntu 14.04.3 Server installation (the link to exactly what I used is in the ingredients). If you decide to use another version of Ubuntu or flavor of Linux, you may have to tweak some things. Zeppelin, Flink, and Spark are all written in Java/Scala so theoretically this could be done on OSX or Windows, but you wouldn’t run a cluster on Windows or OSX boxes, and for a number of other reasons, I’ve chosen Ubuntu 14.04.3. Get this working then try to do something weird if you want.\\nStep 1- Prepping the Box\\nSome basic applications that will be required by the programs we are going to use. Software in Ubuntu is managed via apt. Generally speaking there are three ways to get software.\\n\\nDownload from the repository\\nDownload binaries directly\\nDownload and compile from source\\n\\nIn this episode, we’ll be doing all three. If you’re coming from windows and used to everything being pre-compiled for you with a nice pretty GUI installer… I don’t know what to tell you, other than ‘Welcome to Linux, this is life now.’\\nAny time you see the sudo apt-get we are telling the computer:\\n\\nsudo literally: super user do\\napt-get install use the package manager to install the requested software.\\n\\nSo we are using apt-get to install:\\ngit\\ngit is a program for software version control. We’ll be using to download the latest source code for programs we’re going to compile.\\nsudo apt-get install git\\nopenssh-server\\nA basic Secure Shell server.\\nsudo apt-get install openssh-server\\nOpenJDK 7\\nThe Java Development Kit version 7.\\nsudo apt-get install openjdk-7-jdk openjdk-7-doc openjdk-7-jre-lib\\nBut Maven 3+ isn’t in the repository at the time of this writing. That is to say, if we use apt-get we will get a version of maven that is to old for what we need. So for maven, we are going to download a binary distribution and manually copy it into place.\\nMaven 3.1+\\nIn the same way apt-get is a neat way to manage software which is kept in a repository, maven manages code libraries for us. Check out https://maven.apache.org/ for more info.\\nIf you already have maven installed, we can use apt to remove software as well as install it.\\nsudo apt-get purge maven maven2\\n*note if you don’t have maven installed, this command isn’t going to hurt anything, you’re just going to see an error message about how there was nothing to uninstall.\\nInstalling maven 3.3.3 quick and dirty\\nDownload the maven 3.3.3 binary\\nwget is a CLI (command line interface) downloader.\\nwget \"http://www.us.apache.org/dist/maven/maven-3/3.3.3/binaries/apache-maven-3.3.3-bin.tar.gz\"\\nUnzip the binary (tar -zxvf) and then move (sudo mv) it to /usr/local\\ntar -zxvf apache-maven-3.3.3-bin.tar.gz\\nsudo mv ./apache-maven-3.3.3 /usr/local\\nFinally, create a symbolic link (ln -s) from /usr/bin to the new version. /usr/bin is one of the places Ubuntu looks for programs by default.\\nsudo ln -s /usr/local/apache-maven-3.3.3/bin/mvn /usr/bin/mvn\\nInstalling Zeppelin\\nAt the time this article went to press, the main branch of Zeppelin didn’t have support for Flink 0.10 (which at the time of press was the current stable release of Flink). There is a discussion here, but the short version is you either need to hack Zeppelin yourself or use Till Rohrmann’s branch. ¬†For parsimony, I present the second method and leave ‘How to hack out Zeppelin’ as content for another post…\\nFirst, git clone Apache Zeppelin. This is the third method of getting software discussed earlier. We’re downloading source code to compile.\\ngit clone https://github.com/tillrohrmann/incubator-zeppelin.git\\ngit clone https://github.com/apache/incubator-zeppelin.git\\nUPDATE 5-23:¬†Originally we wanted to get Flink v0.10 and Till’s branch had this, now the real Zeppelin is updated to branch 1.0, so we got to the (real) source.\\nchange directory (cd) to incubator-zeppelin\\ncd incubator-zeppelin\\nA git can have multiple branches. A good overview is here. We want to checkout the branch that Till made with the Zeppelin configuration for flink-0.10-SNAPSHOT.\\ngit checkout flink-0.10-SNAPSHOT\\nNow we instruct maven (mvn) to clean and package the source found in the directory (more on maven build life-cycles). Additionally we pass flags to maven instructing it to build against Spark version 1.5 (-Pspark-1.5) and to skip the tests that make sure it compiled correctly.¬† See Gotchya below.\\nUPDATE 5-23:¬†This all seems to be working now. ¬†We add flags -Psparkr -Ppyspark -Pspark-1.6¬†to make Zeppelin build against Spark 1.6 (included since last time), add support for SparkR and support for pyspark. ¬†At the time of writing -Dflink.version=1.0¬†isn’t necessary, but will hopefully keep this working for a little while longer, especially after Flink v1.1 is released.\\nmvn clean package -DskipTests -Psparkr -Ppyspark -Pspark-1.6 -Dflink.version=1.0\\nGotchya:\\nI explicitly didn’t use the -Pspark-1.5 flag. If I had, it would have built Zeppelin with an internal Spark interpreter at version 1.5. I was having all sorts of issues when doing this, and finally rolled back to make this a simple-as-possible case. If you want to try your hand at Spark 1.5, then add that flag and in the next section when you install Spark, checkout version 1.5 instead.\\nThe maven build will take a little while (26 minutes for me). When it is done, you should see a message saying BUILD SUCEESS and some statistics.\\nAnd finally…drumroll… the moment you’ve all been waiting for …¬†start the Zeppelin daemon.\\nsudo bin/zeppelin-daemon.sh start\\nGotchya\\nYou must use sudo when you start the zeppelin deamon. The onus is on you to remember to do this. It is absolutely possible to start the daemon without sudo and you will be able to run the Flink example listed below, however the Spark example won’t work. The Zeppelin internal Spark interpreter needs super user privileges for creating databases and other various writes.\\nUPDATE 5-23:¬†Don’t use sudo¬†. ¬†If you do it once, you’ll have to do it always, and having Zeppelin running as super user is unwise and unnecessary.\\nTest flight of our new Zeppelin…\\nFirst determine the local IP address of the machine hosting Zeppelin.\\nifconfig\\nThis is the output from my machine, your numbers will be different.\\nSee that the IP of my machine is 192.168.1.109, yours will be different. In subsequent screenshots, in the browser address you will see this IP, however for those following along at home, you need to use your own IP address.\\nOpen a browser and surf to http://yourip:8080, where yourip is the IP you found in the inet addr: field under the eth0 section. Port 8080 is the default port of the Zeppelin WebUI.\\nGuten Tag, Zeppelin.\\nOpen the Tutorial Notebook by clicking on Notebook -> Zeppelin Tutorial\\nDo this.\\nWhen you open the tutorial notebook it will ask you to bind the interpreters, just do it by clicking save. Now run all of the examples in the notebook to make sure they are working. You can do this by going to each cell and clicking Shift+Enter or by clicking the little play button at the top of the note.\\n\\nNow we are going to do a couple of simple examples in Flink and Spark. Zeppelin comes pre-built with its own Flink and Spark interpreters, and will use these until we have it pointed at our own cluster (which happens later). For now, we’re going to test some basic functionality of Zeppelin by running a Flink and a Spark word count example against the internal interpreters.\\nFlink Example\\nFirst, create a new notebook. Do this by clicking on Notebook -> Create New Notebook. Name this notebook “flink example”. Zeppelin doesn’t automatically open the notebook you’ve created, you’ll have to click on Notebook again and the name you gave the new notebook will appear in the list.\\nYou can find a Flink word count gist here. Copy and paste the code from the gist into the Zeppelin note and either hit Shift+Enter or click the play button to run the paragraph.\\nHopefully you see something like this…\\nSpark Example\\nCreate another new notebook, call this one “spark example”, and open it.\\nCopy and paste the gist from here.\\nAssuming your examples are working go ahead and stop Zeppelin.\\nbin/zeppelin-daemon.sh stop\\nInstalling Flink and Spark Clusters\\nThere are lots of how to guides for getting full blown Flink and Spark clusters set up. For this example, we’re just going to install a stand alone of each. The important thing in this tutorial is how to get Zeppelin aimed at Flink and Spark instances outside of the ones that come prepackaged. These external versions can be scaled/built/setup to suit your use case.\\nDownload, checkout, and start Flink\\nDownload\\nWe change directory back to our home directory\\ncd $HOME\\nThen clone the Apache Flink repository\\ngit clone https://github.com/apache/flink.git\\nThen check out release-0.10 ¬†1.0\\ngit checkout release-0.10\\ngit checkout release-1.0\\nUPDATE 5-23:¬†We’re on release 1.0 now. ¬†Release 1.1 (which is what the master¬†branch is on has some cool new stuff like streaming in the shell, but will also break backwards compatibility, e.g. it won’t work). I have a PR that makes it work, but I’ll save that for a future blog post.\\nAnd finally, build the package.\\nmvn clean package -DskipTests\\nBuilding Flink took 20 minutes on my virtual box.\\nStart Flink\\nNow start Flink with the following command:\\nbuild-target/bin/start-cluster.sh\\nNow go to http://yourip:8081 and check out the Flink web-ui.\\nOh, hello there new friend.\\nMake sure there is something listed under the task manager. If there is nothing stop and restart the flink cluster like this:\\nbuild-target/bin/stop-cluster.sh\\nbuild-target/bin/start-cluster.sh\\nDownload, checkout, and start Spark\\nDownload\\nWe change directory back to our home directory\\ncd $HOME\\nThen clone the Apache Spark repository\\ngit clone https://github.com/apache/spark.git\\nThen check out branch-1.4 1.6\\ngit checkout branch-1.4\\ngit checkout branch-1.6 -Psparkr\\nUPDATE 5-23:¬†We’re on branch 1.6 now, and we want SparkR support.\\nAnd finally, build the package.\\nmvn clean package -DskipTests\\nBuilding Spark took 38 minutes on my virtual box.\\nStart Spark\\nIn a cluster, you have a boss that is in charge of distributing the work and collecting the results and a worker that is in charge of actually doing the work. In Spark these are referred to as the master and slave respectively.\\nIn Flink we could start an entire stand alone cluster in one line. In Spark, we must start each individually. We start the master with a flag --webui-port 8082. By default the webui-port is 8080, which is already being used by Zeppelin.\\nsbin/start-master.sh --webui-port 8082\\nNow go check out the Spark master web-ui. It will be at http://yourip:8082.\\nspark://MARKDOWN_HASH61a4bd20fbd00664bbfa5c55f8b64df2MARKDOWN_HASH:7077\\nNote the URL listed. spark://ubuntu:7077. My URL is ubuntu because that is the name of my host. The name of your host will be what ever you set it up as during install. Write this url down, because next we are starting the slave. We have to tell the slave who its master is.\\nsbin/start-slave.sh spark://yourhostname:7077\\nThe argument spark://yourhostname:7077 lets the slave know who its master is. This is literally the master’s URL. If you have another computer with Spark 1.4 installed you could run this line again (substituting ubuntu for the IP address of the master machine) and add another computer to your cluster.\\nGotchya\\nFor those that are not reading carefully and just copying and pasting, you probably won’t see this for a while anyway, but I want to say again, unless you just happen to have named your host ubuntu you need to change that to what ever the name is you found for the Master URL in the Spark Web-UI…\\nNow go back to your master webui and you should see the slave listed under workers.\\n\\nStart Zeppelin\\nNow everything is technically up and running. All we have left to do, is start Zeppelin back up, tell it to run code against our clusters (instead of the internal interpreters), and check that our examples still work.\\nStart Zeppelin with the following\\ncd $HOME\\nincubator-zeppelin/bin/zeppelin-daemon.sh start\\nNow go back to the Zeppelin web-ui at http://yourip:8080 and this time click on Interpreters at the top of the screen.\\nIn the Spark section, click the edit button in the top right corner to make the property values editable. The only field that needs to be edited in the Spark interpreter is the master field. Change this value from local[*] to the URL you used to start the slave, mine was spark://ubuntu:7077.\\nEdit the spark and flink interpreters.\\nClick ‘Save’, then scroll down to the Flink section. Click ‘edit’ and change the value of host from local to localhost. Click ‘Save’ again.\\nNow open the Flink notebook we made earlier.\\nHit Shift+Enter or hit the play button at the top of the notebook to run the paragraphs. Hopefully the result is the same as before. Now in a new tab, go to the Flink Web-UI at http://yourip:8081. You should see the job has completed successfully on the cluster.\\nIt’s beeeeeautiful!\\nNow open the spark example notebook from earlier and rerun this as well. After this notebook has run successfully go to the Spark Web-UI at http://yourip:8082 and see the job has run on this cluster.\\nIs great success.\\nGotchya\\nif the Spark job seems to hang, go to the Spark Web-UI. If there is a job listed under running applications, but there are no workers listed, the slave has died, go back to the command line and run\\ncd $HOME\\nspark/sbin/start-slave.sh spark://yourhostname:7077\\nwhere yourhostname is the hostname you have been using for the Master URL this whole time.\\nNecromancing the Spark slave.\\nSummary\\nDude (or dudette), crack a beer. You just set up two of the most cutting edge big data engines available today in a cluster mode with an up-and-coming cutting edge (and pretty) web interface. Seriously, not a trivial task. Have you checked your linkedIn inbox in the last hour? because you probably have about 30 recruiters blowing you up.\\nSeriously though, It took me a couple of days smashing my head against the wall to make this rig work right and consistently. Seeing as I just saved you so much time, I think the least you could do is head over to, and sign up for, and participate in the user mailing lists for\\n\\nFlink Mailing Lists\\nSpark Mailing Lists\\nZeppelin Mailing Lists\\n\\nOpen source software is a beautiful thing, but it relies on a strong community. All of the projects, (and many more) could use your help, but especially Zeppelin which is still in incubator status.\\n(A thank you to the wonderful developers wouldn’t hurt either, they watch the mailing lists).\\nUPDATE 5-23: ¬†I let this go with out updates longer than I should have, and I’m sorry. ¬†To be honest, I probably won’t do it again. ¬†I’m older and wiser now, the things listed here should remain valid for sometime to come. The big changes are:\\n\\nWe use the actual Zeppelin branch (not Till’s)\\nWe build against Flink 1.0\\nWe build against Spark 1.6 with SparkR support.\\n\\nNote, you don’t actually have SparkR support yet.¬†elbamos instructions for SparkR integration.\\n\\n\\n\\nHappy hacking,\\ntg\\n¬†\\n\\n\\nTagged apache, big data, distributed computing, flink, spark, zeppelin6 Comments \\n\\n\\n\\n\\n \\n\\n\\n\\n\\nBlog at WordPress.com.\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nFollow\\n\\n\\nFollowing\\n\\n\\n\\n\\n\\n\\n\\n\\t\\t\\t\\tThe musings of rawkintrevo\\t\\t\\t\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n \\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tSign me up\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n\\n\\n\\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tAlready have a WordPress.com account? Log in now. \\n\\n\\n\\n\\n\\n\\n \\n\\n\\n\\n\\n\\n\\t\\t\\t\\tThe musings of rawkintrevo\\t\\t\\t\\n\\n\\n\\n Customize\\n\\n\\n\\n\\nFollow\\n\\n\\nFollowing\\n\\n\\nSign up\\nLog in\\n\\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tReport this content\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n\\n\\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tView site in Reader\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n\\n\\nManage subscriptions\\n\\nCollapse this bar\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\xa0\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nLoading Comments...\\n\\n\\n\\n\\xa0\\n\\n\\nWrite a Comment...\\n\\n\\n\\n\\nEmail (Required)\\n\\n\\n\\nName (Required)\\n\\n\\n\\nWebsite\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n \\n\\n\\n\\n\\n\\n', metadata={'source': 'https://rawkintrevo.org/category/flink/', 'title': 'flink – The musings of rawkintrevo', 'description': 'Posts about flink written by rawkintrevo', 'language': 'en'}),\n",
              " Document(page_content='Just a moment...Enable JavaScript and cookies to continue', metadata={'source': 'https://data-flair.training/blogs/category/flink/', 'title': 'Just a moment...', 'language': 'en-US'}),\n",
              " Document(page_content='\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nBuilding Applications with Apache Flink (Part 1): Dataset, Data Preparation and Building a Model\\n\\n\\n\\n\\n\\nwww.bytefish.de\\n\\n\\nblog\\nabout\\npages\\ndocuments\\n\\n\\n\\n\\n\\n\\nBuilding Applications with Apache Flink (Part 1): Dataset, Data Preparation and Building a Model\\nBy Philipp Wagner | July 03, 2016\\n\\n\\nIn this series of articles I want to show how to build an application with Apache Flink.\\n\\nApache Flink is an open source platform for distributed stream and batch data processing. Flinkâ€™s core \\r\\nis a streaming dataflow engine that provides data distribution, communication, and fault tolerance for \\r\\ndistributed computations over data streams. Flink also builds batch processing on top of the streaming \\r\\nengine, overlaying native iteration support, managed memory, and program optimization.\\n\\nWe are going to build an application, that processes the hourly weather measurements of more than 1,600 weather stations \\r\\nwith Apache Flink. The articles will show how to write custom Source functions for generating data and how to implement custom \\r\\nSink functions for writing to PostgreSQL and Elasticsearch.\\nSource Code\\nYou can find the full source code for the example in my git repository at:\\n\\nhttps://github.com/bytefish/FlinkExperiments\\n\\nDependencies\\nApache Flink\\nIn the example I am going to use the latest stable version 1.3.2 of Apache Flink. \\nIn the properties element of the POM File we add:\\n<properties>\\r\\n    <project.build.sourceEncoding>UTF-8</project.build.sourceEncoding>\\r\\n    <flink.version>1.3.2</flink.version>\\r\\n</properties>\\r\\n\\nAnd in the dependencies element of the POM file we add the Apache Flink dependencies:\\n<dependencies>\\r\\n\\r\\n    <dependency>\\r\\n        <groupId>org.apache.flink</groupId>\\r\\n        <artifactId>flink-java</artifactId>\\r\\n        <version>${flink.version}</version>\\r\\n    </dependency>\\r\\n\\r\\n    <dependency>\\r\\n        <groupId>org.apache.flink</groupId>\\r\\n        <artifactId>flink-streaming-java_2.10</artifactId>\\r\\n        <version>${flink.version}</version>\\r\\n    </dependency>\\r\\n\\r\\n    <dependency>\\r\\n        <groupId>org.apache.flink</groupId>\\r\\n        <artifactId>flink-clients_2.10</artifactId>\\r\\n        <version>${flink.version}</version>\\r\\n    </dependency>\\r\\n\\r\\n</dependencies>\\r\\n\\nJTinyCsvParser\\nJTinyCsvParser is a library for high-performance CSV parsing in Java.\\n<dependency>\\r\\n    <groupId>de.bytefish</groupId>\\r\\n    <artifactId>jtinycsvparser</artifactId>\\r\\n    <version>1.2</version>\\r\\n</dependency>\\r\\n\\nPgBulkInsert\\nPgBulkInsert is a Java library for bulk inserts to PostgreSQL.\\n<dependency>\\r\\n    <groupId>de.bytefish</groupId>\\r\\n    <artifactId>pgbulkinsert</artifactId>\\r\\n    <version>1.4</version>\\r\\n</dependency>\\r\\n\\nDataset\\nThe data is the Quality Controlled Local Climatological Data (QCLCD): \\n\\nQuality Controlled Local Climatological Data (QCLCD) consist of hourly, daily, and monthly summaries for approximately \\r\\n1,600 U.S. locations. Daily Summary forms are not available for all stations. Data are available beginning January 1, 2005 \\r\\nand continue to the present. Please note, there may be a 48-hour lag in the availability of the most recent data.\\n\\nThe data is available as CSV files at:\\n\\nhttp://www.ncdc.noaa.gov/orders/qclcd/\\n\\nWe are going to use the data from March 2015, which is located in the zipped file QCLCD201503.zip.\\nAnalyzing the Data\\nThe first step when processing data is to analyze the data, so we can model the problem in the application.\\nThe weather data measurements are given as CSV files contained in the file 201503hourly.txt. The list of corresponding weather stations \\r\\nis given in the file 201503station.txt. The weather stations are identified by their WBAN number, which is an abbreviation for \\r\\nWeather-Bureau-Army-Navy.\\nThe local weather data in 201503hourly.txt has more than 30 columns, such as the WBAN Identifier (Column 1), time of measurement \\r\\n(Columns 2, 3), Sky Condition (Column 5), Air Temperature (Column 13), Wind Speed (Column 25) and Pressure level (Column 31). The column \\r\\ndelimiter is a Comma character (,). \\nThe data of the weather stations is given in the 201503station.txt file. It has 14 columns, such as the WBAN identifier (Column 1), \\r\\na Name (Column 7) and most importantly the GPS position. The GPS position is given in latitude (Column 10) and longitude (Column 11). \\r\\nThe measurements in the 201503hourly.txt are given in local time, so we also need to take the stations timezone into account, which \\r\\nis given in the column 15. The column delimiter is a pipe symbol (|).\\nMissing data is indicated by an M.\\nBuilding the Domain Model\\nIt\\'s time to design the domain model, we want to work with. This domain model is going to be used throughout the application. The domain model \\r\\nshould not include any persistence related details, such as foreign keys or framework-specific attributes. Do not pollute your domain model, \\r\\nalways try to keep the concern separated.\\nIn the example we need a class for a measurement (LocalWeatherData), a class for a station (Station) and a class for the GPS data (GeoLocation). \\nWe are designing the domain model as POJOs (Plain Old Java Objects), so they can be easily serialized and deserialized. \\nGeoLocation\\nThe GeoLocation holds the latitude and longitude. \\n// Copyright (c) Philipp Wagner. All rights reserved.\\r\\n// Licensed under the MIT license. See LICENSE file in the project root for full license information.\\r\\n\\r\\npackage model;\\r\\n\\r\\npublic class GeoLocation {\\r\\n\\r\\n    private double lat;\\r\\n\\r\\n    private double lon;\\r\\n\\r\\n    public GeoLocation(){\\r\\n\\r\\n    }\\r\\n\\r\\n    public GeoLocation(double lat, double lon) {\\r\\n        this.lat = lat;\\r\\n        this.lon = lon;\\r\\n    }\\r\\n\\r\\n    public void setLat(double lat) {\\r\\n        this.lat = lat;\\r\\n    }\\r\\n\\r\\n    public void setLon(double lon) {\\r\\n        this.lon = lon;\\r\\n    }\\r\\n\\r\\n    public double getLat() {\\r\\n        return lat;\\r\\n    }\\r\\n\\r\\n    public double getLon() {\\r\\n        return lon;\\r\\n    }\\r\\n}\\r\\n\\nStation\\nEach Station has an assigned GeoLocation. We also keep track of its timezone, so we can calculate a coordinated timestamp for each measurement.\\n// Copyright (c) Philipp Wagner. All rights reserved.\\r\\n// Licensed under the MIT license. See LICENSE file in the project root for full license information.\\r\\n\\r\\npackage model;\\r\\n\\r\\npublic class Station {\\r\\n\\r\\n    private String wban;\\r\\n\\r\\n    private String name;\\r\\n\\r\\n    private String state;\\r\\n\\r\\n    private String location;\\r\\n\\r\\n    private Integer timeZone;\\r\\n\\r\\n    private GeoLocation geoLocation;\\r\\n\\r\\n    public Station() {\\r\\n\\r\\n    }\\r\\n\\r\\n    public Station(String wban, String name, String state, String location, Integer timeZone, GeoLocation geoLocation) {\\r\\n        this.wban = wban;\\r\\n        this.name = name;\\r\\n        this.state = state;\\r\\n        this.location = location;\\r\\n        this.timeZone = timeZone;\\r\\n        this.geoLocation = geoLocation;\\r\\n    }\\r\\n\\r\\n    public void setWban(String wban) {\\r\\n        this.wban = wban;\\r\\n    }\\r\\n\\r\\n    public void setName(String name) {\\r\\n        this.name = name;\\r\\n    }\\r\\n\\r\\n    public void setState(String state) {\\r\\n        this.state = state;\\r\\n    }\\r\\n\\r\\n    public void setLocation(String location) {\\r\\n        this.location = location;\\r\\n    }\\r\\n\\r\\n    public void setTimeZone(Integer timeZone) {\\r\\n        this.timeZone = timeZone;\\r\\n    }\\r\\n\\r\\n    public void setGeoLocation(GeoLocation geoLocation) {\\r\\n        this.geoLocation = geoLocation;\\r\\n    }\\r\\n\\r\\n    public String getWban() {\\r\\n        return wban;\\r\\n    }\\r\\n\\r\\n    public String getName() {\\r\\n        return name;\\r\\n    }\\r\\n\\r\\n    public String getState() {\\r\\n        return state;\\r\\n    }\\r\\n\\r\\n    public String getLocation() {\\r\\n        return location;\\r\\n    }\\r\\n\\r\\n    public Integer getTimeZone() {\\r\\n        return timeZone;\\r\\n    }\\r\\n\\r\\n    public GeoLocation getGeoLocation() {\\r\\n        return geoLocation;\\r\\n    }\\r\\n}\\r\\n\\nLocalWeatherData\\nEach LocalWeatherData measurement was generated by a Station. We are interested in the time of measurements, temperature, wind speed, station pressure \\r\\nand sky condition. One could argue, that referencing the entire Station object is bad for performance and memory efficiency. But I have a huge dislike \\r\\nfor premature optimization, so you should first focus on modelling the problem.\\n// Copyright (c) Philipp Wagner. All rights reserved.\\r\\n// Licensed under the MIT license. See LICENSE file in the project root for full license information.\\r\\n\\r\\npackage model;\\r\\n\\r\\nimport java.time.LocalDate;\\r\\nimport java.time.LocalTime;\\r\\n\\r\\npublic class LocalWeatherData {\\r\\n\\r\\n    private Station station;\\r\\n\\r\\n    private LocalDate date;\\r\\n\\r\\n    private LocalTime time;\\r\\n\\r\\n    private Float temperature;\\r\\n\\r\\n    private Float windSpeed;\\r\\n\\r\\n    private Float stationPressure;\\r\\n\\r\\n    private String skyCondition;\\r\\n\\r\\n    public LocalWeatherData() {\\r\\n\\r\\n    }\\r\\n\\r\\n    public LocalWeatherData(Station station, LocalDate date, LocalTime time, Float temperature, Float windSpeed, Float stationPressure, String skyCondition) {\\r\\n        this.station = station;\\r\\n        this.date = date;\\r\\n        this.time = time;\\r\\n        this.temperature = temperature;\\r\\n        this.windSpeed = windSpeed;\\r\\n        this.stationPressure = stationPressure;\\r\\n        this.skyCondition = skyCondition;\\r\\n    }\\r\\n\\r\\n    public void setStation(Station station) {\\r\\n        this.station = station;\\r\\n    }\\r\\n\\r\\n    public void setDate(LocalDate date) {\\r\\n        this.date = date;\\r\\n    }\\r\\n\\r\\n    public void setTime(LocalTime time) {\\r\\n        this.time = time;\\r\\n    }\\r\\n\\r\\n    public void setTemperature(Float temperature) {\\r\\n        this.temperature = temperature;\\r\\n    }\\r\\n\\r\\n    public void setWindSpeed(Float windSpeed) {\\r\\n        this.windSpeed = windSpeed;\\r\\n    }\\r\\n\\r\\n    public void setStationPressure(Float stationPressure) {\\r\\n        this.stationPressure = stationPressure;\\r\\n    }\\r\\n\\r\\n    public void setSkyCondition(String skyCondition) {\\r\\n        this.skyCondition = skyCondition;\\r\\n    }\\r\\n\\r\\n    public Station getStation() {\\r\\n        return station;\\r\\n    }\\r\\n\\r\\n    public LocalDate getDate() {\\r\\n        return date;\\r\\n    }\\r\\n\\r\\n    public LocalTime getTime() {\\r\\n        return time;\\r\\n    }\\r\\n\\r\\n    public Float getTemperature() {\\r\\n        return temperature;\\r\\n    }\\r\\n\\r\\n    public Float getWindSpeed() {\\r\\n        return windSpeed;\\r\\n    }\\r\\n\\r\\n    public Float getStationPressure() {\\r\\n        return stationPressure;\\r\\n    }\\r\\n\\r\\n    public String getSkyCondition() {\\r\\n        return skyCondition;\\r\\n    }\\r\\n}\\r\\n\\nParsing the CSV Data\\nModel\\nBuilding different data models for the analysis and CSV data might look like a total overkill. But keeping the concerns separated is \\r\\nthe only way to not leak persistence details into your application. You really, really need to decouple your application logic from \\r\\nany specific data persistence technology, or it will bite you.\\nSo the CSV data model is slightly different from the domain modell. It is basically a flat representation of the data, with only a \\r\\nStation and LocalWeatherData class, that map directly to both CSV files. JTinyCsvParser creates these objects, so these \\r\\nclasses are POJOs (a class with a parameterless constructor, getters and setters).\\nLocalWeatherData\\n// Copyright (c) Philipp Wagner. All rights reserved.\\r\\n// Licensed under the MIT license. See LICENSE file in the project root for full license information.\\r\\n\\r\\npackage csv.model;\\r\\n\\r\\nimport java.time.LocalDate;\\r\\nimport java.time.LocalTime;\\r\\n\\r\\npublic class LocalWeatherData {\\r\\n\\r\\n    private String wban;\\r\\n\\r\\n    private LocalDate date;\\r\\n\\r\\n    private LocalTime time;\\r\\n\\r\\n    private String skyCondition;\\r\\n\\r\\n    private Float dryBulbCelsius;\\r\\n\\r\\n    private Float windSpeed;\\r\\n\\r\\n    private Float stationPressure;\\r\\n\\r\\n    public LocalWeatherData() {\\r\\n\\r\\n    }\\r\\n\\r\\n    // Getters and Setters ...\\r\\n}\\r\\n\\nStation\\n// Copyright (c) Philipp Wagner. All rights reserved.\\r\\n// Licensed under the MIT license. See LICENSE file in the project root for full license information.\\r\\n\\r\\npackage csv.model;\\r\\n\\r\\npublic class Station {\\r\\n\\r\\n    private String wban;\\r\\n\\r\\n    private String wmo;\\r\\n\\r\\n    private String callSign;\\r\\n\\r\\n    private String climateDivisionCode;\\r\\n\\r\\n    private String climateDivisionStateCode;\\r\\n\\r\\n    private String climateDivisionStationCode;\\r\\n\\r\\n    private String name;\\r\\n\\r\\n    private String state;\\r\\n\\r\\n    private String location;\\r\\n\\r\\n    private Float latitude;\\r\\n\\r\\n    private Float longitude;\\r\\n\\r\\n    private Integer groundHeight;\\r\\n\\r\\n    private Integer stationHeight;\\r\\n\\r\\n    private Integer barometer;\\r\\n\\r\\n    private Integer timeZone;\\r\\n\\r\\n    public Station() {\\r\\n    }\\r\\n\\r\\n    // Getters and Setters ...\\r\\n\\r\\n}\\r\\n\\nMapper\\nLocalWeatherDataMapper\\nThe measurement date is given in the format yyyyMMdd and the measurement time is given in the format HHmm. That\\'s why the \\r\\nLocalDateConverter and LocalTimeConverter are instantiated with custom formats. \\nThe columns for temperature, wind speed and station pressure may include missing values indicated by a M. That\\'s why we need to map \\r\\nit with an IgnoreMissingValuesConverter. The IgnoreMissingValuesConverter returns null, when it encounters the missing value sign \\r\\ninstead of trying to parse the data.\\n// Copyright (c) Philipp Wagner. All rights reserved.\\r\\n// Licensed under the MIT license. See LICENSE file in the project root for full license information.\\r\\n\\r\\npackage csv.mapping;\\r\\n\\r\\nimport csv.model.LocalWeatherData;\\r\\nimport de.bytefish.jtinycsvparser.builder.IObjectCreator;\\r\\nimport de.bytefish.jtinycsvparser.mapping.CsvMapping;\\r\\nimport de.bytefish.jtinycsvparser.typeconverter.FloatConverter;\\r\\nimport de.bytefish.jtinycsvparser.typeconverter.IgnoreMissingValuesConverter;\\r\\nimport de.bytefish.jtinycsvparser.typeconverter.LocalDateConverter;\\r\\nimport de.bytefish.jtinycsvparser.typeconverter.LocalTimeConverter;\\r\\n\\r\\nimport java.time.LocalDate;\\r\\nimport java.time.LocalTime;\\r\\nimport java.time.format.DateTimeFormatter;\\r\\n\\r\\npublic class LocalWeatherDataMapper extends CsvMapping<LocalWeatherData>\\r\\n{\\r\\n    public LocalWeatherDataMapper(IObjectCreator creator)\\r\\n    {\\r\\n        super(creator);\\r\\n\\r\\n        mapProperty(0, String.class, LocalWeatherData::setWban);\\r\\n        mapProperty(1, LocalDate.class, LocalWeatherData::setDate, new LocalDateConverter(DateTimeFormatter.ofPattern(\"yyyyMMdd\")));\\r\\n        mapProperty(2, LocalTime.class, LocalWeatherData::setTime, new LocalTimeConverter(DateTimeFormatter.ofPattern(\"HHmm\")));\\r\\n        mapProperty(4, String.class, LocalWeatherData::setSkyCondition);\\r\\n        mapProperty(12, Float.class, LocalWeatherData::setDryBulbCelsius, new IgnoreMissingValuesConverter<>(new FloatConverter(), \"M\"));\\r\\n        mapProperty(24, Float.class, LocalWeatherData::setWindSpeed, new IgnoreMissingValuesConverter<>(new FloatConverter(), \"M\"));\\r\\n        mapProperty(30, Float.class, LocalWeatherData::setStationPressure, new IgnoreMissingValuesConverter<>(new FloatConverter(), \"M\"));\\r\\n    }\\r\\n}\\r\\n\\nStationMapper\\nThe StationMapper maps all of the 14 available columns. Again the Ground Height, Station Height and Barometer for some stations may be missing\\r\\nin the data, so we also map them with an IgnoreMissingValuesConverter.\\n// Copyright (c) Philipp Wagner. All rights reserved.\\r\\n// Licensed under the MIT license. See LICENSE file in the project root for full license information.\\r\\n\\r\\npackage csv.mapping;\\r\\n\\r\\nimport csv.model.Station;\\r\\nimport de.bytefish.jtinycsvparser.builder.IObjectCreator;\\r\\nimport de.bytefish.jtinycsvparser.mapping.CsvMapping;\\r\\nimport de.bytefish.jtinycsvparser.typeconverter.FloatConverter;\\r\\nimport de.bytefish.jtinycsvparser.typeconverter.IgnoreMissingValuesConverter;\\r\\nimport de.bytefish.jtinycsvparser.typeconverter.IntegerConverter;\\r\\n\\r\\npublic class StationMapper extends CsvMapping<Station>\\r\\n{\\r\\n    public StationMapper(IObjectCreator creator)\\r\\n    {\\r\\n        super(creator);\\r\\n\\r\\n        mapProperty(0, String.class, Station::setWban);\\r\\n        mapProperty(1, String.class, Station::setWmo);\\r\\n        mapProperty(2, String.class, Station::setCallSign);\\r\\n        mapProperty(3, String.class, Station::setClimateDivisionCode);\\r\\n        mapProperty(4, String.class, Station::setClimateDivisionStateCode);\\r\\n        mapProperty(5, String.class, Station::setClimateDivisionStationCode);\\r\\n        mapProperty(6, String.class, Station::setName);\\r\\n        mapProperty(7, String.class, Station::setState);\\r\\n        mapProperty(8, String.class, Station::setLocation);\\r\\n        mapProperty(9, Float.class, Station::setLatitude);\\r\\n        mapProperty(10, Float.class, Station::setLongitude);\\r\\n        mapProperty(11, Integer.class, Station::setGroundHeight, new IgnoreMissingValuesConverter<>(new IntegerConverter()));\\r\\n        mapProperty(12, Integer.class, Station::setStationHeight, new IgnoreMissingValuesConverter<>(new IntegerConverter()));\\r\\n        mapProperty(13, Integer.class, Station::setBarometer, new IgnoreMissingValuesConverter<>(new IntegerConverter()));\\r\\n        mapProperty(14, Integer.class, Station::setTimeZone);\\r\\n    }\\r\\n}\\r\\n\\nParser\\nA CsvParser defines how a CSV file is tokenized and mapped to a Java object. We are using a simple StringSplitTokenizer, which splits a \\r\\nline at a given column delimiter. For the station file a pipe symbol | is used as a column delimiter. For the local weather data measurements \\r\\na comma , is used as column delimiter.\\nBoth CsvParser are set to ignore the header line.\\n// Copyright (c) Philipp Wagner. All rights reserved.\\r\\n// Licensed under the MIT license. See LICENSE file in the project root for full license information.\\r\\n\\r\\npackage csv.parser;\\r\\n\\r\\nimport csv.mapping.LocalWeatherDataMapper;\\r\\nimport csv.mapping.StationMapper;\\r\\nimport csv.model.LocalWeatherData;\\r\\nimport csv.model.Station;\\r\\nimport de.bytefish.jtinycsvparser.CsvParser;\\r\\nimport de.bytefish.jtinycsvparser.CsvParserOptions;\\r\\nimport de.bytefish.jtinycsvparser.tokenizer.StringSplitTokenizer;\\r\\n\\r\\npublic class Parsers {\\r\\n\\r\\n    public static CsvParser<Station> StationParser() {\\r\\n\\r\\n        return new CsvParser<>(new CsvParserOptions(true, new StringSplitTokenizer(\"\\\\\\\\|\", true)), new StationMapper(() -> new Station()));\\r\\n    }\\r\\n\\r\\n    public static CsvParser<LocalWeatherData> LocalWeatherDataParser()\\r\\n    {\\r\\n        return new CsvParser<>(new CsvParserOptions(true, new StringSplitTokenizer(\",\", true)), new LocalWeatherDataMapper(() -> new LocalWeatherData()));\\r\\n    }\\r\\n\\r\\n}\\r\\n\\nMapping between CSV model and the Domain model\\nWhat\\'s left is the mapping between the CSV model and the applications domain model. This is done by writing a simple converter, which takes the \\r\\nCSV representation of the data and returns the application model of the data.\\n// Copyright (c) Philipp Wagner. All rights reserved.\\r\\n// Licensed under the MIT license. See LICENSE file in the project root for full license information.\\r\\n\\r\\npackage stream.sources.csv.converter;\\r\\n\\r\\nimport java.time.LocalDate;\\r\\nimport java.time.LocalTime;\\r\\n\\r\\npublic class LocalWeatherDataConverter {\\r\\n\\r\\n    public static model.LocalWeatherData convert(csv.model.LocalWeatherData csvLocalWeatherData, csv.model.Station csvStation) {\\r\\n\\r\\n        LocalDate date = csvLocalWeatherData.getDate();\\r\\n        LocalTime time = csvLocalWeatherData.getTime();\\r\\n        String skyCondition = csvLocalWeatherData.getSkyCondition();\\r\\n        Float stationPressure = csvLocalWeatherData.getStationPressure();\\r\\n        Float temperature = csvLocalWeatherData.getDryBulbCelsius();\\r\\n        Float windSpeed = csvLocalWeatherData.getWindSpeed();\\r\\n\\r\\n        // Convert the Station data:\\r\\n        model.Station station = convert(csvStation);\\r\\n\\r\\n        return new model.LocalWeatherData(station, date, time, temperature, windSpeed, stationPressure, skyCondition);\\r\\n    }\\r\\n\\r\\n    public static model.Station convert(csv.model.Station csvStation) {\\r\\n        String wban = csvStation.getWban();\\r\\n        String name = csvStation.getName();\\r\\n        String state = csvStation.getState();\\r\\n        String location = csvStation.getLocation();\\r\\n        Integer timeZone = csvStation.getTimeZone();\\r\\n        model.GeoLocation geoLocation = new model.GeoLocation(csvStation.getLatitude(), csvStation.getLongitude());\\r\\n\\r\\n        return new model.Station(wban, name, state, location, timeZone, geoLocation);\\r\\n    }\\r\\n\\r\\n}\\r\\n\\nPreparing the Data\\nIf you analyze the data, you will notice, that the measurements are not sorted by the measurement timestamp. In order to simulate incoming weather data \\r\\nmeasurements as realistic as possible, we need to make sure the measurements are emitted with a monotonic increasing timestamp.\\nSo we are going to write a small application, which sorts the CSV file by monotonically ascending measurement timestamps. The idea is to basically \\r\\nenumerate the entire dataset first, once it is enumerated the invalid lines are discarded. After the invalid lines have been discarded, we calculate \\r\\nthe UTC timestamp of the measurement by taking the station zone offset into account. These coordinated timestamps (with their original index in file) \\r\\nare then sorted by the timestamp.\\nFinally this list of indices is used to sort the CSV file and write it into a new file 201503hourly_sorted.txt.\\nI have 16 GB RAM, so I can safely sort the entire dataset in memory. If the dataset becomes larger, this approach obviously does not scale.\\n// Copyright (c) Philipp Wagner. All rights reserved.\\r\\n// Licensed under the MIT license. See LICENSE file in the project root for full license information.\\r\\n\\r\\npackage csv.sorting;\\r\\n\\r\\nimport csv.model.Station;\\r\\nimport csv.parser.Parsers;\\r\\nimport de.bytefish.jtinycsvparser.mapping.CsvMappingResult;\\r\\nimport org.apache.commons.lang3.tuple.ImmutablePair;\\r\\n\\r\\nimport java.io.BufferedWriter;\\r\\nimport java.nio.charset.StandardCharsets;\\r\\nimport java.nio.file.FileSystems;\\r\\nimport java.nio.file.Files;\\r\\nimport java.nio.file.Path;\\r\\nimport java.time.OffsetDateTime;\\r\\nimport java.time.ZoneOffset;\\r\\nimport java.util.ArrayList;\\r\\nimport java.util.Comparator;\\r\\nimport java.util.List;\\r\\nimport java.util.Map;\\r\\nimport java.util.concurrent.atomic.AtomicInteger;\\r\\nimport java.util.stream.Collectors;\\r\\nimport java.util.stream.Stream;\\r\\n\\r\\npublic class PrepareWeatherData {\\r\\n\\r\\n    public static void main(String[] args) throws Exception {\\r\\n\\r\\n        // Path to read the CSV data from:\\r\\n        final Path csvStationDataFilePath = FileSystems.getDefault().getPath(\"C:\\\\\\\\Users\\\\\\\\philipp\\\\\\\\Downloads\\\\\\\\csv\\\\\\\\201503station.txt\");\\r\\n        final Path csvLocalWeatherDataUnsortedFilePath = FileSystems.getDefault().getPath(\"C:\\\\\\\\Users\\\\\\\\philipp\\\\\\\\Downloads\\\\\\\\csv\\\\\\\\201503hourly.txt\");\\r\\n        final Path csvLocalWeatherDataSortedFilePath = FileSystems.getDefault().getPath(\"C:\\\\\\\\Users\\\\\\\\philipp\\\\\\\\Downloads\\\\\\\\csv\\\\\\\\201503hourly_sorted.txt\");\\r\\n\\r\\n        // A map between the WBAN and Station for faster Lookups:\\r\\n        final Map<String, Station> stationMap = getStationMap(csvStationDataFilePath);\\r\\n\\r\\n        // Holds the List of Sorted DateTimes (including ZoneOffset):\\r\\n        List<Integer> indices = new ArrayList<>();\\r\\n\\r\\n        // Comparator for sorting the File:\\r\\n        Comparator<OffsetDateTime> byMeasurementTime = (e1, e2) -> e1.compareTo(e2);\\r\\n\\r\\n        // Get the sorted indices from the stream of LocalWeatherData Elements:\\r\\n        try (Stream<CsvMappingResult<csv.model.LocalWeatherData>> stream = getLocalWeatherData(csvLocalWeatherDataUnsortedFilePath)) {\\r\\n\\r\\n            // Holds the current line index, when processing the input Stream:\\r\\n            AtomicInteger currentIndex = new AtomicInteger(1);\\r\\n\\r\\n            // We want to get a list of indices, which sorts the CSV file by measurement time:\\r\\n            indices = stream\\r\\n                    // Skip the CSV Header:\\r\\n                    .skip(1)\\r\\n                    // Start by enumerating ALL mapping results:\\r\\n                    .map(x -> new ImmutablePair<>(currentIndex.getAndAdd(1), x))\\r\\n                    // Then only take those lines, that are actually valid:\\r\\n                    .filter(x -> x.getRight().isValid())\\r\\n                    // Now take the parsed entity from the CsvMappingResult:\\r\\n                    .map(x -> new ImmutablePair<>(x.getLeft(), x.getRight().getResult()))\\r\\n                    // Take only those measurements, that are also available in the list of stations:\\r\\n                    .filter(x -> stationMap.containsKey(x.getRight().getWban()))\\r\\n                    // Get the OffsetDateTime from the LocalWeatherData, which includes the ZoneOffset of the Station:\\r\\n                    .map(x -> {\\r\\n                        // Get the matching station:\\r\\n                        csv.model.Station station = stationMap.get(x.getRight().getWban());\\r\\n                        // Calculate the OffsetDateTime from the given measurement:\\r\\n                        OffsetDateTime measurementTime = OffsetDateTime.of(x.getRight().getDate(), x.getRight().getTime(), ZoneOffset.ofHours(station.getTimeZone()));\\r\\n                        // Build the Immutable pair with the Index again:\\r\\n                        return new ImmutablePair<>(x.getLeft(), measurementTime);\\r\\n                    })\\r\\n                    // Now sort the Measurements by their Timestamp:\\r\\n                    .sorted((x, y) -> byMeasurementTime.compare(x.getRight(), y.getRight()))\\r\\n                    // Take only the Index:\\r\\n                    .map(x -> x.getLeft())\\r\\n                    // And turn it into a List:\\r\\n                    .collect(Collectors.toList());\\r\\n        }\\r\\n\\r\\n        // Now sort the File by Line Number:\\r\\n        writeSortedFileByIndices(csvLocalWeatherDataUnsortedFilePath, indices, csvLocalWeatherDataSortedFilePath);\\r\\n    }\\r\\n\\r\\n    private static void writeSortedFileByIndices(Path csvFileIn, List<Integer> indices, Path csvFileOut) {\\r\\n        try {\\r\\n            List<String> csvDataList = new ArrayList<>();\\r\\n\\r\\n            // This is sorting for the dumb (like me). Read the entire CSV file, skipping the first line:\\r\\n            try (Stream<String> lines = Files.lines(csvFileIn, StandardCharsets.US_ASCII).skip(1))\\r\\n            {\\r\\n                csvDataList = lines.collect(Collectors.toList());\\r\\n            }\\r\\n            // Now write the sorted file:\\r\\n            try(BufferedWriter writer = Files.newBufferedWriter(csvFileOut)) {\\r\\n                for (Integer index : indices) {\\r\\n                    writer.write(csvDataList.get(index));\\r\\n                    writer.newLine();\\r\\n                }\\r\\n            }\\r\\n        } catch(Exception e) {\\r\\n            throw new RuntimeException(e);\\r\\n        }\\r\\n    }\\r\\n\\r\\n    private static Stream<CsvMappingResult<csv.model.LocalWeatherData>> getLocalWeatherData(Path path) {\\r\\n        return Parsers.LocalWeatherDataParser().readFromFile(path, StandardCharsets.US_ASCII);\\r\\n    }\\r\\n\\r\\n    private static Stream<csv.model.Station> getStations(Path path) {\\r\\n        return Parsers.StationParser().readFromFile(path, StandardCharsets.US_ASCII)\\r\\n                .filter(x -> x.isValid())\\r\\n                .map(x -> x.getResult());\\r\\n    }\\r\\n\\r\\n    private static Map<String, csv.model.Station> getStationMap(Path path) {\\r\\n        try (Stream<csv.model.Station> stationStream = getStations(path)) {\\r\\n            return stationStream\\r\\n                    .collect(Collectors.toMap(csv.model.Station::getWban, x -> x));\\r\\n        }\\r\\n    }\\r\\n}\\r\\n\\nConclusion\\nIn this part of the series we have analyzed the CSV data, wrote the neccessary classes to parse the files and \\r\\npreprocessed it.  We have defined the domain model, that we are going to work with and wrote a converter between \\r\\nthe CSV data and the domain model.\\nThe next part of the series shows how to write a source function for emitting the local weather data events to Apache Flink.\\n\\n\\nHow to contribute\\nOne of the easiest ways to contribute is to participate in discussions. You can also contribute by submitting pull requests.\\nGeneral feedback and discussions?\\nDo you have questions or feedback on this article? Please create an issue on the GitHub issue tracker.\\nSomething is wrong or missing?\\nThere may be something wrong or missing in this article. If you want to help fixing it, then please make a Pull Request to this file on GitHub.\\n\\n\\n\\n\\n\\n', metadata={'source': 'https://www.bytefish.de/blog/apache_flink_series_1.html', 'title': 'Building Applications with Apache Flink (Part 1): Dataset, Data Preparation and Building a Model', 'description': 'This article shows how to work with Apache Flink.', 'language': 'en'}),\n",
              " Document(page_content=\"\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nBuilding Applications with Apache Flink (Part 2): Writing a custom SourceFunction for the CSV Data\\n\\n\\n\\n\\n\\nwww.bytefish.de\\n\\n\\nblog\\nabout\\npages\\ndocuments\\n\\n\\n\\n\\n\\n\\nBuilding Applications with Apache Flink (Part 2): Writing a custom SourceFunction for the CSV Data\\nBy Philipp Wagner | July 03, 2016\\n\\n\\nIn the previous article we have obtained a CSV dataset, analyzed it and built the neccessary tools for parsing it. A domain model \\r\\nwas created, which will be used for the Stream processing. What's left is how to feed a DataStream with the actual CSV data, \\r\\nand this is where the SourceFunction fits in.\\nWhat we are going to build\\nWe are going to build a SourceFunction, that uses the parsers from the previous article to read the CSV data. The LocalWeatherDataConverter is \\r\\nused to transform the CSV objects into the domain model, which is the emitted to the Apache Flink SourceContext.\\nSource Code\\nYou can find the full source code for the example in my git repository at:\\n\\nhttps://github.com/bytefish/FlinkExperiments\\n\\nLocalWeatherDataSourceFunction\\nThe LocalWeatherDataSourceFunction implements the SourceFunction interface, parses the CSV data from the previous and emits the measurements to the Apache Flink SourceContext.\\n// Copyright (c) Philipp Wagner. All rights reserved.\\r\\n// Licensed under the MIT license. See LICENSE file in the project root for full license information.\\r\\n\\r\\npackage stream.sources.csv;\\r\\n\\r\\nimport csv.parser.Parsers;\\r\\nimport org.apache.flink.streaming.api.functions.source.SourceFunction;\\r\\nimport stream.sources.csv.converter.LocalWeatherDataConverter;\\r\\n\\r\\nimport java.nio.charset.StandardCharsets;\\r\\nimport java.nio.file.FileSystems;\\r\\nimport java.nio.file.Path;\\r\\nimport java.util.Iterator;\\r\\nimport java.util.Map;\\r\\nimport java.util.stream.Collectors;\\r\\nimport java.util.stream.Stream;\\r\\n\\r\\npublic class LocalWeatherDataSourceFunction implements SourceFunction<model.LocalWeatherData> {\\r\\n\\r\\n    private volatile boolean isRunning = true;\\r\\n\\r\\n    private String stationFilePath;\\r\\n    private String localWeatherDataFilePath;\\r\\n\\r\\n    public LocalWeatherDataSourceFunction(String stationFilePath, String localWeatherDataFilePath) {\\r\\n        this.stationFilePath = stationFilePath;\\r\\n        this.localWeatherDataFilePath = localWeatherDataFilePath;\\r\\n    }\\r\\n\\r\\n    @Override\\r\\n    public void run(SourceFunction.SourceContext<model.LocalWeatherData> sourceContext) throws Exception {\\r\\n\\r\\n        // The Source needs to be Serializable, so we have to construct the Paths at this point:\\r\\n        final Path csvStationPath = FileSystems.getDefault().getPath(stationFilePath);\\r\\n        final Path csvLocalWeatherDataPath = FileSystems.getDefault().getPath(localWeatherDataFilePath);\\r\\n\\r\\n        // Get the Stream of LocalWeatherData Elements in the CSV File:\\r\\n        try(Stream<model.LocalWeatherData> stream = getLocalWeatherData(csvStationPath, csvLocalWeatherDataPath)) {\\r\\n\\r\\n            // We need to get an iterator, since the SourceFunction has to break out of its main loop on cancellation:\\r\\n            Iterator<model.LocalWeatherData> iterator = stream.iterator();\\r\\n\\r\\n            // Make sure to cancel, when the Source function is canceled by an external event:\\r\\n            while (isRunning && iterator.hasNext()) {\\r\\n                synchronized (sourceContext.getCheckpointLock()) {\\r\\n                    sourceContext.collect(iterator.next());\\r\\n                }\\r\\n            }\\r\\n        }\\r\\n    }\\r\\n\\r\\n    @Override\\r\\n    public void cancel() {\\r\\n        isRunning = false;\\r\\n    }\\r\\n\\r\\n    private Stream<model.LocalWeatherData> getLocalWeatherData(Path csvStationPath, Path csvLocalWeatherDataPath) {\\r\\n\\r\\n        // A map between the WBAN and Station for faster Lookups:\\r\\n        final Map<String, csv.model.Station> stationMap = getStationMap(csvStationPath);\\r\\n\\r\\n        // Turns the Stream of CSV data into the Elasticsearch representation:\\r\\n        return getLocalWeatherData(csvLocalWeatherDataPath)\\r\\n                // Only use Measurements with a Station:\\r\\n                .filter(x -> stationMap.containsKey(x.getWban()))\\r\\n                // And turn the Station and LocalWeatherData into the ElasticSearch representation:\\r\\n                .map(x -> {\\r\\n                    // First get the matching Station:\\r\\n                    csv.model.Station station = stationMap.get(x.getWban());\\r\\n                    // Convert to the Elastic Representation:\\r\\n                    return LocalWeatherDataConverter.convert(x, station);\\r\\n                });\\r\\n    }\\r\\n\\r\\n    private static Stream<csv.model.LocalWeatherData> getLocalWeatherData(Path path) {\\r\\n        return Parsers.LocalWeatherDataParser().readFromFile(path, StandardCharsets.US_ASCII)\\r\\n                .filter(x -> x.isValid())\\r\\n                .map(x -> x.getResult());\\r\\n    }\\r\\n\\r\\n    private static Stream<csv.model.Station> getStations(Path path) {\\r\\n        return Parsers.StationParser().readFromFile(path, StandardCharsets.US_ASCII)\\r\\n                .filter(x -> x.isValid())\\r\\n                .map(x -> x.getResult());\\r\\n    }\\r\\n\\r\\n    private Map<String, csv.model.Station> getStationMap(Path path) {\\r\\n        try (Stream<csv.model.Station> stationStream = getStations(path)) {\\r\\n            return stationStream\\r\\n                    .collect(Collectors.toMap(csv.model.Station::getWban, x -> x));\\r\\n        }\\r\\n    }\\r\\n}\\r\\n\\nConclusion\\nThe next part of the series shows how to utilize the SourceFunction to serve a DataStream.\\n\\n\\nHow to contribute\\nOne of the easiest ways to contribute is to participate in discussions. You can also contribute by submitting pull requests.\\nGeneral feedback and discussions?\\nDo you have questions or feedback on this article? Please create an issue on the GitHub issue tracker.\\nSomething is wrong or missing?\\nThere may be something wrong or missing in this article. If you want to help fixing it, then please make a Pull Request to this file on GitHub.\\n\\n\\n\\n\\n\\n\", metadata={'source': 'https://www.bytefish.de/blog/apache_flink_series_2.html', 'title': 'Building Applications with Apache Flink (Part 2): Writing a custom SourceFunction for the CSV Data', 'description': 'This article shows how to work with Apache Flink.', 'language': 'en'}),\n",
              " Document(page_content='\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nBuilding Applications with Apache Flink (Part 3): Stream Processing with the DataStream API\\n\\n\\n\\n\\n\\nwww.bytefish.de\\n\\n\\nblog\\nabout\\npages\\ndocuments\\n\\n\\n\\n\\n\\n\\nBuilding Applications with Apache Flink (Part 3): Stream Processing with the DataStream API\\nBy Philipp Wagner | July 03, 2016\\n\\n\\nIn the previous article we have written a SourceFunction to emit measurements from a CSV source file. In this article we are going to use the SourceFunction to serve a DataStream. \\nWhat we are going to build\\nYou will see how to use the DataStream API for applying operators on the stream, so that for each station the maximum temperature for a day is identified.\\nSource Code\\nYou can find the full source code for the example in my git repository at:\\n\\nhttps://github.com/bytefish/FlinkExperiments\\n\\nDataStream API\\nThe DataStream API of Apache Flink makes it possible to apply a various operations on a stream of incoming data.\\nThe Apache Flink documentation describes a DataStream as:\\n\\nDataStream programs in Flink are regular programs that implement transformations on data streams (e.g., filtering, updating state, defining windows, aggregating). \\r\\nThe data streams are initially created from various sources (e.g., message queues, socket streams, files). Results are returned via sinks, which may for example \\r\\nwrite the data to files, or to standard output (for example the command line terminal). Flink programs run in a variety of contexts, standalone, or embedded in \\r\\nother programs. The execution can happen in a local JVM, or on clusters of many machines.\\n\\nExample Program: Maximum Air Temperature by station and day\\nIn this example we are using the SourceFunction from the previous article to serve the DataStream. We are first setting the time characteristics of the \\r\\nDataStream to the EventTime, because each measurement carries the measurement timestamp. We are then building a KeyedStream over the DataStream, which \\r\\ngroups the incoming data by its station. And finally we use a non-overlapping tumbling window with 1 day length, from which the maximum temperature is used.\\nThe results in this example are written to a Console, but in the next article you will learn how to write a custom SinkFunction to write the data \\r\\ninto a PostgreSQL database for further data analysis.\\n// Copyright (c) Philipp Wagner. All rights reserved.\\r\\n// Licensed under the MIT license. See LICENSE file in the project root for full license information.\\r\\n\\r\\npackage app;\\r\\n\\r\\nimport model.LocalWeatherData;\\r\\nimport org.apache.flink.api.common.functions.FilterFunction;\\r\\nimport org.apache.flink.api.common.functions.MapFunction;\\r\\nimport org.apache.flink.api.java.functions.KeySelector;\\r\\nimport org.apache.flink.streaming.api.TimeCharacteristic;\\r\\nimport org.apache.flink.streaming.api.datastream.DataStream;\\r\\nimport org.apache.flink.streaming.api.datastream.KeyedStream;\\r\\nimport org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;\\r\\nimport org.apache.flink.streaming.api.functions.timestamps.AscendingTimestampExtractor;\\r\\nimport org.apache.flink.streaming.api.windowing.time.Time;\\r\\nimport stream.sources.csv.LocalWeatherDataSourceFunction;\\r\\nimport utils.DateUtilities;\\r\\n\\r\\nimport stream.sources.csv.LocalWeatherDataSourceFunction;\\r\\nimport utils.DateUtilities;\\r\\n\\r\\npublic class WeatherDataStreamingExample {\\r\\n\\r\\n    public static void main(String[] args) throws Exception {\\r\\n\\r\\n        final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();\\r\\n\\r\\n        // Use the Measurement Timestamp of the Event:\\r\\n        env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime);\\r\\n\\r\\n        // We are sequentially reading the historic data from a CSV file:\\r\\n        env.setParallelism(1);\\r\\n\\r\\n        // Path to read the CSV data from:\\r\\n        final String csvStationDataFilePath = \"C:\\\\\\\\Users\\\\\\\\philipp\\\\\\\\Downloads\\\\\\\\csv\\\\\\\\201503station.txt\";\\r\\n        final String csvLocalWeatherDataFilePath = \"C:\\\\\\\\Users\\\\\\\\philipp\\\\\\\\Downloads\\\\\\\\csv\\\\\\\\201503hourly_sorted.txt\";\\r\\n\\r\\n        // Add the CSV Data Source and assign the Measurement Timestamp:\\r\\n        DataStream<model.LocalWeatherData> localWeatherDataDataStream = env\\r\\n                .addSource(new LocalWeatherDataSourceFunction(csvStationDataFilePath, csvLocalWeatherDataFilePath))\\r\\n                .assignTimestampsAndWatermarks(new AscendingTimestampExtractor<LocalWeatherData>() {\\r\\n                    @Override\\r\\n                    public long extractAscendingTimestamp(LocalWeatherData localWeatherData) {\\r\\n                        Date measurementTime = DateUtilities.from(localWeatherData.getDate(), localWeatherData.getTime(), ZoneOffset.ofHours(localWeatherData.getStation().getTimeZone()));\\r\\n\\r\\n                        return measurementTime.getTime();\\r\\n                    }\\r\\n                });\\r\\n\\r\\n        // First build a KeyedStream over the Data with LocalWeather:\\r\\n        KeyedStream<LocalWeatherData, String> localWeatherDataByStation = localWeatherDataDataStream\\r\\n                // Filte for Non-Null Temperature Values, because we might have missing data:\\r\\n                .filter(new FilterFunction<LocalWeatherData>() {\\r\\n                    @Override\\r\\n                    public boolean filter(LocalWeatherData localWeatherData) throws Exception {\\r\\n                        return localWeatherData.getTemperature() != null;\\r\\n                    }\\r\\n                })\\r\\n                // Now create the keyed stream by the Station WBAN identifier:\\r\\n                .keyBy(new KeySelector<LocalWeatherData, String>() {\\r\\n                    @Override\\r\\n                    public String getKey(LocalWeatherData localWeatherData) throws Exception {\\r\\n                        return localWeatherData.getStation().getWban();\\r\\n                    }\\r\\n                });\\r\\n\\r\\n        // Now take the Maximum Temperature per day from the KeyedStream:\\r\\n        DataStream<LocalWeatherData> maxTemperaturePerDay =\\r\\n                localWeatherDataByStation\\r\\n                        // Use non-overlapping tumbling window with 1 day length:\\r\\n                        .timeWindow(Time.days(1))\\r\\n                        // And use the maximum temperature:\\r\\n                        .maxBy(\"temperature\");\\r\\n\\r\\n        env.execute(\"Max Temperature By Day example\");\\r\\n    }\\r\\n}\\r\\n\\nConclusion\\nIn this part of the series you have seen how to use the DataStream API to analyze data from the custom SourceFunction.\\nThe next part of the series shows how to write a custom SinkFunction for writing the DataStream results into a PostgreSQL database.\\n\\n\\nHow to contribute\\nOne of the easiest ways to contribute is to participate in discussions. You can also contribute by submitting pull requests.\\nGeneral feedback and discussions?\\nDo you have questions or feedback on this article? Please create an issue on the GitHub issue tracker.\\nSomething is wrong or missing?\\nThere may be something wrong or missing in this article. If you want to help fixing it, then please make a Pull Request to this file on GitHub.\\n\\n\\n\\n\\n\\n', metadata={'source': 'https://www.bytefish.de/blog/apache_flink_series_3.html', 'title': 'Building Applications with Apache Flink (Part 3): Stream Processing with the DataStream API', 'description': 'This article shows how to work with Apache Flink.', 'language': 'en'}),\n",
              " Document(page_content='\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nBuilding Applications with Apache Flink (Part 4): Writing and Using a custom PostgreSQL SinkFunction\\n\\n\\n\\n\\n\\nwww.bytefish.de\\n\\n\\nblog\\nabout\\npages\\ndocuments\\n\\n\\n\\n\\n\\n\\nBuilding Applications with Apache Flink (Part 4): Writing and Using a custom PostgreSQL SinkFunction\\nBy Philipp Wagner | July 03, 2016\\n\\n\\nIn this article I am going to show how to write a custom Apache Flink SinkFunction, that bulk writes results of a DataStream into a PostgreSQL database.\\nWhat we are going to build\\nProcessed data often needs to be written into a relational database, simply because SQL makes it easy to work with data. Often \\r\\nenough you will also need to generate reports for customers or feed an existing application, which uses the relational database. \\nA custom data sink for Apache Flink needs to implement the SinkFunction interface. If a resource needs to be opened and closed, then a \\r\\nRichSinkFunction needs to be implemented.\\nSource Code\\nYou can find the full source code for the example in my git repository at:\\n\\nhttps://github.com/bytefish/FlinkExperiments\\n\\nPostgreSQL SinkFunction\\nBasePostgresSink\\nWe start by implementing the abstract base class BasePostgresSink<TEntity>. It implements the RichSinkFunction, so it can create \\r\\na new BulkProcessor when opening the Sink, and close the BulkProcessor when closing the Sink. \\nYou may wonder, why I don\\'t pass the BulkProcessor as a dependency into the base class. It\\'s simply because Apache Flink serializes and distributes \\r\\nthe RichSinkFunction to each of its workers. That\\'s why the BulkProcessor is created inside of the RichSinkFunction, because all members of a \\r\\nRichSinkFunction need to be Serializable.\\n// Copyright (c) Philipp Wagner. All rights reserved.\\r\\n// Licensed under the MIT license. See LICENSE file in the project root for full license information.\\r\\n\\r\\npackage stream.sinks.pgsql;\\r\\n\\r\\nimport de.bytefish.pgbulkinsert.IPgBulkInsert;\\r\\nimport de.bytefish.pgbulkinsert.pgsql.processor.BulkProcessor;\\r\\nimport de.bytefish.pgbulkinsert.pgsql.processor.handler.BulkWriteHandler;\\r\\nimport org.apache.flink.configuration.Configuration;\\r\\nimport org.apache.flink.streaming.api.functions.sink.RichSinkFunction;\\r\\nimport pgsql.connection.PooledConnectionFactory;\\r\\n\\r\\nimport java.net.URI;\\r\\n\\r\\npublic abstract class BasePostgresSink<TEntity> extends RichSinkFunction<TEntity> {\\r\\n\\r\\n    private final URI databaseUri;\\r\\n    private final int bulkSize;\\r\\n\\r\\n    private BulkProcessor<TEntity> bulkProcessor;\\r\\n\\r\\n    public BasePostgresSink(URI databaseUri, int bulkSize) {\\r\\n        this.databaseUri = databaseUri;\\r\\n        this.bulkSize = bulkSize;\\r\\n    }\\r\\n\\r\\n    @Override\\r\\n    public void invoke(TEntity entity) throws Exception {\\r\\n        bulkProcessor.add(entity);\\r\\n    }\\r\\n\\r\\n    @Override\\r\\n    public void open(Configuration parameters) throws Exception {\\r\\n        this.bulkProcessor = new BulkProcessor<>(new BulkWriteHandler<>(getBulkInsert(), new PooledConnectionFactory(databaseUri)), bulkSize);\\r\\n    }\\r\\n\\r\\n    @Override\\r\\n    public void close() throws Exception {\\r\\n        bulkProcessor.close();\\r\\n    }\\r\\n\\r\\n    protected abstract IPgBulkInsert<TEntity> getBulkInsert();\\r\\n}\\r\\n\\nPooledConnectionFactory\\nThe BulkProcessor of PgBulkInsert needs a way to obtain a Connection for the database access. I don\\'t like reinventing \\r\\nthe wheel, so in my projects I simply use the great DBCP2 project for handling database connections. \\nYou can add the following dependencies to your pom.xml to include DBCP2 in your project:\\n<dependency>\\r\\n    <groupId>org.apache.commons</groupId>\\r\\n    <artifactId>commons-dbcp2</artifactId>\\r\\n    <version>2.0.1</version>\\r\\n</dependency>\\r\\n\\nThe Connection Factory for the BulkProcessor can then be implemented like this.\\n// Copyright (c) Philipp Wagner. All rights reserved.\\r\\n// Licensed under the MIT license. See LICENSE file in the project root for full license information.\\r\\n\\r\\npackage pgsql.connection;\\r\\n\\r\\nimport de.bytefish.pgbulkinsert.functional.Func1;\\r\\nimport org.apache.commons.dbcp2.BasicDataSource;\\r\\n\\r\\nimport java.net.URI;\\r\\nimport java.sql.Connection;\\r\\n\\r\\npublic class PooledConnectionFactory implements Func1<Connection> {\\r\\n\\r\\n    private final BasicDataSource connectionPool;\\r\\n\\r\\n    public PooledConnectionFactory(URI databaseUri) {\\r\\n        this.connectionPool = new BasicDataSource();\\r\\n\\r\\n        initializeConnectionPool(connectionPool, databaseUri);\\r\\n    }\\r\\n\\r\\n    private void initializeConnectionPool(BasicDataSource connectionPool, URI databaseUri) {\\r\\n        final String dbUrl = \"jdbc:postgresql://\" + databaseUri.getHost() + databaseUri.getPath();\\r\\n\\r\\n        if (databaseUri.getUserInfo() != null) {\\r\\n            connectionPool.setUsername(databaseUri.getUserInfo().split(\":\")[0]);\\r\\n            connectionPool.setPassword(databaseUri.getUserInfo().split(\":\")[1]);\\r\\n        }\\r\\n        connectionPool.setDriverClassName(\"org.postgresql.Driver\");\\r\\n        connectionPool.setUrl(dbUrl);\\r\\n        connectionPool.setInitialSize(1);\\r\\n    }\\r\\n\\r\\n    @Override\\r\\n    public Connection invoke() throws Exception {\\r\\n        return connectionPool.getConnection();\\r\\n    }\\r\\n}\\r\\n\\nExample\\nDatabase Setup\\nFirst of all we are going to write the DDL scripts for creating the database schema and tables. \\nSchema\\nI am using schemas to keep my database clean and so should you. A database schema logically groups the objects such as tables, views, \\r\\nstored procedures, ... and makes it possible to assign user permissions to the schema. In this example the sample schema is going \\r\\nto contain the tables for the station and measurement data.\\nDO $$\\r\\nBEGIN\\r\\n\\r\\nIF NOT EXISTS (SELECT 1 FROM information_schema.schemata WHERE schema_name = \\'sample\\') THEN\\r\\n\\r\\n    CREATE SCHEMA sample;\\r\\n\\r\\nEND IF;\\r\\n\\r\\nEND;\\r\\n$$;\\r\\n\\nTables\\nDO $$\\r\\nBEGIN\\r\\n\\r\\nIF NOT EXISTS (\\r\\n    SELECT 1 \\r\\n    FROM information_schema.tables \\r\\n    WHERE  table_schema = \\'sample\\' \\r\\n    AND table_name = \\'station\\'\\r\\n) THEN\\r\\n\\r\\nCREATE TABLE sample.station\\r\\n(\\r\\n    station_id SERIAL PRIMARY KEY,\\r\\n    wban VARCHAR(255) NOT NULL,\\r\\n    name VARCHAR(255) NOT NULL,\\r\\n    state VARCHAR(255), \\r\\n    location VARCHAR(255),\\r\\n    latitude REAL NOT NULL,\\r\\n    longitude REAL NOT NULL,\\r\\n    ground_height SMALLINT,\\r\\n    station_height SMALLINT,\\r\\n    TimeZone SMALLINT\\r\\n);\\r\\n\\r\\nEND IF;\\r\\n\\r\\nIF NOT EXISTS (\\r\\n    SELECT 1 \\r\\n    FROM information_schema.tables \\r\\n    WHERE  table_schema = \\'sample\\' \\r\\n    AND table_name = \\'weather_data\\'\\r\\n) THEN\\r\\n\\r\\nCREATE TABLE sample.weather_data\\r\\n(\\r\\n    wban VARCHAR(255),\\r\\n    dateTime TIMESTAMP,\\r\\n    temperature REAL,\\r\\n    windSpeed REAL,\\r\\n    stationPressure REAL,\\r\\n    skyCondition VARCHAR(255)   \\r\\n);\\r\\n\\r\\nEND IF;\\r\\n\\r\\nEND;\\r\\n$$;\\r\\n\\nConstraints\\nDO $$\\r\\nBEGIN\\r\\n\\r\\nIF NOT EXISTS (SELECT 1 FROM pg_constraint WHERE conname = \\'uk_station_wban\\') THEN\\r\\n    ALTER TABLE sample.station\\r\\n        ADD CONSTRAINT uk_station_wban\\r\\n        UNIQUE (wban);\\r\\nEND IF;\\r\\n\\r\\nEND;\\r\\n$$;\\r\\n\\nSecurity\\nDO $$\\r\\nBEGIN\\r\\n\\r\\nREVOKE ALL ON sample.station FROM public;\\r\\nREVOKE ALL ON sample.weather_data FROM public;\\r\\n\\r\\nEND;\\r\\n$$;\\r\\n\\nStation Data\\nWe are not going to persist the station data in the SQL database. That\\'s why the Stations are initially populated with a simple insert script, which was generated \\r\\nfrom the original data. It contains all the informations, which is also set in the domain model.\\nDO $$\\r\\nBEGIN\\r\\n\\r\\nIF EXISTS (\\r\\n    SELECT 1 \\r\\n    FROM information_schema.tables \\r\\n    WHERE  table_schema = \\'sample\\' \\r\\n    AND table_name = \\'station\\'\\r\\n) THEN\\r\\n\\r\\n\\r\\nINSERT INTO sample.station(wban, name, state, location, latitude, longitude, ground_height, station_height, timeZone)\\r\\nSELECT \\'00100\\', \\'ARKADELPHIA\\', \\'AR\\', \\'DEXTER B FLORENCE MEM FLD AP\\', 34.09972, -93.06583, 182, NULL, -6\\r\\nWHERE NOT EXISTS (SELECT 1 FROM sample.station WHERE wban=\\'00100\\');\\r\\n\\r\\n-- ... station data\\r\\n\\r\\n\\r\\nEND IF;\\r\\n\\r\\nEND\\r\\n$$;\\r\\n\\nCreating a Deployment Script\\nYou could copy and paste the above scripts for this tutorial. This is totally OK for small applications, but it won\\'t scale for any real project. \\r\\nBelieve me, you need to automate the task of creating and migrating a database as early as possible in your project.\\nI am working in a Windows environment right now, so I have used a Batch file to automate the database setup. There is no magic going on, I am just \\r\\nsetting the path to psql and use the PGPASSWORD environment variable to pass the password to the command line.\\n@echo off\\r\\n\\r\\n:: Copyright (c) Philipp Wagner. All rights reserved.\\r\\n:: Licensed under the MIT license. See LICENSE file in the project root for full license information.\\r\\n\\r\\nset PGSQL_EXECUTABLE=\"C:\\\\Program Files\\\\PostgreSQL\\\\9.4\\\\bin\\\\psql.exe\"\\r\\nset STDOUT=stdout.log\\r\\nset STDERR=stderr.log\\r\\nset LOGFILE=query_output.log\\r\\n\\r\\nset HostName=localhost\\r\\nset PortNumber=5432\\r\\nset DatabaseName=sampledb\\r\\nset UserName=philipp\\r\\nset Password=\\r\\n\\r\\ncall :AskQuestionWithYdefault \"Use Host (%HostName%) Port (%PortNumber%) [Y,n]?\" reply_\\r\\nif /i [%reply_%] NEQ [y] (\\r\\n    set /p HostName=\"Enter HostName: \"\\r\\n    set /p PortNumber=\"Enter Port: \"\\r\\n)\\r\\n\\r\\ncall :AskQuestionWithYdefault \"Use Database (%DatabaseName%) [Y,n]?\" reply_\\r\\nif /i [%reply_%] NEQ [y]  (\\r\\n    set /p ServerName=\"Enter Database: \"\\r\\n)\\r\\n\\r\\ncall :AskQuestionWithYdefault \"Use User (%UserName%) [Y,n]?\" reply_\\r\\nif /i [%reply_%] NEQ [y]  (\\r\\n    set /p UserName=\"Enter User: \"\\r\\n)\\r\\n\\r\\nset /p PGPASSWORD=\"Password: \"\\r\\n\\r\\n1>%STDOUT% 2>%STDERR% (\\r\\n\\r\\n    :: Schemas\\r\\n    %PGSQL_EXECUTABLE% -h %HostName% -p %PortNumber% -d %DatabaseName% -U %UserName% < 01_Schemas/schema_sample.sql -L %LOGFILE%\\r\\n\\r\\n    :: Tables\\r\\n    %PGSQL_EXECUTABLE% -h %HostName% -p %PortNumber% -d %DatabaseName% -U %UserName% < 02_Tables/tables_sample.sql -L %LOGFILE%\\r\\n\\r\\n    :: Keys\\r\\n    %PGSQL_EXECUTABLE% -h %HostName% -p %PortNumber% -d %DatabaseName% -U %UserName% < 03_Keys/keys_sample.sql -L %LOGFILE%\\r\\n\\r\\n    :: Security\\r\\n    %PGSQL_EXECUTABLE% -h %HostName% -p %PortNumber% -d %DatabaseName% -U %UserName% < 05_Security/security_sample.sql -L %LOGFILE%\\r\\n\\r\\n    :: Data\\r\\n    %PGSQL_EXECUTABLE% -h %HostName% -p %PortNumber% -d %DatabaseName% -U %UserName% < 06_Data/data_sample_stations.sql -L %LOGFILE%\\r\\n)\\r\\n\\r\\ngoto :end\\r\\n\\r\\n:: The question as a subroutine\\r\\n:AskQuestionWithYdefault\\r\\n    setlocal enableextensions\\r\\n    :_asktheyquestionagain\\r\\n    set return_=\\r\\n    set ask_=\\r\\n    set /p ask_=\"%~1\"\\r\\n    if \"%ask_%\"==\"\" set return_=y\\r\\n    if /i \"%ask_%\"==\"Y\" set return_=y\\r\\n    if /i \"%ask_%\"==\"n\" set return_=n\\r\\n    if not defined return_ goto _asktheyquestionagain\\r\\n    endlocal & set \"%2=%return_%\" & goto :EOF\\r\\n\\r\\n:end\\r\\npause\\r\\n\\nPostgreSQL Model\\nYou have already seen, that we are building a separate model for each use case. This keeps the analysis model clean and so we do not leak any \\r\\ndatabase related modelling into the analysis model (foreign keys, column names, ...). Note, that the measurement time should be stored as a \\r\\nPostgreSQL timestamp, so it doesn\\'t have any timezone information. The data in the station table already holds the relevant timezone offset. \\nI have decided against using the Primary Key of a Station as a Foreign Key constraint, because such measurement data should be stored as fast as \\r\\npossible, without being eventually slowed down by Foreign Key constraints.\\n// Copyright (c) Philipp Wagner. All rights reserved.\\r\\n// Licensed under the MIT license. See LICENSE file in the project root for full license information.\\r\\n\\r\\npackage pgsql.model;\\r\\n\\r\\nimport java.time.LocalDateTime;\\r\\n\\r\\npublic class LocalWeatherData {\\r\\n\\r\\n    private String wban;\\r\\n\\r\\n    private LocalDateTime dateTime;\\r\\n\\r\\n    private Float temperature;\\r\\n\\r\\n    private Float windSpeed;\\r\\n\\r\\n    private Float stationPressure;\\r\\n\\r\\n    private String skyCondition;\\r\\n\\r\\n    public LocalWeatherData(String wban, LocalDateTime dateTime, Float temperature, Float windSpeed, Float stationPressure, String skyCondition) {\\r\\n        this.wban = wban;\\r\\n        this.dateTime = dateTime;\\r\\n        this.temperature = temperature;\\r\\n        this.windSpeed = windSpeed;\\r\\n        this.stationPressure = stationPressure;\\r\\n        this.skyCondition = skyCondition;\\r\\n    }\\r\\n\\r\\n    public String getWban() {\\r\\n        return wban;\\r\\n    }\\r\\n\\r\\n    public LocalDateTime getDateTime() {\\r\\n        return dateTime;\\r\\n    }\\r\\n\\r\\n    public Float getTemperature() {\\r\\n        return temperature;\\r\\n    }\\r\\n\\r\\n    public Float getWindSpeed() {\\r\\n        return windSpeed;\\r\\n    }\\r\\n\\r\\n    public Float getStationPressure() {\\r\\n        return stationPressure;\\r\\n    }\\r\\n\\r\\n    public String getSkyCondition() {\\r\\n        return skyCondition;\\r\\n    }\\r\\n}\\r\\n\\nPgBulkInsert Database Mapping\\nThe BasePostgresSink function has to implement a function, that returns a PgBulkInsert<TEntity>. A PgBulkInsert<TEntity> in PgBulkInsert simply \\r\\ndefines the mapping between a database table and the domain model.\\n// Copyright (c) Philipp Wagner. All rights reserved.\\r\\n// Licensed under the MIT license. See LICENSE file in the project root for full license information.\\r\\n\\r\\npackage pgsql.mapping;\\r\\n\\r\\nimport de.bytefish.pgbulkinsert.PgBulkInsert;\\r\\n\\r\\npublic class LocalWeatherDataBulkInsert extends PgBulkInsert<pgsql.model.LocalWeatherData> {\\r\\n\\r\\n    public LocalWeatherDataBulkInsert(String schemaName, String tableName) {\\r\\n\\r\\n        super(schemaName, tableName);\\r\\n\\r\\n        mapString(\"wban\", pgsql.model.LocalWeatherData::getWban);\\r\\n        mapTimeStamp(\"dateTime\", pgsql.model.LocalWeatherData::getDateTime);\\r\\n        mapReal(\"temperature\", pgsql.model.LocalWeatherData::getTemperature);\\r\\n        mapReal(\"windSpeed\", pgsql.model.LocalWeatherData::getWindSpeed);\\r\\n        mapReal(\"stationPressure\", pgsql.model.LocalWeatherData::getStationPressure);\\r\\n        mapString(\"skyCondition\", pgsql.model.LocalWeatherData::getSkyCondition);\\r\\n    }\\r\\n\\r\\n}\\r\\n\\nLocalWeatherDataPostgresSink\\nWith the PostgreSQL domain model defined, the BasePostgresSink and the PgBulkInsert mapping, the LocalWeatherDataPostgresSink for the example can easily be implemented.\\n// Copyright (c) Philipp Wagner. All rights reserved.\\r\\n// Licensed under the MIT license. See LICENSE file in the project root for full license information.\\r\\n\\r\\npackage stream.sinks.pgsql;\\r\\n\\r\\nimport de.bytefish.pgbulkinsert.IPgBulkInsert;\\r\\n\\r\\nimport java.net.URI;\\r\\n\\r\\npublic class LocalWeatherDataPostgresSink extends BasePostgresSink<pgsql.model.LocalWeatherData> {\\r\\n\\r\\n    public LocalWeatherDataPostgresSink(URI databaseUri, int bulkSize) {\\r\\n        super(databaseUri, bulkSize);\\r\\n    }\\r\\n\\r\\n    @Override\\r\\n    protected IPgBulkInsert<pgsql.model.LocalWeatherData> getBulkInsert() {\\r\\n        return new pgsql.mapping.LocalWeatherDataBulkInsert(\"sample\", \"weather_data\");\\r\\n    }\\r\\n\\r\\n}\\r\\n\\nPlugging it into the DataStream\\nOnce the SinkFunction is written, it can be plugged into the existing DataStream pipeline. In the example the general DataStream<model.LocalWeatherData> \\r\\nis first transformed into a DataStream``. Then the custom PostgreSQL Sink is added to the DataStream, with a connection factory that \\r\\n connects to a local database instance and a bulk size of 1000 entities.\\n// Converts the general stream into the Postgres-specific representation:\\r\\nDataStream<pgsql.model.LocalWeatherData> pgsqlDailyMaxTemperature = maxTemperaturePerDay\\r\\n        .map(new MapFunction<model.LocalWeatherData, pgsql.model.LocalWeatherData>() {\\r\\n            @Override\\r\\n            public pgsql.model.LocalWeatherData map(model.LocalWeatherData localWeatherData) throws Exception {\\r\\n                return pgsql.converter.LocalWeatherDataConverter.convert(localWeatherData);\\r\\n            }\\r\\n        });\\r\\n\\r\\n// Add a new Postgres Sink with a Bulk Size of 1000 entities:\\r\\npgsqlDailyMaxTemperature.addSink(new LocalWeatherDataPostgresSink(URI.create(\"postgres://philipp:test_pwd@127.0.0.1:5432/sampledb\"), 1000));\\r\\n\\nConverter\\nThe LocalWeatherDataConverter simply takes a model.LocalWeatherData and converts it into a pgsql.model.LocalWeatherData.\\n// Copyright (c) Philipp Wagner. All rights reserved.\\r\\n// Licensed under the MIT license. See LICENSE file in the project root for full license information.\\r\\n\\r\\npackage pgsql.converter;\\r\\n\\r\\nimport java.time.LocalDateTime;\\r\\n\\r\\npublic class LocalWeatherDataConverter {\\r\\n\\r\\n    public static pgsql.model.LocalWeatherData convert(model.LocalWeatherData modelLocalWeatherData) {\\r\\n\\r\\n        String wban = modelLocalWeatherData.getStation().getWban();\\r\\n        LocalDateTime dateTime = modelLocalWeatherData.getDate().atTime(modelLocalWeatherData.getTime());\\r\\n        Float temperature = modelLocalWeatherData.getTemperature();\\r\\n        Float windSpeed = modelLocalWeatherData.getWindSpeed();\\r\\n        Float stationPressure = modelLocalWeatherData.getStationPressure();\\r\\n        String skyCondition = modelLocalWeatherData.getSkyCondition();\\r\\n\\r\\n        return new pgsql.model.LocalWeatherData(wban, dateTime, temperature, windSpeed, stationPressure, skyCondition);\\r\\n    }\\r\\n}\\r\\n\\nConclusion\\nIn this article you have seen how to write a custom SinkFunction for Apache Flink. In the next article you will see how to write a custom SinkFunction for \\r\\nwriting into an Elasticsearch database and visualize the results with Kibana, which is a Frontend to Elasticsearch.\\n\\n\\nHow to contribute\\nOne of the easiest ways to contribute is to participate in discussions. You can also contribute by submitting pull requests.\\nGeneral feedback and discussions?\\nDo you have questions or feedback on this article? Please create an issue on the GitHub issue tracker.\\nSomething is wrong or missing?\\nThere may be something wrong or missing in this article. If you want to help fixing it, then please make a Pull Request to this file on GitHub.\\n\\n\\n\\n\\n\\n', metadata={'source': 'https://www.bytefish.de/blog/apache_flink_series_4.html', 'title': 'Building Applications with Apache Flink (Part 4): Writing and Using a custom PostgreSQL SinkFunction', 'description': 'This article shows how to write a custom PostgreSQL SinkFunction for Apache Flink.', 'language': 'en'}),\n",
              " Document(page_content='\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nBuilding Applications with Apache Flink (Part 5): Complex Event Processing with Apache Flink\\n\\n\\n\\n\\n\\nwww.bytefish.de\\n\\n\\nblog\\nabout\\npages\\ndocuments\\n\\n\\n\\n\\n\\n\\nBuilding Applications with Apache Flink (Part 5): Complex Event Processing with Apache Flink\\nBy Philipp Wagner | July 10, 2016\\n\\n\\nIn this article I want to show you how to work with the Complex Event Processing (CEP) engine of Apache Flink. \\nThe Apache Flink documentation describes FlinkCEP as:\\n\\nFlinkCEP is the complex event processing library for Flink. It allows you to easily detect complex event patterns in a stream \\r\\nof endless data. Complex events can then be constructed from matching sequences. This gives you the opportunity to quickly get \\r\\nhold of what\\'s really important in your data.\\n\\nWhat we are going to build\\nImagine we are designing an application to generate warnings based on certain weather events.\\nThe application should generate weather warnings from a Stream of incoming measurements:\\n\\nExtreme Cold (less than -46 °C for three days)\\nSevere Heat (above 30 °C for two days)\\nExcessive Heat (above 41°C for two days)\\nHigh Wind (wind speed between 39 mph and 110 mph)\\nExtreme Wind (wind speed above 110 mph)\\n\\nSource Code\\nYou can find the full source code for the example in my git repository at:\\n\\nhttps://github.com/bytefish/FlinkExperiments\\n\\nWarnings and Patterns\\nFirst of all we are designing the model for Warnings and their associated patterns.\\nAll warnings in the application derive from the marker Interface IWarning.\\n// Copyright (c) Philipp Wagner. All rights reserved.\\r\\n// Licensed under the MIT license. See LICENSE file in the project root for full license information.\\r\\n\\r\\npackage app.cep.model;\\r\\n\\r\\n/**\\r\\n * Marker interface used for Warnings.\\r\\n */\\r\\npublic interface IWarning {\\r\\n\\r\\n}\\r\\n\\nA warning is always generated by a certain pattern, so we create an interface IWarningPattern for it. The actual patterns for Apache Flink will be defined with the Pattern API. \\nOnce a pattern has been matched, Apache Flink emits a Map<String, TEventType> to the environment, which contains the names and events of the match. So implementations of \\r\\nthe IWarningPattern also define how to map between the Apache Flink result and a certain warning. \\nAnd finally to simplify reflection when building the Apache Flink stream processing pipeline, the IWarningPattern also returns the type of the warning.\\n// Copyright (c) Philipp Wagner. All rights reserved.\\r\\n// Licensed under the MIT license. See LICENSE file in the project root for full license information.\\r\\n\\r\\npackage app.cep.model;\\r\\n\\r\\nimport org.apache.flink.cep.pattern.Pattern;\\r\\n\\r\\nimport java.io.Serializable;\\r\\nimport java.util.List;\\r\\nimport java.util.Map;\\r\\n\\r\\n/**\\r\\n * A Warning Pattern describes the pattern of a Warning, which is triggered by an Event.\\r\\n *\\r\\n * @param <TEventType> Event Type\\r\\n * @param <TWarningType> Warning Type\\r\\n */\\r\\npublic interface IWarningPattern<TEventType, TWarningType extends IWarning> extends Serializable {\\r\\n\\r\\n    /**\\r\\n     * Implements the mapping between the pattern matching result and the warning.\\r\\n     *\\r\\n     * @param pattern Pattern, which has been matched by Apache Flink.\\r\\n     * @return The warning created from the given match result.\\r\\n     */\\r\\n    TWarningType create(Map<String, List<TEventType>> pattern);\\r\\n\\r\\n    /**\\r\\n     * Implementes the Apache Flink CEP Event Pattern which triggers a warning.\\r\\n     *\\r\\n     * @return The Apache Flink CEP Pattern definition.\\r\\n     */\\r\\n    Pattern<TEventType, ?> getEventPattern();\\r\\n\\r\\n    /**\\r\\n     * Returns the Warning Class for simplifying reflection.\\r\\n     *\\r\\n     * @return Class Type of the Warning.\\r\\n     */\\r\\n    Class<TWarningType> getWarningTargetType();\\r\\n\\r\\n}\\r\\n\\nExcessive Heat Warning\\nNow we can implement the weather warnings and their patterns. The patterns are highly simplified in this article. \\nOne of the warnings could be a warning for Excessive Heat, which is described on Wikipedia as:\\n\\nExcessive Heat Warning – Extreme Heat Index (HI) values forecast to meet or exceed locally defined warning criteria for at least two days.\\r\\nSpecific criteria varies among local Weather Forecast Offices, due to climate variability and the effect of excessive heat on the local\\r\\npopulation.\\nTypical HI values are maximum daytime temperatures above 105 to 110 °F (41 to 43 °C) and minimum nighttime temperatures above 75 °F (24 °C).\\n\\nWarning Model\\nThe warning should be issued if we expect daytime temperatures above 41 °C for at least two days. So the ExcessiveHeatWarning class \\r\\ntakes two LocalWeatherData measurements, and also provides a short summary in its toString method.\\n// Copyright (c) Philipp Wagner. All rights reserved.\\r\\n// Licensed under the MIT license. See LICENSE file in the project root for full license information.\\r\\n\\r\\npackage app.cep.model.warnings.temperature;\\r\\n\\r\\nimport app.cep.model.IWarning;\\r\\nimport model.LocalWeatherData;\\r\\n\\r\\npublic class ExcessiveHeatWarning implements IWarning {\\r\\n\\r\\n    private final LocalWeatherData localWeatherData0;\\r\\n    private final LocalWeatherData localWeatherData1;\\r\\n\\r\\n    public ExcessiveHeatWarning(LocalWeatherData localWeatherData0, LocalWeatherData localWeatherData1) {\\r\\n        this.localWeatherData0 = localWeatherData0;\\r\\n        this.localWeatherData1 = localWeatherData1;\\r\\n    }\\r\\n\\r\\n    public LocalWeatherData getLocalWeatherData0() {\\r\\n        return localWeatherData0;\\r\\n    }\\r\\n\\r\\n    public LocalWeatherData getLocalWeatherData1() {\\r\\n        return localWeatherData1;\\r\\n    }\\r\\n\\r\\n    @Override\\r\\n    public String toString() {\\r\\n        return String.format(\"ExcessiveHeatWarning (WBAN = %s, First Measurement = (%s), Second Measurement = (%s))\",\\r\\n                localWeatherData0.getStation().getWban(),\\r\\n                getEventSummary(localWeatherData0),\\r\\n                getEventSummary(localWeatherData1));\\r\\n    }\\r\\n\\r\\n    private String getEventSummary(LocalWeatherData localWeatherData) {\\r\\n\\r\\n        return String.format(\"Date = %s, Time = %s, Temperature = %f\",\\r\\n                localWeatherData.getDate(), localWeatherData.getTime(), localWeatherData.getTemperature());\\r\\n    }\\r\\n}\\r\\n\\nWarning Pattern\\nNow comes the interesting part, the Pattern. The ExcessiveHeatWarningPattern implements the IWarningPattern interface and \\r\\nuses the Pattern API to define the matching pattern. You can see, that we are using strict contiguity for the events, using the \\r\\nthe next operator. The events should occur for the maximum temperature of 2 days, so we expect these events to be within 2 days.\\n// Copyright (c) Philipp Wagner. All rights reserved.\\r\\n// Licensed under the MIT license. See LICENSE file in the project root for full license information.\\r\\n\\r\\npackage app.cep.model.patterns.temperature;\\r\\n\\r\\nimport app.cep.model.IWarningPattern;\\r\\nimport app.cep.model.warnings.temperature.ExcessiveHeatWarning;\\r\\nimport model.LocalWeatherData;\\r\\nimport org.apache.flink.cep.pattern.Pattern;\\r\\nimport org.apache.flink.cep.pattern.conditions.SimpleCondition;\\r\\nimport org.apache.flink.streaming.api.windowing.time.Time;\\r\\n\\r\\nimport java.util.List;\\r\\nimport java.util.Map;\\r\\n\\r\\n/**\\r\\n * Excessive Heat Warning – Extreme Heat Index (HI) values forecast to meet or exceed locally defined warning criteria for at least two days.\\r\\n * Specific criteria varies among local Weather Forecast Offices, due to climate variability and the effect of excessive heat on the local\\r\\n * population.\\r\\n *\\r\\n * Typical HI values are maximum daytime temperatures above 105 to 110 °F (41 to 43 °C) and minimum nighttime temperatures above 75 °F (24 °C).\\r\\n */\\r\\npublic class ExcessiveHeatWarningPattern implements IWarningPattern<LocalWeatherData, ExcessiveHeatWarning> {\\r\\n\\r\\n    public ExcessiveHeatWarningPattern() {}\\r\\n\\r\\n    @Override\\r\\n    public ExcessiveHeatWarning create(Map<String, List<LocalWeatherData>> pattern) {\\r\\n        LocalWeatherData first = pattern.get(\"First Event\").get(0);\\r\\n        LocalWeatherData second = pattern.get(\"Second Event\").get(0);\\r\\n\\r\\n        return new ExcessiveHeatWarning(first, second);\\r\\n    }\\r\\n\\r\\n    @Override\\r\\n    public Pattern<LocalWeatherData, ?> getEventPattern() {\\r\\n        return Pattern\\r\\n                .<LocalWeatherData>begin(\"First Event\").where(\\r\\n                        new SimpleCondition<LocalWeatherData>() {\\r\\n                            @Override\\r\\n                            public boolean filter(LocalWeatherData event) throws Exception {\\r\\n                                return event.getTemperature() >= 41.0f;\\r\\n                            }\\r\\n                        })\\r\\n                .next(\"Second Event\").where(\\r\\n                        new SimpleCondition<LocalWeatherData>() {\\r\\n                            @Override\\r\\n                            public boolean filter(LocalWeatherData event) throws Exception {\\r\\n                                return event.getTemperature() >= 41.0f;\\r\\n                            }\\r\\n                        })\\r\\n                .within(Time.days(2));\\r\\n    }\\r\\n\\r\\n    @Override\\r\\n    public Class<ExcessiveHeatWarning> getWarningTargetType() {\\r\\n        return ExcessiveHeatWarning.class;\\r\\n    }\\r\\n}\\r\\n\\nConverting a Stream into a Stream of Warnings\\nNow it\\'s time to apply these patterns on a DataStream<TEventType>. In this example we are operating on the Stream of historical weather measurements, which \\r\\nhave been used in previous articles. These historical values could easily be exchanged with forecasts, so it makes a nice example.\\nI have written a method toWarningStream, which will take a DataStream<LocalWeatherData> and generate a DataStream with the warnings.\\n// Copyright (c) Philipp Wagner. All rights reserved.\\r\\n// Licensed under the MIT license. See LICENSE file in the project root for full license information.\\r\\n\\r\\npackage app.cep;\\r\\n\\r\\nimport app.cep.model.IWarning;\\r\\nimport app.cep.model.IWarningPattern;\\r\\nimport app.cep.model.patterns.temperature.SevereHeatWarningPattern;\\r\\nimport app.cep.model.warnings.temperature.SevereHeatWarning;\\r\\nimport model.LocalWeatherData;\\r\\nimport org.apache.flink.api.common.functions.FilterFunction;\\r\\nimport org.apache.flink.api.java.functions.KeySelector;\\r\\nimport org.apache.flink.api.java.typeutils.GenericTypeInfo;\\r\\nimport org.apache.flink.cep.CEP;\\r\\nimport org.apache.flink.cep.PatternSelectFunction;\\r\\nimport org.apache.flink.cep.PatternStream;\\r\\nimport org.apache.flink.streaming.api.TimeCharacteristic;\\r\\nimport org.apache.flink.streaming.api.datastream.DataStream;\\r\\nimport org.apache.flink.streaming.api.datastream.KeyedStream;\\r\\nimport org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;\\r\\nimport org.apache.flink.streaming.api.functions.timestamps.AscendingTimestampExtractor;\\r\\nimport org.apache.flink.streaming.api.windowing.time.Time;\\r\\nimport stream.sources.csv.LocalWeatherDataSourceFunction;\\r\\nimport utils.DateUtilities;\\r\\n\\r\\nimport java.time.ZoneOffset;\\r\\nimport java.util.Date;\\r\\nimport java.util.List;\\r\\nimport java.util.Map;\\r\\n\\r\\npublic class WeatherDataComplexEventProcessingExample {\\r\\n\\r\\n    public static void main(String[] args) throws Exception {\\r\\n\\r\\n        final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();\\r\\n\\r\\n        // Use the Measurement Timestamp of the Event:\\r\\n        env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime);\\r\\n\\r\\n        // Path to read the CSV data from:\\r\\n        final String csvStationDataFilePath = \"C:\\\\\\\\Users\\\\\\\\philipp\\\\\\\\Downloads\\\\\\\\csv\\\\\\\\201503station.txt\";\\r\\n        final String csvLocalWeatherDataFilePath = \"C:\\\\\\\\Users\\\\\\\\philipp\\\\\\\\Downloads\\\\\\\\csv\\\\\\\\201503hourly_sorted.txt\";\\r\\n\\r\\n\\r\\n        // Add the CSV Data Source and assign the Measurement Timestamp:\\r\\n        DataStream<model.LocalWeatherData> localWeatherDataDataStream = env\\r\\n                .addSource(new LocalWeatherDataSourceFunction(csvStationDataFilePath, csvLocalWeatherDataFilePath))\\r\\n                .assignTimestampsAndWatermarks(new AscendingTimestampExtractor<LocalWeatherData>() {\\r\\n                    @Override\\r\\n                    public long extractAscendingTimestamp(LocalWeatherData localWeatherData) {\\r\\n                        Date measurementTime = DateUtilities.from(localWeatherData.getDate(), localWeatherData.getTime(), ZoneOffset.ofHours(0));\\r\\n\\r\\n                        return measurementTime.getTime();\\r\\n                    }\\r\\n                });\\r\\n\\r\\n        // First build a KeyedStream over the Data with LocalWeather:\\r\\n        KeyedStream<LocalWeatherData, String> localWeatherDataByStation = localWeatherDataDataStream\\r\\n                // Filter for Non-Null Temperature Values, because we might have missing data:\\r\\n                .filter(new FilterFunction<LocalWeatherData>() {\\r\\n                    @Override\\r\\n                    public boolean filter(LocalWeatherData localWeatherData) throws Exception {\\r\\n                        return localWeatherData.getTemperature() != null;\\r\\n                    }\\r\\n                })\\r\\n                // Now create the keyed stream by the Station WBAN identifier:\\r\\n                .keyBy(new KeySelector<LocalWeatherData, String>() {\\r\\n                    @Override\\r\\n                    public String getKey(LocalWeatherData localWeatherData) throws Exception {\\r\\n                        return localWeatherData.getStation().getWban();\\r\\n                    }\\r\\n                });\\r\\n\\r\\n        // Now take the Maximum Temperature per day from the KeyedStream:\\r\\n        DataStream<LocalWeatherData> maxTemperaturePerDay =\\r\\n                localWeatherDataByStation\\r\\n                        // Use non-overlapping tumbling window with 1 day length:\\r\\n                        .timeWindow(Time.days(1))\\r\\n                        // And use the maximum temperature:\\r\\n                        .maxBy(\"temperature\");\\r\\n\\r\\n        // Now apply the SevereHeatWarningPattern on the Stream:\\r\\n        DataStream<SevereHeatWarning> warnings =  toWarningStream(maxTemperaturePerDay, new SevereHeatWarningPattern());\\r\\n\\r\\n        // Print the warning to the Console for now:\\r\\n        warnings.print();\\r\\n\\r\\n       // Finally execute the Stream:\\r\\n        env.execute(\"CEP Weather Warning Example\");\\r\\n    }\\r\\n\\r\\n    private static <TWarningType extends IWarning> DataStream<TWarningType> toWarningStream(DataStream<LocalWeatherData> localWeatherDataDataStream, IWarningPattern<LocalWeatherData, TWarningType> warningPattern) {\\r\\n        PatternStream<LocalWeatherData> tempPatternStream = CEP.pattern(\\r\\n                localWeatherDataDataStream.keyBy(new KeySelector<LocalWeatherData, String>() {\\r\\n                    @Override\\r\\n                    public String getKey(LocalWeatherData localWeatherData) throws Exception {\\r\\n                        return localWeatherData.getStation().getWban();\\r\\n                    }\\r\\n                }),\\r\\n                warningPattern.getEventPattern());\\r\\n\\r\\n        DataStream<TWarningType> warnings = tempPatternStream.select(new PatternSelectFunction<LocalWeatherData, TWarningType>() {\\r\\n            @Override\\r\\n            public TWarningType select(Map<String, List<LocalWeatherData>> map) throws Exception {\\r\\n                return warningPattern.create(map);\\r\\n            }\\r\\n        }, new GenericTypeInfo<TWarningType>(warningPattern.getWarningTargetType()));\\r\\n\\r\\n        return warnings;\\r\\n    }\\r\\n\\r\\n}\\r\\n\\n\\n\\nHow to contribute\\nOne of the easiest ways to contribute is to participate in discussions. You can also contribute by submitting pull requests.\\nGeneral feedback and discussions?\\nDo you have questions or feedback on this article? Please create an issue on the GitHub issue tracker.\\nSomething is wrong or missing?\\nThere may be something wrong or missing in this article. If you want to help fixing it, then please make a Pull Request to this file on GitHub.\\n\\n\\n\\n\\n\\n', metadata={'source': 'https://www.bytefish.de/blog/apache_flink_series_5.html', 'title': 'Building Applications with Apache Flink (Part 5): Complex Event Processing with Apache Flink', 'description': 'This article shows how to work with the Complex Event Processing Engine of Apache Flink.', 'language': 'en'})]"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "list_of_websites = [\"https://cwiki.apache.org/confluence/display/FLINK/Apache+Flink+Home\",\n",
        "                    \"https://twitter.com/ApacheFlink\",\n",
        "                    \"https://www.meetup.com/topics/apache-flink/\",\n",
        "                    \"https://flink.apache.org/posts/\",\n",
        "                    \"https://www.ververica.com/blog\",\n",
        "                    \"https://rawkintrevo.org/category/flink/\",\n",
        "                    \"https://data-flair.training/blogs/category/flink/\",\n",
        "                    \"https://www.bytefish.de/blog/apache_flink_series_1.html\",\n",
        "                    \"https://www.bytefish.de/blog/apache_flink_series_2.html\",\n",
        "                    \"https://www.bytefish.de/blog/apache_flink_series_3.html\",\n",
        "                    \"https://www.bytefish.de/blog/apache_flink_series_4.html\",\n",
        "                    \"https://www.bytefish.de/blog/apache_flink_series_5.html\",\n",
        "                    ]\n",
        "loader = WebBaseLoader(list_of_websites)\n",
        "webs = loader.load()\n",
        "webs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zMFt9hChvMl7",
        "outputId": "b0e798a5-fd16-41a3-dab4-9a02c133ec63"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Document(page_content='Skip to content', metadata={'source': 'https://www.meetup.com/topics/apache-flink/', 'language': 'en-US'})"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "webs[2]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2UuDLSiqQhha"
      },
      "source": [
        "Now we install the remaining libraries:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0_4wHAWtmAvJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "010596b1-55eb-4c23-9752-28ff803c09df"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m770.9/770.9 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m16.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m179.1/179.1 kB\u001b[0m \u001b[31m15.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.0/90.0 kB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.5/62.5 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m34.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m36.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "apache-beam 2.50.0 requires protobuf<4.24.0,>=3.20.3, but you have protobuf 3.19.6 which is incompatible.\n",
            "tensorflow 2.13.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3, but you have protobuf 3.19.6 which is incompatible.\n",
            "tensorflow-datasets 4.9.2 requires protobuf>=3.20, but you have protobuf 3.19.6 which is incompatible.\n",
            "tensorflow-metadata 1.14.0 requires protobuf<4.21,>=3.20.3, but you have protobuf 3.19.6 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "# Please ignore the library conflicts.\n",
        "!pip3 install -qU \\\n",
        "  langchain==0.0.162 \\\n",
        "  openai==0.27.7 \\\n",
        "  tiktoken==0.4.0 \\\n",
        "  \"pinecone-client[grpc]\"==2.2.2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HelKCvWEQhhb"
      },
      "source": [
        "---\n",
        "\n",
        "🚨 _Note: the above `pip install` is formatted for Jupyter notebooks. If running elsewhere you may need to drop the `!`._\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OPpcO-TwuQwD"
      },
      "source": [
        "Every record contains *a lot* of text. Our first task is therefore to identify a good preprocessing methodology for chunking these articles into more \"concise\" chunks to later be embedding and stored in our Pinecone vector database.\n",
        "\n",
        "For this we use LangChain's `RecursiveCharacterTextSplitter` to split our text into chunks of a specified max length."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-kohScp9Qhhb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "edc71f07-0949-41e8-d31d-135b69348d1f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<Encoding 'cl100k_base'>"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "import tiktoken\n",
        "\n",
        "tiktoken.encoding_for_model('gpt-3.5-turbo')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a3ChSxlcwX8n",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0e0cb231-3e5e-4b2a-bb69-99792db9f529"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "26"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "import tiktoken\n",
        "\n",
        "tokenizer = tiktoken.get_encoding('cl100k_base')\n",
        "\n",
        "# create the length function\n",
        "def tiktoken_len(text):\n",
        "    tokens = tokenizer.encode(\n",
        "        text,\n",
        "        disallowed_special=()\n",
        "    )\n",
        "    return len(tokens)\n",
        "\n",
        "tiktoken_len(\"hello I am a chunk of text and using the tiktoken_len function \"\n",
        "             \"we can find the length of this chunk of text in tokens\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "58J-y6GHtvQP"
      },
      "outputs": [],
      "source": [
        "# This code is not in use, please run the following one.\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=300,\n",
        "    chunk_overlap=0,\n",
        "    length_function=tiktoken_len,\n",
        "    separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W8KGqv-rzEgH"
      },
      "outputs": [],
      "source": [
        "chunks = text_splitter.split_documents(data)\n",
        "# chunks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YbV4SrBaFbS4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "36f54ee7-022b-4312-e703-c93a5f316642"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'source': 'pdf/1506.08603.pdf', 'page': 0}\n"
          ]
        }
      ],
      "source": [
        "print(chunks[0].metadata)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K9hdjy22zVuJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "394e2d24-5d66-4735-d214-bf764fa5c9d4"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(379, 384, 55)"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "tiktoken_len(chunks[0].page_content), tiktoken_len(chunks[1].page_content), tiktoken_len(chunks[2].page_content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SvApQNma0K8u"
      },
      "source": [
        "Using the `text_splitter` we get much better sized chunks of text. We'll use this functionality during the indexing process later. Now let's take a look at embedding.\n",
        "\n",
        "## Creating Embeddings\n",
        "\n",
        "Building embeddings using LangChain's OpenAI embedding support is fairly straightforward. We first need to add our [OpenAI api key]() by running the next cell:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dphi6CC33p62"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# get openai api key from platform.openai.com\n",
        "OPENAI_API_KEY = os.getenv('OPENAI_API_KEY') or 'OPENAI_API_KEY'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "49hoj_ZS3wAr"
      },
      "source": [
        "*(Note that OpenAI is a paid service and so running the remainder of this notebook may incur some small cost)*\n",
        "\n",
        "After initializing the API key we can initialize our `text-embedding-ada-002` embedding model like so:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mBLIWLkLzyGi"
      },
      "outputs": [],
      "source": [
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "\n",
        "model_name = 'text-embedding-ada-002'\n",
        "\n",
        "embed = OpenAIEmbeddings(\n",
        "    model=model_name,\n",
        "    openai_api_key=OPENAI_API_KEY\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SwbZGT-v4iMi"
      },
      "source": [
        "Now we embed some text like so:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vM-HuKtl4cyt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "deaa34b5-c2ac-41e8-b096-c4a2c798377d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2, 1536)"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ],
      "source": [
        "texts = [\n",
        "    'this is the first chunk of text',\n",
        "    'then another second chunk of text is here'\n",
        "]\n",
        "\n",
        "res = embed.embed_documents(texts)\n",
        "len(res), len(res[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QPUmWYSA43eC"
      },
      "source": [
        "From this we get *two* (aligning to our two chunks of text) 1536-dimensional embeddings.\n",
        "\n",
        "Now we move on to initializing our Pinecone vector database.\n",
        "\n",
        "## Vector Database\n",
        "\n",
        "To create our vector database we first need a [free API key from Pinecone](https://app.pinecone.io). Then we initialize like so:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yhz501Q7Qhhe"
      },
      "outputs": [],
      "source": [
        "index_name = 'YOUR_PINECONE_INDEX_NAME'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9pT9C4nW4vwo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dcbbb6e8-d71a-4006-cf0f-cb34ff956d67"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pinecone/index.py:4: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
            "  from tqdm.autonotebook import tqdm\n"
          ]
        }
      ],
      "source": [
        "import pinecone\n",
        "\n",
        "# find API key in console at app.pinecone.io\n",
        "PINECONE_API_KEY = os.getenv('PINECONE_API_KEY') or 'PINECONE_API_KEY'\n",
        "# find ENV (cloud region) next to API key in console\n",
        "PINECONE_ENVIRONMENT = os.getenv('PINECONE_ENVIRONMENT') or 'PINECONE_ENVIRONMENT'\n",
        "\n",
        "pinecone.init(\n",
        "    api_key='PINECONE_API_KEY',\n",
        "    environment='PINECONE_ENVIRONMENT'\n",
        ")\n",
        "\n",
        "if index_name not in pinecone.list_indexes():\n",
        "    # we create a new index\n",
        "    pinecone.create_index(\n",
        "        name=index_name,\n",
        "        metric='cosine',\n",
        "        dimension=len(res[0])  # 1536 dim of text-embedding-ada-002\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YgPUwd6REY6z"
      },
      "source": [
        "Then we connect to the new index:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RFydARw4EcoQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2f5c58bb-4262-4cc9-c61c-3b3dd9bf29ca"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'dimension': 1536,\n",
              " 'index_fullness': 0.0,\n",
              " 'namespaces': {'': {'vector_count': 0}},\n",
              " 'total_vector_count': 0}"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ],
      "source": [
        "index = pinecone.GRPCIndex(index_name)\n",
        "\n",
        "index.describe_index_stats()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0RqIF2mIDwFu"
      },
      "source": [
        "We should see that the new Pinecone index has a `total_vector_count` of `0`, as we haven't added any vectors yet.\n",
        "\n",
        "## Indexing\n",
        "\n",
        "We can perform the indexing task using the LangChain vector store object. But for now it is much faster to do it via the Pinecone python client directly. We will do this in batches of `100` or more."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GKFRLGuC1IDo",
        "outputId": "07c8b299-e2e1-4b00-8cab-942a551116e4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1000\n",
            "1012\n",
            "1658\n"
          ]
        }
      ],
      "source": [
        "final_data = []\n",
        "for i in data:\n",
        "  final_data.append(i)\n",
        "print(len(final_data))\n",
        "for i in webs:\n",
        "  final_data.append(i)\n",
        "print(len(final_data))\n",
        "for i in webs2:\n",
        "  final_data.append(i)\n",
        "print(len(final_data))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W-cIOoTWGY1R"
      },
      "outputs": [],
      "source": [
        "from tqdm.auto import tqdm\n",
        "from uuid import uuid4\n",
        "\n",
        "batch_limit = 100\n",
        "\n",
        "texts = []\n",
        "metadatas = []\n",
        "\n",
        "# for i, document in enumerate(tqdm(data)):\n",
        "for i, document in enumerate(tqdm(final_data)):\n",
        "    # first get metadata fields for this record\n",
        "    # print(i)\n",
        "    # print(type(i))\n",
        "    metadata = {\n",
        "        'source': document.metadata['source']\n",
        "    }\n",
        "    # print(document.metadata['source'])\n",
        "    # now we create chunks from the record text\n",
        "    record_texts = text_splitter.split_text(document.page_content)\n",
        "    # create individual metadata dicts for each chunk\n",
        "    record_metadatas = [{\n",
        "        \"chunk\": j, \"text\": text, **metadata\n",
        "    } for j, text in enumerate(record_texts)]\n",
        "    # append these to current batches\n",
        "    texts.extend(record_texts)\n",
        "    metadatas.extend(record_metadatas)\n",
        "    # if we have reached the batch_limit we can add texts\n",
        "    if len(texts) >= batch_limit:\n",
        "        ids = [str(uuid4()) for _ in range(len(texts))]\n",
        "        embeds = embed.embed_documents(texts)\n",
        "        index.upsert(vectors=zip(ids, embeds, metadatas))\n",
        "        texts = []\n",
        "        metadatas = []\n",
        "\n",
        "# for i, document in enumerate(tqdm(webs)):\n",
        "#     # first get metadata fields for this record\n",
        "#     # print(i)\n",
        "#     # print(type(i))\n",
        "#     metadata = {\n",
        "#         'source': str(document.metadata['source'])\n",
        "#     }\n",
        "#     print(document.metadata['source'])\n",
        "#     # now we create chunks from the record text\n",
        "#     record_texts = text_splitter.split_text(document.page_content)\n",
        "#     # create individual metadata dicts for each chunk\n",
        "#     record_metadatas = [{\n",
        "#         \"chunk\": j, \"text\": text, **metadata\n",
        "#     } for j, text in enumerate(record_texts)]\n",
        "#     # append these to current batches\n",
        "#     texts.extend(record_texts)\n",
        "#     metadatas.extend(record_metadatas)\n",
        "#     # if we have reached the batch_limit we can add texts\n",
        "#     if len(texts) >= batch_limit:\n",
        "#         ids = [str(uuid4()) for _ in range(len(texts))]\n",
        "#         embeds = embed.embed_documents(texts)\n",
        "#         index.upsert(vectors=zip(ids, embeds, metadatas))\n",
        "#         texts = []\n",
        "#         metadatas = []\n",
        "\n",
        "if len(texts) > 0:\n",
        "    ids = [str(uuid4()) for _ in range(len(texts))]\n",
        "    embeds = embed.embed_documents(texts)\n",
        "    index.upsert(vectors=zip(ids, embeds, metadatas))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XaF3daSxyCwB"
      },
      "source": [
        "We've now indexed everything. We can check the number of vectors in our index like so:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CaEBhsAM22M3"
      },
      "outputs": [],
      "source": [
        "index.describe_index_stats()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-8P2PryCy8W3"
      },
      "source": [
        "## Creating a Vector Store and Querying\n",
        "\n",
        "Now that we've build our index we can switch back over to LangChain. We start by initializing a vector store using the same index we just built. We do that like so:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qMXlvXOAyJHy",
        "outputId": "ed580f37-2aec-41e7-9b31-d2bad584ca13"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain/vectorstores/pinecone.py:59: UserWarning: Passing in `embedding` as a Callable is deprecated. Please pass in an Embeddings object instead.\n",
            "  texts: Iterable[str],\n"
          ]
        }
      ],
      "source": [
        "from langchain.vectorstores import Pinecone\n",
        "\n",
        "text_field = \"text\"\n",
        "\n",
        "# switch back to normal index for langchain\n",
        "index = pinecone.Index(index_name)\n",
        "\n",
        "vectorstore = Pinecone(\n",
        "    index, embed.embed_query, text_field\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "COT5s7hcyPiq",
        "outputId": "274f7add-8435-4ba3-fe47-6d81796c7e11"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[Document(page_content='Chapter 3. \\nThe Architecture of\\nApache Flink\\nChapter 2\\n discussed\\n important concepts of distributed stream\\nprocessing, such as parallelization, time, and state. In this chapter, we\\ngive a high-level introduction to Flink’s architecture and describe how\\nFlink addresses the aspects of stream processing we discussed earlier.\\nIn particular, we explain Flink’s distributed architecture, show how it\\nhandles time and state in streaming applications, and discuss its fault-\\ntolerance mechanisms. This chapter provides relevant background\\ninformation to successfully implement and operate advanced streaming\\napplications with Apache Flink. It will help you to understand Flink’s\\ninternals and to reason about the performance and behavior of\\nstreaming applications.\\nSystem Architecture\\nFlink\\n is a distributed system for stateful parallel data stream\\nprocessing. A Flink setup consists of multiple processes that typically\\nrun distributed across multiple machines. Common challenges that\\ndistributed systems need to address are allocation and management of\\ncompute resources in a cluster, process coordination, durable and\\nhighly available data storage, and failure recovery.\\nFlink does not implement all this functionality by itself. Instead, it', metadata={'chunk': 0.0, 'source': 'pdf/Stream Processing with Apache Flink.pdf'}),\n",
              " Document(page_content='Flink Architecture | Apache Flink\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nv1.19-SNAPSHOT\\n\\n\\n\\n\\n\\n\\n\\n\\n\\xa0\\xa0Try Flink▾\\n\\n\\n\\nFirst steps\\n\\n\\nFraud Detection with the DataStream API\\n\\n\\nReal Time Reporting with the Table API\\n\\n\\nFlink Operations Playground\\n\\n\\n\\n\\n\\n\\xa0\\xa0Learn Flink▾\\n\\n\\n\\nOverview\\n\\n\\nIntro to the DataStream API\\n\\n\\nData Pipelines & ETL\\n\\n\\nStreaming Analytics\\n\\n\\nEvent-driven Applications\\n\\n\\nFault Tolerance\\n\\n\\n\\n\\n\\n\\xa0\\xa0Concepts▾\\n\\n\\n\\nOverview\\n\\n\\nStateful Stream Processing\\n\\n\\nTimely Stream Processing\\n\\n\\nFlink Architecture\\n\\n\\nGlossary\\n\\n\\n\\n\\n\\n\\n\\xa0\\xa0Application Development▾\\n\\n\\n\\n\\nProject Configuration▾\\n\\n\\n\\nOverview\\n\\n\\nUsing Maven\\n\\n\\nUsing Gradle\\n\\n\\nConnectors and Formats\\n\\n\\nTest Dependencies\\n\\n\\nAdvanced Configuration\\n\\n\\n\\n\\n\\nDataStream API▾\\n\\n\\n\\nOverview\\n\\n\\nExecution Mode (Batch/Streaming)\\n\\n\\n\\nEvent Time▾\\n\\n\\n\\nGenerating Watermarks\\n\\n\\nBuiltin Watermark Generators\\n\\n\\n\\n\\n\\nState & Fault Tolerance▾\\n\\n\\n\\nWorking with State\\n\\n\\nThe Broadcast State Pattern\\n\\n\\nCheckpointing\\n\\n\\nState Backends\\n\\n\\n\\nData Types & Serialization▾\\n\\n\\n\\nOverview\\n\\n\\nState Schema Evolution\\n\\n\\nCustom State Serialization\\n\\n\\n3rd Party Serializers\\n\\n\\n\\n\\n\\n\\nUser-Defined Functions\\n\\n\\n\\nOperators▾\\n\\n\\n\\nOverview\\n\\n\\nWindows', metadata={'chunk': 0.0, 'source': 'https://nightlies.apache.org/flink/flink-docs-master/docs/concepts/flink-architecture/'}),\n",
              " Document(page_content='In this section we lay out the architecture of Flink as a software stack and as a distributed system. While Flink’s\\nstack of APIs continues to grow, we can distinguish four main layers: deployment, core, APIs, and libraries.\\nFlink’s Runtime and APIs. Figure 1 shows Flink’s software stack. The core of Flink is the distributed dataﬂow\\nengine, which executes dataﬂow programs. A Flink runtime program is a DAG of stateful operators connected\\nwith data streams. There are two core APIs in Flink: the DataSet API for processing ﬁnite data sets (often\\nreferred to as batch processing ), and the DataStream API for processing potentially unbounded data streams\\n(often referred to as stream processing ). Flink’s core runtime engine can be seen as a streaming dataﬂow engine,\\nand both the DataSet and DataStream APIs create runtime programs executable by the engine. As such, it serves\\nas the common fabric to abstract both bounded (batch) and unbounded (stream) processing. On top of the core\\n29', metadata={'chunk': 2.0, 'source': 'pdf/FULLTEXT01.pdf'})]"
            ]
          },
          "execution_count": 166,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "query = \"What is the architecture of flink?\"\n",
        "\n",
        "vectorstore.similarity_search(\n",
        "    query,  # our search query\n",
        "    k=3  # return 3 most relevant docs\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZCvtmREd0pdo"
      },
      "source": [
        "All of these are good, relevant results. But what can we do with this? There are many tasks, one of the most interesting (and well supported by LangChain) is called _\"Generative Question-Answering\"_ or GQA.\n",
        "\n",
        "## Generative Question-Answering\n",
        "\n",
        "In GQA we take the query as a question that is to be answered by a LLM, but the LLM must answer the question based on the information it is seeing being returned from the `vectorstore`.\n",
        "\n",
        "To do this we initialize a `RetrievalQA` object like so:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "moCvQR-p0Zsb"
      },
      "outputs": [],
      "source": [
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.chains import RetrievalQA\n",
        "\n",
        "# completion llm\n",
        "llm = ChatOpenAI(\n",
        "    openai_api_key=OPENAI_API_KEY,\n",
        "    model_name='gpt-3.5-turbo',\n",
        "    temperature=0.0\n",
        ")\n",
        "\n",
        "qa = RetrievalQA.from_chain_type(\n",
        "    llm=llm,\n",
        "    chain_type=\"stuff\",\n",
        "    retriever=vectorstore.as_retriever()\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 214
        },
        "id": "KS9sa19K3LkQ",
        "outputId": "5350219c-6466-46f2-ad5f-9c4cada53874"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"The architecture of Apache Flink is a distributed system for stateful parallel data stream processing. It consists of multiple processes that typically run distributed across multiple machines. The main components of Flink's architecture are:\\n\\n1. JobManager: The JobManager is responsible for coordinating the execution of Flink applications. It receives the dataflow program from the client, schedules tasks, and manages the overall execution of the application.\\n\\n2. TaskManagers: TaskManagers are responsible for executing the tasks assigned to them by the JobManager. They run the actual computations and process the data streams. TaskManagers can run on multiple machines in a cluster.\\n\\n3. Client: The client is not part of the runtime and program execution, but it is used to prepare and send the dataflow program to the JobManager. The client can disconnect after submitting the program or stay connected to receive progress reports.\\n\\n4. Cluster Resource Manager: Flink integrates with common cluster resource managers like Hadoop YARN and Kubernetes. These resource managers handle the allocation and management of compute resources in the cluster.\\n\\nOverall, Flink's architecture enables distributed and parallel processing of data streams, with fault-tolerance mechanisms to handle failures and ensure reliable processing.\""
            ]
          },
          "execution_count": 168,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "qa.run(query)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0qf5e3xf3ggq"
      },
      "source": [
        "We can also include the sources of information that the LLM is using to answer our question. We can do this using a slightly different version of `RetrievalQA` called `RetrievalQAWithSourcesChain`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aYVMGDA13cTz"
      },
      "outputs": [],
      "source": [
        "from langchain.chains import RetrievalQAWithSourcesChain\n",
        "\n",
        "qa_with_sources = RetrievalQAWithSourcesChain.from_chain_type(\n",
        "    llm=llm,\n",
        "    chain_type=\"stuff\",\n",
        "    retriever=vectorstore.as_retriever()\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RXsVEh3S4ZJO",
        "outputId": "220f1525-15b8-44ba-f2a1-dd2d879918ed"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'question': 'What is the architecture of flink?',\n",
              " 'answer': 'The architecture of Flink consists of multiple processes that run distributed across multiple machines. It is a distributed system for stateful parallel data stream processing. The Flink runtime consists of a JobManager and one or more TaskManagers. The JobManager is responsible for coordinating the execution of the dataflow program, while the TaskManagers are responsible for executing the actual tasks. Flink integrates with cluster resource managers such as Hadoop YARN and Kubernetes, but can also be set up to run as a standalone cluster or as a library.\\n',\n",
              " 'sources': 'pdf/Stream Processing with Apache Flink.pdf, https://nightlies.apache.org/flink/flink-docs-master/docs/concepts/flink-architecture/, pdf/FULLTEXT01.pdf'}"
            ]
          },
          "execution_count": 170,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "qa_with_sources(query)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nMRk_P3Q7l5J"
      },
      "source": [
        "Now we answer the question being asked, *and* return the source of this information being used by the LLM."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ehJEn68qADoH"
      },
      "source": [
        "---"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}